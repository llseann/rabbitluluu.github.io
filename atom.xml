<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sean</title>
  
  <subtitle>Keep looking.Don&#39;t settle</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="rabbitluluu.github.io/"/>
  <updated>2019-03-15T04:50:50.331Z</updated>
  <id>rabbitluluu.github.io/</id>
  
  <author>
    <name>刘生</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hbase的Shell操作以及读写过程解析</title>
    <link href="rabbitluluu.github.io/2019/03/15/Hbase%E7%9A%84Shell%E6%93%8D%E4%BD%9C%E4%BB%A5%E5%8F%8A%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B%E8%A7%A3%E6%9E%90/"/>
    <id>rabbitluluu.github.io/2019/03/15/Hbase的Shell操作以及读写过程解析/</id>
    <published>2019-03-15T04:50:15.000Z</published>
    <updated>2019-03-15T04:50:50.331Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、基本操作"><a href="#1、基本操作" class="headerlink" title="1、基本操作"></a>1、基本操作</h1><ol><li><p>启动终端</p><p><code>bin/hbase shell</code></p></li><li><p>显示所有表</p><p><code>list</code></p></li><li><p>显示命令介绍</p><p><code>help</code></p></li><li><p>显示当前的服务器状态</p><p><code>status &#39;node01&#39;</code></p><p><code>1 active master, 0 backup masters, 4 servers, 0 dead, 0.5000 average load</code></p><p>1 active master：一个存活的master</p><p>0 backup masters：0个备份master</p><p>4 servers：4个regionserver</p><p>0 dead：0个宕机</p><p>0.5000 average load：平均加载</p></li><li><p>显示当前用户</p><p><code>whoami</code></p></li><li><p>创建表</p><p><code>create &#39;user&#39;,&#39;info1&#39;,&#39;info2&#39;</code></p></li><li><p>全表扫描</p><p><code>scan &#39;user&#39;</code></p></li><li><p>存入数据(put ‘表名’，’Rowkey’，’列族:列名’，’值’)</p><p><code>put &#39;user&#39;,&#39;1001&#39;,&#39;info1:name&#39;,&#39;sean&#39;</code></p><p><code>put &#39;user&#39;,&#39;1001&#39;,&#39;info1:age&#39;,&#39;22&#39;</code></p></li><li><p>覆盖(hbase中无修改)</p><p><code>put &#39;user&#39;,&#39;1001&#39;,&#39;info2:id&#39;,&#39;112&#39;</code></p><p>一步一步深入，对应上表名，Rowkey，列族，列即可</p></li><li><p>查看表结构</p><p><code>describe &#39;user&#39;</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Table user is ENABLED                                                                   </div><div class="line">user                                                                                             </div><div class="line">COLUMN FAMILIES DESCRIPTION                                                                       </div><div class="line">&#123;NAME =&gt; &apos;info1&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS </div><div class="line">=&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =</div><div class="line">&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;                      </div><div class="line">&#123;NAME =&gt; &apos;info2&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS </div><div class="line">=&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =</div><div class="line">&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;                      </div><div class="line">2 row(s) in 0.1500 seconds</div></pre></td></tr></table></figure></li><li><p>变更表结构信息</p><p><code>alter &#39;user&#39;,{NAME =&gt; &#39;info1&#39;,VERSIONS =&gt; &#39;5&#39;}</code></p><p>需要指定列族，再指定到版本才能修改</p></li><li><p>过滤信息(get ‘表名’,’Rowkey’)</p><p><code>get &#39;user&#39;,&#39;1001&#39;</code></p><p><code>get &#39;user&#39;,&#39;1001&#39;,&#39;info2:id&#39;</code></p></li><li><p>清空数据</p><p><code>truncate &#39;user&#39;</code></p></li><li><p>删除表</p><p><code>drop &#39;user&#39;</code></p><p>但是返回：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ERROR: Table user is enabled. Disable it first.</div><div class="line"></div><div class="line">Here is some help for this command:</div><div class="line">Drop the named table. Table must first be disabled:</div><div class="line">  hbase&gt; drop &apos;t1&apos;</div><div class="line">  hbase&gt; drop &apos;ns1:t1&apos;</div></pre></td></tr></table></figure><p>正确步骤为：</p><p><code>disable &#39;user&#39;</code>：指定表不可用</p><p><code>drop &#39;user&#39;</code>：删除</p></li><li><p>扫描指定rowkey范围</p><p><code>scan &#39;user&#39;,{STARTROW=&gt;&#39;2202&#39;}</code></p><p>含头不含尾:</p><p><code>scan &#39;user&#39;,{STARTROW=&gt;&#39;2202&#39;,STOPROW=&gt;&#39;2202&#39;}</code></p></li><li><p>统计行数</p><p><code>count &#39;user&#39;</code></p></li></ol><h1 id="2、文件读写具体流程"><a href="#2、文件读写具体流程" class="headerlink" title="2、文件读写具体流程"></a>2、文件读写具体流程</h1><p>客户端访问zk,返回root表元数据位置，根据元数据信息去查找对应regionserver，根据root表去查找到META表，再根据meta表元数据查找region，返回元数据给客户端</p><h2 id="2-1、Hbase读取数据-总体"><a href="#2-1、Hbase读取数据-总体" class="headerlink" title="2.1、Hbase读取数据(总体)"></a>2.1、Hbase读取数据(总体)</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Hbase%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B%EF%BC%88%E6%80%BB%E4%BD%93%EF%BC%89.png" alt="流程"></p><p>过程梳理：</p><ol><li>客户端访问zk，发出请求</li><li>zk返回-ROOT表位置元数据信息</li><li>根据zk返回信息找到具体的HRegionserver，获取到-ROOT表</li><li>根据-ROOT表元数据信息找到.META表</li><li>根据.META表中存储的Region元数据信息找到具体的Region</li><li>返回所需要读取的信息</li></ol><h2 id="2-2、Hbase读取数据-局部"><a href="#2-2、Hbase读取数据-局部" class="headerlink" title="2.2、Hbase读取数据(局部)"></a>2.2、Hbase读取数据(局部)</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Hbase%E5%85%B7%E4%BD%93%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B.png" alt="Hbase读取数据(局部)"></p><p>过程梳理：</p><ol><li>client从memstore中读取数据</li><li>如果所需要的数据恰好在memstore中，那么直接返回到客户端</li><li>memstore没有找到则会在blockcache中查找</li><li>倘若blockcache中还没找到，那么将通过hdfs客户端对Hfile进行查找</li><li>将读取到的数据缓存到blockcache并返回</li><li>数据返回客户端</li></ol><h2 id="2-3、Hbase写数据过程"><a href="#2-3、Hbase写数据过程" class="headerlink" title="2.3、Hbase写数据过程"></a>2.3、Hbase写数据过程</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Hbase%E5%86%99%E6%95%B0%E6%8D%AE%E8%BF%87%E7%A8%8B.png" alt="写数据过程"></p><p>过程梳理：</p><ol><li>发出请求给zk</li><li>返回元数据信息</li><li>查找到对应的RegionServer(此处省略掉-ROOT，.META的嵌套查询过程)</li><li>找到对应的Region，同时将操作写入到Hlog</li><li>将数据写入到memstore中</li><li>满后溢写数据到StoreFile中，生成Hfile</li><li>hdfs客户端将Hfile写入到集群</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、基本操作&quot;&gt;&lt;a href=&quot;#1、基本操作&quot; class=&quot;headerlink&quot; title=&quot;1、基本操作&quot;&gt;&lt;/a&gt;1、基本操作&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;启动终端&lt;/p&gt;
&lt;p&gt;&lt;code&gt;bin/hbase shell&lt;/code&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Hbase" scheme="rabbitluluu.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hbase概述和部署</title>
    <link href="rabbitluluu.github.io/2019/03/14/Hbase%E6%A6%82%E8%BF%B0%E5%92%8C%E9%83%A8%E7%BD%B2/"/>
    <id>rabbitluluu.github.io/2019/03/14/Hbase概述和部署/</id>
    <published>2019-03-14T09:27:57.000Z</published>
    <updated>2019-03-14T09:28:35.628Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p><a href="http://www.apache.org/" target="_blank" rel="external">Apache</a> HBase™是<a href="http://hadoop.apache.org/" target="_blank" rel="external">Hadoop</a>数据库，是一个分布式，可扩展的大数据存储。</p><p>​    当需要对大数据进行随机，实时读/写访问时，请使用Apache HBase™。该项目的目标是托管非常大的表 - 数十亿行X百万列 - 在商品硬件集群上。Apache HBase是一个开源的，分布式的，版本化的非关系数据库，模仿Google的<a href="http://research.google.com/archive/bigtable.html" target="_blank" rel="external">Bigtable：</a> Chang等人的<a href="http://research.google.com/archive/bigtable.html" target="_blank" rel="external">结构化数据分布式存储系统</a>。正如Bigtable利用Google文件系统提供的分布式数据存储一样，Apache HBase在Hadoop和HDFS之上提供类似Bigtable的功能。</p><h1 id="2、集群角色"><a href="#2、集群角色" class="headerlink" title="2、集群角色"></a>2、集群角色</h1><p>hdfs：Namenode + DataNode</p><p>yarn：ResourceManager + NodeManger</p><p>hbase：HMaser + Regionserver</p><p>HMaser作用：</p><ul><li>对Regionserver监控</li><li>处理一些元数据的变更</li><li>对Regionserver进行故障转移</li><li>空闲时对数据进行负载均衡</li><li>对region进行管理</li><li>发布位置到客户端</li></ul><p>Regionserver作用：</p><ul><li>存储hbase的实际数据</li><li>刷新缓存数据到hdfs</li><li>处理Region</li><li>可以进行压缩</li><li>对Hlog进行维护</li><li>对region进行分片</li></ul><h1 id="3、集群安装"><a href="#3、集群安装" class="headerlink" title="3、集群安装"></a>3、集群安装</h1><ol><li><p>安装好zookeeper集群</p></li><li><p>安装好hadoop集群</p></li><li><p>解压，修改配置<code>hbase-env.sh</code> 和 <code>hbase-site.xml</code></p><p>hbase-env.sh:</p><p>export JAVA_HOME=/opt/modules/jdk1.8.0_141</p><p>export HBASE_MANAGES_ZK=false</p><p>hbase-site.xml:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">   &lt;property&gt;</div><div class="line">&lt;name&gt;hbase.rootdir&lt;/name&gt;</div><div class="line">&lt;value&gt;hdfs://node01:9000/hbase&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">   </div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</div><div class="line">&lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">   </div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hbase.master.port&lt;/name&gt;</div><div class="line">&lt;value&gt;16000&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">   </div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</div><div class="line">&lt;value&gt;node01:2181,node02:2181,node03:2181,node04:2181&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">   </div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</div><div class="line">&lt;value&gt;/opt/modules/zookeeper-3.4.10/zkData&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure><ol><li>拷贝core-site.xml 和 hdfs-site.xml 到conf下，或者制作软连接<br><code>ln -s /opt/modules/hadoop-2.8.4/etc/hadoop/core-site.xml</code></li></ol><p>​       <code>ln -s /opt/modules/hadoop-2.8.4/etc/hadoop/hdfs-site.xml</code></p><ol><li><p>拷贝安装包到其他机器</p></li><li><p>启动测试：</p><p>​    启动Hmaster：bin/hbase-daemon.sh start master</p><p>​    启动Regionserver：bin/hbase-daemon.sh start regionserver</p><p>操作后查看进程出现hmaster和regionserver，且在webUI（端口16010）中可以查看到节点即配置成功</p><font color="red">特别注意：hmaster启动指令中不叫Hmaster！！！！！！</font></li></ol><h1 id="4、集群架构解析"><a href="#4、集群架构解析" class="headerlink" title="4、集群架构解析"></a>4、集群架构解析</h1><p>架构图：</p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/%E5%9B%BE%E7%89%872%EF%BC%9AHbase%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="hbase架构"></p></li></ol><p>名词解释：</p><p>​    HLog：预写入日志，记录hbase修改记录，倘若系统故障，可以通过log文件进行重建，进行数据恢复。</p><p>​    HRegion：表的分片，根据Rowkey（行键）进行划分</p><p>​    Store：一个store对应hbase中的一个列族</p><p>​    StoreFile和HFile：HFile是真正的物理存储文件，StoreFile是逻辑概念</p><p>​    MemStore：接受来自HLog的数据，相当于内存，满了之后数据存到HFile，可以有效的提高查询效率</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;http://www.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;external
      
    
    </summary>
    
    
      <category term="Hbase" scheme="rabbitluluu.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban概述和使用</title>
    <link href="rabbitluluu.github.io/2019/03/13/Azkaban%E6%A6%82%E8%BF%B0%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>rabbitluluu.github.io/2019/03/13/Azkaban概述和使用/</id>
    <published>2019-03-13T01:58:11.000Z</published>
    <updated>2019-03-13T02:09:22.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、-Azkaban概述"><a href="#1、-Azkaban概述" class="headerlink" title="1、 Azkaban概述"></a>1、 Azkaban概述</h1><p><strong>Azkaban</strong>是在LinkedIn上创建的批处理工作流作业调度程序，用于运行Hadoop作业。Azkaban通过作业<strong>依赖</strong>性解决订单，并提供易于使用的Web用户界面来维护和跟踪您的工作流程。</p><p>工作流作业：</p><p>flume-&gt;hdfs-&gt;mr-&gt;hive建表-&gt;导入load data脚本</p><p><strong>自动化</strong>调度</p><h1 id="2、-环境搭建"><a href="#2、-环境搭建" class="headerlink" title="2、 环境搭建"></a>2、 环境搭建</h1><ol><li>创建相关mysql库并导入脚本</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">create database azkaban;</div><div class="line">use azkaban;</div><div class="line">source /root/hd/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql;</div></pre></td></tr></table></figure><ol><li><p>生成证书</p><p><code>keytool -keystore keystore -alias jetty -genkey -keyalg RSA</code></p><p>牢记密码后续要用</p><p>并将其拷贝到server中</p></li><li><p>时间同步配置</p><p>开启发送指令到所有窗口，保证集群时间同步，任务调度，所以和本地时间保持一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo date -s &apos;当前时间&apos;</div><div class="line">hwclock -w</div></pre></td></tr></table></figure></li><li><p>修改server配置文件<code>azkaban.properties</code>和<code>azkaban-users.xml</code></p><p>前者修改mysql相关配置和时区</p><p>后者添加账户密码<code>&lt;user username=&quot;admin&quot; password=&quot;admin&quot; roles=&quot;admin,metrics&quot;/&gt;</code>权限为管理员</p></li><li><p>修改excutor端配置文件<code>azkaban.properties</code>时区设置和mysql相关配置</p></li></ol><h1 id="3、azkaban实例"><a href="#3、azkaban实例" class="headerlink" title="3、azkaban实例"></a>3、azkaban实例</h1><font color="red">注意1，启动时需在主目录下，否则会有如下错误</font><p><code>Exception in thread &quot;main&quot; java.io.FileNotFoundException: conf/global.properties (No such file or directory)</code></p><font color="red">注意2，打包只能打成zip包，否则会无法识别</font><font color="red">注意3，指令写一行，不换行</font><font color="red">注意4，脚本不识别环境变量，需要将其启动脚本的绝对路径写全</font><ol><li><p>实例1：启动yarn集群</p><p><code>type=commandcommand=/opt/modules/hadoop-2.8.4/sbin/start-yarn.sh</code></p></li><li><p>实例2：启动mr程序</p><p><code>type=commandcommand=/opt/modules/hadoop-2.8.4/bin/hadoop jar /opt/modules/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /input/wc /output/wc2</code></p></li><li><p>实例3：设置任务依赖</p><p><strong>job_a.job:</strong></p><p><code>type=commandcommand=echo &#39;gakki&#39;</code></p><p><strong>job_b.job:</strong></p><p><code>type=commanddependencies=job_a.jobcommand=echo &#39;is my love&#39;</code></p></li><li><p>实例4：创建hive表并从本地导入数据</p><p><strong>hive.sql：</strong></p><p><code>use default;create table azhive(id int, name string, int num) row format delimited fields terminated by &#39;\t&#39;;load data inpath &#39;/temp/dept.txt&#39; into table azhive;</code></p><p><strong>hive-f.job：</strong></p><p><code>type=commandcommand=//opt/modules/hive/bin/hive -f &quot;hive.sql&quot;</code></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、-Azkaban概述&quot;&gt;&lt;a href=&quot;#1、-Azkaban概述&quot; class=&quot;headerlink&quot; title=&quot;1、 Azkaban概述&quot;&gt;&lt;/a&gt;1、 Azkaban概述&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Azkaban&lt;/strong&gt;是在Link
      
    
    </summary>
    
    
      <category term="Azkaban" scheme="rabbitluluu.github.io/tags/Azkaban/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop概述和使用</title>
    <link href="rabbitluluu.github.io/2019/03/13/Sqoop%E6%A6%82%E8%BF%B0%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>rabbitluluu.github.io/2019/03/13/Sqoop概述和使用/</id>
    <published>2019-03-13T01:57:57.000Z</published>
    <updated>2019-03-13T01:58:43.499Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、sqoop概述"><a href="#1、sqoop概述" class="headerlink" title="1、sqoop概述"></a>1、sqoop概述</h1><p>flume数据采集 采集日志数据</p><p>sqoop数据迁移 hdfs–&gt;mysql</p><p>azkaban任务调度</p><p>流程：flume–&gt;hdfs–&gt;shell–&gt;hive–&gt;sql–&gt;BI</p><p>sqoop数据迁移==mapreduce，处理离线数据</p><p>整个过程就是数据导入处理导出过程，直接使用map清洗</p><p>方法介绍：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">codegen            Generate code to interact with database records</div><div class="line">create-hive-table  Import a table definition into Hive</div><div class="line">eval               Evaluate a SQL statement and display the results</div><div class="line">export             Export an HDFS directory to a database table</div><div class="line">help               List available commands</div><div class="line">import             Import a table from a database to HDFS</div><div class="line">import-all-tables  Import tables from a database to HDFS</div><div class="line">import-mainframe   Import datasets from a mainframe server to HDFS</div><div class="line">job                Work with saved jobs</div><div class="line">list-databases     List available databases on a server</div><div class="line">list-tables        List available tables in a database</div><div class="line">merge              Merge results of incremental imports</div><div class="line">metastore          Run a standalone Sqoop metastore</div><div class="line">version            Display version information</div></pre></td></tr></table></figure><h1 id="2、-安装部署"><a href="#2、-安装部署" class="headerlink" title="2、 安装部署"></a>2、 安装部署</h1><ol><li><p>上传</p></li><li><p>下载</p></li><li><p>解压</p></li><li><p>重命名配置文件模板</p><p><code>mv sqoop-env-template.sh  sqoop-env.sh</code></p></li><li><p>修改配置文件</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>Set path to where bin/hadoop is available</div><div class="line">set HADOOP_COMMON_HOME=</div><div class="line"><span class="meta"></span></div><div class="line">#Set path to where hadoop-*-core.jar is available</div><div class="line">set HADOOP_MAPRED_HOME=</div><div class="line"><span class="meta"></span></div><div class="line">#set the path to where bin/hbase is available</div><div class="line"><span class="meta">#</span>set HBASE_HOME=</div><div class="line"><span class="meta"></span></div><div class="line">#Set the path to where bin/hive is available</div><div class="line">set HIVE_HOME=</div><div class="line"><span class="meta"></span></div><div class="line">#Set the path for where zookeper config dir is</div><div class="line">set ZOOCFGDIR=</div></pre></td></tr></table></figure><h1 id="3、-指令使用"><a href="#3、-指令使用" class="headerlink" title="3、 指令使用"></a>3、 指令使用</h1><p>import带入：mysql–&gt;hdfs</p><p>export导出：hdfs–&gt;mysql</p><h2 id="3-1、使用示例1：从mysql导入hdfs"><a href="#3-1、使用示例1：从mysql导入hdfs" class="headerlink" title="3.1、使用示例1：从mysql导入hdfs"></a>3.1、使用示例1：从mysql导入hdfs</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop import </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop  //指定mysql地址和数据库名</div><div class="line">--username root   //指定用户名</div><div class="line">--password root   //指定密码</div><div class="line">--table emp   //指定表名</div><div class="line">--target-dir /sqoop/data   //指定hdfs路径</div><div class="line">--num-mappers 1   //启动的map个数</div><div class="line">--fields-terminated-by "\t"          //设置数据的切分格式</div></pre></td></tr></table></figure><h2 id="3-2-、使用示例2：使用query对数据进行过滤"><a href="#3-2-、使用示例2：使用query对数据进行过滤" class="headerlink" title="3.2 、使用示例2：使用query对数据进行过滤"></a>3.2 、使用示例2：使用query对数据进行过滤</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop import </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root </div><div class="line">--password root </div><div class="line">--target-dir /sqoop/selectimport </div><div class="line">--num-mappers 1 </div><div class="line">--fields-terminated-by "\t" </div><div class="line">--query 'select * from emp where deptno &gt;= 20 and $CONDITIONS'</div></pre></td></tr></table></figure><p>检索deptno大于等于20的信息写入hdfs，此时运行一个map程序</p><p><code>$CONDITIONS</code>关键字起到索引作用，让map数据处理时不会重复</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop import </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root --password root </div><div class="line">--target-dir /sqoop/selectimport </div><div class="line">--num-mappers 2 </div><div class="line">--fields-terminated-by "\t" </div><div class="line">--query 'select * from emp where deptno &gt;= 20 and $CONDITIONS' </div><div class="line">--split-by empno</div></pre></td></tr></table></figure><p>设置两个map程序，但需要指明按照哪一项进行切分–split-by </p><h2 id="3-3-、使用示例3：导入到hive"><a href="#3-3-、使用示例3：导入到hive" class="headerlink" title="3.3 、使用示例3：导入到hive"></a>3.3 、使用示例3：导入到hive</h2><font color="red">倘若遇到报错import failed错误，可以通过添加环境变量来解决</font><p>解决办法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/modules/hive/lib/*</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">bin/sqoop import </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root </div><div class="line">--password root </div><div class="line">--table user </div><div class="line">--num-mappers 1 </div><div class="line">--hive-import </div><div class="line">--fields-terminated-by "\t" </div><div class="line">--hive-overwrite </div><div class="line">--hive-table user_sqoop</div></pre></td></tr></table></figure><h2 id="3-4、-使用示例4：hive导出mysql"><a href="#3-4、-使用示例4：hive导出mysql" class="headerlink" title="3.4、 使用示例4：hive导出mysql"></a>3.4、 使用示例4：hive导出mysql</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop export </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root</div><div class="line">--password root </div><div class="line">--table user </div><div class="line">--num-mappers 1 </div><div class="line">--export-dir /user/hive/warehouse/user_sqoop </div><div class="line">--input-fields-terminated-by "\t";</div></pre></td></tr></table></figure><p>常用参数：</p><p>import：导入数据到集群</p><p>export：从集群导出数据</p><p>create-hive-table：创建hive表</p><p>import-all-tables：导入所有库中表到hdfs集群</p><p>list-databases：列出所有数据库</p><p>list-tables：列出所有数据库表</p><p>merge：合并hdfs中的不同文件</p><p>codegen：获取某张表数据生成JavaBean并打包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop codegen </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root </div><div class="line">--password root </div><div class="line">--table user </div><div class="line">--bindir /root/sqoopjar/userBean </div><div class="line">--class-name UserBean </div><div class="line">--fields-terminated-by "\t"</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">bin/sqoop merge </div><div class="line">--new-data /testmerge/new </div><div class="line">--onto /testmerge/old </div><div class="line">--target-dir /testmerge/merged </div><div class="line">--jar-file /root/sqoopjar/userBean/UserBean.jar </div><div class="line">--class-name UserBean </div><div class="line">--merge-key id</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、sqoop概述&quot;&gt;&lt;a href=&quot;#1、sqoop概述&quot; class=&quot;headerlink&quot; title=&quot;1、sqoop概述&quot;&gt;&lt;/a&gt;1、sqoop概述&lt;/h1&gt;&lt;p&gt;flume数据采集 采集日志数据&lt;/p&gt;
&lt;p&gt;sqoop数据迁移 hdfs–&amp;gt
      
    
    </summary>
    
    
      <category term="Sqoop" scheme="rabbitluluu.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Flume安装及使用</title>
    <link href="rabbitluluu.github.io/2019/03/08/Flume%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
    <id>rabbitluluu.github.io/2019/03/08/Flume安装及使用/</id>
    <published>2019-03-08T10:10:14.000Z</published>
    <updated>2019-03-13T01:57:05.826Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>​    Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日<br>志数据。它具有基于流数据流的简单灵活的架构。它具有可靠的可靠性机制和许多故障<br>转移和恢复机制，具有强大的容错性。它使用简单的可扩展数据模型，允许在线分析应<br>用程序。</p><ol><li>数据采集（爬虫\日志数据\flume）</li><li>数据存储（hdfs/hive/hbase(nosql)）</li><li>数据计算（mapreduce/hive/sparkSQL/sparkStreaming/flink）</li><li>数据可视化(BI)</li></ol><h1 id="2、角色"><a href="#2、角色" class="headerlink" title="2、角色"></a>2、角色</h1><ol><li>source<br>数据源，用户采集数据，source产生数据流，同时会把产生的数据流传输到channel。</li><li>channel<br>传输通道，用于桥接source和sink</li><li>sink<br>下沉，用于收集channel传输的数据，将数据源传递到目标源</li><li>event<br>在flume中使用事件作为传输的基本单元</li></ol><p>图示：</p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/DevGuide_image00.png" alt="架构"></p><h1 id="3、-安装"><a href="#3、-安装" class="headerlink" title="3、 安装"></a>3、 安装</h1><ol><li>下载</li><li>上传到linux</li><li>解压<br>tar -zxvf flume.jar</li><li>重命名<br>mv apache-flume-1.6.0-bin/ flume<br>mv flume-env.sh.template flume-env.sh</li><li>修改配置<br>vi flume-env.sh<br>export JAVA_HOME=/opt/modules/jdk1.8.0_141</li></ol><h1 id="4、使用案例"><a href="#4、使用案例" class="headerlink" title="4、使用案例"></a>4、使用案例</h1><h2 id="4-1、-官方案例：监听端口"><a href="#4-1、-官方案例：监听端口" class="headerlink" title="4.1、 官方案例：监听端口"></a>4.1、 官方案例：监听端口</h2><p>配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>smple.conf: A single-node Flume configuration</div><div class="line"><span class="meta"></span></div><div class="line"># Name the components on this agent 定义变量方便调用 加s可以有多个此角色</div><div class="line">a1.sources = r1</div><div class="line">a1.sinks = k1</div><div class="line">a1.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source 描述source角色 进行内容定制</div><div class="line"><span class="meta">#</span> 此配置属于tcp source 必须是netcat类型</div><div class="line">a1.sources.r1.type = netcat </div><div class="line">a1.sources.r1.bind = localhost</div><div class="line">a1.sources.r1.port = 44444</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink 输出日志文件</div><div class="line">a1.sinks.k1.type = logger</div><div class="line"><span class="meta"></span></div><div class="line"># Use a channel which buffers events in memory（file） 使用内存 总大小1000 每次传输100</div><div class="line">a1.channels.c1.type = memory</div><div class="line">a1.channels.c1.capacity = 1000</div><div class="line">a1.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel 一个source可以绑定多个channel </div><div class="line"><span class="meta">#</span> 一个sinks可以只能绑定一个channel  使用的是图二的模型</div><div class="line">a1.sources.r1.channels = c1</div><div class="line">a1.sinks.k1.channel = c1</div></pre></td></tr></table></figure><p>启动指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">bin/flume-ng agent 使用ng启动agent</div><div class="line">--conf conf/ 指定配置所在的文件夹</div><div class="line">--name a1 指定的agent别名</div><div class="line">--conf-file conf/flumejob_telnet.conf 文件</div><div class="line">-Dflume.root.logger=INFO,console 日志级别</div></pre></td></tr></table></figure><h2 id="4-2、案例二：监控hive日志，实时同步到hdfs"><a href="#4-2、案例二：监控hive日志，实时同步到hdfs" class="headerlink" title="4.2、案例二：监控hive日志，实时同步到hdfs"></a>4.2、案例二：监控hive日志，实时同步到hdfs</h2><p>配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> Name the components on this agent agent别名设置</div><div class="line">a1.sources = r1</div><div class="line">a1.sinks = k1</div><div class="line">a1.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source  设置数据源监听本地文件配置</div><div class="line"><span class="meta">#</span> exec 执行一个命令的方式去查看文件 tail -F 实时查看</div><div class="line">a1.sources.r1.type = exec</div><div class="line"><span class="meta">#</span> 要执行的脚本command tail -F 默认10行 man tail  查看帮助</div><div class="line">a1.sources.r1.command = tail -F /tmp/root/hive.log</div><div class="line"><span class="meta">#</span> 执行这个command使用的是哪个脚本 -c 指定使用什么命令</div><div class="line"><span class="meta">#</span> whereis bash</div><div class="line"><span class="meta">#</span> bash: /usr/bin/bash /usr/share/man/man1/bash.1.gz </div><div class="line">a1.sources.r1.shell = /usr/bin/bash -c</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink </div><div class="line">a1.sinks.k1.type = hdfs</div><div class="line">a1.sinks.k1.hdfs.path = hdfs://node01:9000/flume/%Y%m%d/%H</div><div class="line"><span class="meta">#</span>上传文件的前缀</div><div class="line">a1.sinks.k1.hdfs.filePrefix = logs-</div><div class="line"><span class="meta">#</span>是否按照时间滚动文件夹</div><div class="line">a1.sinks.k1.hdfs.round = true</div><div class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹  秒 （默认30s）</div><div class="line">a1.sinks.k1.hdfs.roundValue = 1</div><div class="line"><span class="meta">#</span>重新定义时间单位（每小时滚动一个文件夹）</div><div class="line">a1.sinks.k1.hdfs.roundUnit = minute</div><div class="line"><span class="meta">#</span>是否使用本地时间戳</div><div class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</div><div class="line"><span class="meta">#</span>积攒多少个 Event 才 flush 到 HDFS 一次</div><div class="line">a1.sinks.k1.hdfs.batchSize = 500</div><div class="line"><span class="meta">#</span>设置文件类型，可支持压缩</div><div class="line">a1.sinks.k1.hdfs.fileType = DataStream</div><div class="line"><span class="meta">#</span>多久生成一个新的文件 秒</div><div class="line">a1.sinks.k1.hdfs.rollInterval = 30</div><div class="line"><span class="meta">#</span>设置每个文件的滚动大小 字节（最好128M）</div><div class="line">a1.sinks.k1.hdfs.rollSize = 134217700</div><div class="line"><span class="meta">#</span>文件的滚动与 Event 数量无关</div><div class="line">a1.sinks.k1.hdfs.rollCount = 0</div><div class="line"><span class="meta">#</span>最小冗余数(备份数 生成滚动功能则生效roll hadoop本身有此功能 无需配置) 1份 不冗余</div><div class="line">a1.sinks.k1.hdfs.minBlockReplicas = 1</div><div class="line"><span class="meta"></span></div><div class="line"># Use a channel which buffers events in memory </div><div class="line">a1.channels.c1.type = memory </div><div class="line">a1.channels.c1.capacity = 1000</div><div class="line">a1.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel</div><div class="line">a1.sources.r1.channels = c1</div><div class="line">a1.sinks.k1.channel = c1</div></pre></td></tr></table></figure><p>启动指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">bin/flume-ng agent </div><div class="line">--conf conf/ </div><div class="line">--name a1 </div><div class="line">--conf-file conf/flumejob_hdfs.conf</div></pre></td></tr></table></figure><h2 id="4-3、-案例三：flume监听文件夹"><a href="#4-3、-案例三：flume监听文件夹" class="headerlink" title="4.3、 案例三：flume监听文件夹"></a>4.3、 案例三：flume监听文件夹</h2><p>配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> 定义别名</div><div class="line">a1.sources = r1</div><div class="line">a1.sinks = k1</div><div class="line">a1.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source</div><div class="line">a1.sources.r1.type = spooldir</div><div class="line"><span class="meta">#</span> 监控的文件夹</div><div class="line">a1.sources.r1.spoolDir = /root/temp/testdir</div><div class="line"><span class="meta">#</span> 上传成功后显示后缀名 </div><div class="line">a1.sources.r1.fileSuffix = .COMPLETED</div><div class="line"><span class="meta">#</span> 如论如何 加绝对路径的文件名 默认false</div><div class="line">a1.sources.r1.fileHeader = true</div><div class="line"><span class="meta">#</span>忽略所有以.tmp 结尾的文件（正在被写入），不上传</div><div class="line"><span class="meta">#</span> ^以任何开头 出现无限次 以.tmp结尾的</div><div class="line">a1.sources.r1.ignorePattern = ([^ ]*\.tmp)</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink 下沉到hdfs</div><div class="line">a1.sinks.k1.type = hdfs</div><div class="line">a1.sinks.k1.hdfs.path = hdfs://node01:9000/flume/testdir/%Y%m%d/%H</div><div class="line"><span class="meta">#</span>上传文件的前缀</div><div class="line">a1.sinks.k1.hdfs.filePrefix = testdir-</div><div class="line"><span class="meta">#</span>是否按照时间滚动文件夹</div><div class="line">a1.sinks.k1.hdfs.round = true</div><div class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹</div><div class="line">a1.sinks.k1.hdfs.roundValue = 1</div><div class="line"><span class="meta">#</span>重新定义时间单位</div><div class="line">a1.sinks.k1.hdfs.roundUnit = hour</div><div class="line"><span class="meta">#</span>是否使用本地时间戳</div><div class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</div><div class="line"><span class="meta">#</span>积攒多少个 Event 才 flush 到 HDFS 一次</div><div class="line">a1.sinks.k1.hdfs.batchSize = 100</div><div class="line"><span class="meta">#</span>设置文件类型，可支持压缩</div><div class="line">a1.sinks.k1.hdfs.fileType = DataStream</div><div class="line"><span class="meta">#</span>多久生成一个新的文件</div><div class="line">a1.sinks.k1.hdfs.rollInterval = 600</div><div class="line"><span class="meta">#</span>设置每个文件的滚动大小大概是 128M </div><div class="line">a1.sinks.k1.hdfs.rollSize = 134217700</div><div class="line"><span class="meta">#</span>文件的滚动与 Event 数量无关</div><div class="line">a1.sinks.k1.hdfs.rollCount = 0</div><div class="line"><span class="meta">#</span>最小副本数</div><div class="line">a1.sinks.k1.hdfs.minBlockReplicas = 1</div><div class="line"><span class="meta"></span></div><div class="line"># Use a channel which buffers events in memory </div><div class="line">a1.channels.c1.type = memory </div><div class="line">a1.channels.c1.capacity = 1000</div><div class="line">a1.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel</div><div class="line">a1.sources.r1.channels = c1 </div><div class="line">a1.sinks.k1.channel = c1</div></pre></td></tr></table></figure><p>启动指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">bin/flume-ng agent </div><div class="line">--conf conf/ </div><div class="line">--name a1 </div><div class="line">--conf-file conf/flumejob_dir.conf</div></pre></td></tr></table></figure><h2 id="4-4、案例四：多sink情景，同时监听hive日志到hdfs和local"><a href="#4-4、案例四：多sink情景，同时监听hive日志到hdfs和local" class="headerlink" title="4.4、案例四：多sink情景，同时监听hive日志到hdfs和local"></a>4.4、案例四：多sink情景，同时监听hive日志到hdfs和local</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/%E5%A4%9Asink%E6%83%85%E6%99%AF.jpg" alt="情景"></p><p>配置文件flumejob_1.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> name the components on this agent 别名设置</div><div class="line">a1.sources = r1</div><div class="line">a1.sinks = k1 k2 </div><div class="line">a1.channels = c1 c2</div><div class="line"><span class="meta"></span></div><div class="line"># 将数据流复制给多个 channel</div><div class="line">a1.sources.r1.selector.type = replicating</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source </div><div class="line">a1.sources.r1.type = exec</div><div class="line">a1.sources.r1.command = tail -F /tmp/root/hive.log</div><div class="line">a1.sources.r1.shell = /bin/bash -c</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink</div><div class="line"><span class="meta">#</span> 分两个端口发送数据 </div><div class="line">a1.sinks.k1.type = avro </div><div class="line">a1.sinks.k1.hostname = node01</div><div class="line">a1.sinks.k1.port = 4141</div><div class="line"></div><div class="line">a1.sinks.k2.type = avro </div><div class="line">a1.sinks.k2.hostname = node01</div><div class="line">a1.sinks.k2.port = 4142</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the channel </div><div class="line">a1.channels.c1.type = memory </div><div class="line">a1.channels.c1.capacity = 1000</div><div class="line">a1.channels.c1.transactionCapacity = 100</div><div class="line"></div><div class="line">a1.channels.c2.type = memory </div><div class="line">a1.channels.c2.capacity = 1000</div><div class="line">a1.channels.c2.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel </div><div class="line">a1.sources.r1.channels = c1 c2 </div><div class="line">a1.sinks.k1.channel = c1</div><div class="line">a1.sinks.k2.channel = c2</div></pre></td></tr></table></figure><p>配置文件flumejob_2.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> Name the components on this agent </div><div class="line">a2.sources = r1</div><div class="line">a2.sinks = k1 </div><div class="line">a2.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source</div><div class="line">a2.sources.r1.type = avro </div><div class="line"><span class="meta">#</span> 端口抓取数据</div><div class="line">a2.sources.r1.bind = node01</div><div class="line">a2.sources.r1.port = 4141</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink </div><div class="line">a2.sinks.k1.type = hdfs</div><div class="line">a2.sinks.k1.hdfs.path = hdfs://node01:9000/flume2/%Y%m%d/%H</div><div class="line"><span class="meta"></span></div><div class="line">#上传文件的前缀</div><div class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</div><div class="line"><span class="meta">#</span>是否按照时间滚动文件夹</div><div class="line">a2.sinks.k1.hdfs.round = true</div><div class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹</div><div class="line">a2.sinks.k1.hdfs.roundValue = 1</div><div class="line"><span class="meta">#</span>重新定义时间单位</div><div class="line">a2.sinks.k1.hdfs.roundUnit = hour</div><div class="line"><span class="meta">#</span>是否使用本地时间戳</div><div class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</div><div class="line"><span class="meta">#</span>积攒多少个 Event 才 flush 到 HDFS 一次</div><div class="line">a2.sinks.k1.hdfs.batchSize = 100</div><div class="line"><span class="meta"></span></div><div class="line">#设置文件类型，可支持压缩</div><div class="line">a2.sinks.k1.hdfs.fileType = DataStream</div><div class="line"><span class="meta">#</span>多久生成一个新的文件</div><div class="line">a2.sinks.k1.hdfs.rollInterval = 600</div><div class="line"><span class="meta">#</span>设置每个文件的滚动大小大概是 128M </div><div class="line">a2.sinks.k1.hdfs.rollSize = 134217700</div><div class="line"><span class="meta">#</span>文件的滚动与 Event 数量无关</div><div class="line">a2.sinks.k1.hdfs.rollCount = 0</div><div class="line"><span class="meta">#</span>最小副本数</div><div class="line">a2.sinks.k1.hdfs.minBlockReplicas = 1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the channel </div><div class="line">a2.channels.c1.type = memory </div><div class="line">a2.channels.c1.capacity = 1000</div><div class="line">a2.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel </div><div class="line">a2.sources.r1.channels = c1</div><div class="line">a2.sinks.k1.channel = c1</div></pre></td></tr></table></figure><p>配置文件flumejob_3.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> Name the components on this agent </div><div class="line">a3.sources = r1</div><div class="line">a3.sinks = k1 </div><div class="line">a3.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source </div><div class="line">a3.sources.r1.type = avro</div><div class="line">a3.sources.r1.bind = node01</div><div class="line">a3.sources.r1.port = 4142</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink </div><div class="line">a3.sinks.k1.type = file_roll</div><div class="line">a3.sinks.k1.sink.directory = /root/flume2</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the channel </div><div class="line">a3.channels.c1.type = memory </div><div class="line">a3.channels.c1.capacity = 1000</div><div class="line">a3.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel </div><div class="line">a3.sources.r1.channels = c1</div><div class="line">a3.sinks.k1.channel = c1</div></pre></td></tr></table></figure><font color="red"><strong>注意：有启动顺序</strong></font>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h1&gt;&lt;p&gt;​    Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日&lt;br&gt;志数据。它具有基于流数据流的简单
      
    
    </summary>
    
    
      <category term="Flume" scheme="rabbitluluu.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Hive基础指令三</title>
    <link href="rabbitluluu.github.io/2019/03/08/Hive%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4%E4%B8%89/"/>
    <id>rabbitluluu.github.io/2019/03/08/Hive基础指令三/</id>
    <published>2019-03-08T10:10:05.000Z</published>
    <updated>2019-03-08T10:14:58.540Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive基础三"><a href="#Hive基础三" class="headerlink" title="Hive基础三"></a>Hive基础三</h1><h2 id="1、排序"><a href="#1、排序" class="headerlink" title="1、排序"></a>1、排序</h2><p><strong>Order By：</strong>全局排序</p><ol><li><p>按照员工表的奖金金额进行正序排列</p><p><code>select * from emptable order by emptable.comm;</code></p><p>默认正序，后面加asc， 可以省略</p></li><li><p>按照员工表的奖金金额进行倒序排列</p><p><code>select * from emptable order by emptable.comm desc;</code></p></li><li><p>按照部门和奖金进行升序排列</p><p><code>select * from emptable order by deptno,comm;</code></p></li></ol><p><strong>Sort By：</strong>内部排序，分区表中进行 并设置Reducer个数</p><ol><li><p>在<strong>分区表</strong>表中查询所有信息，启用3个reduce并根据deptno降序</p><p><code>set mapreduce.job.reduces=3;</code></p></li></ol><p>​    <code>select * from dept_partitions sort by deptno desc;</code></p><p><strong>Distribute By：</strong>分区排序</p><ol><li><p>先按照部门编号进行升序排序，再按照地域编号进行降序排序</p><p><code>select * from dept_partitions distribute by deptno sort by loc desc;</code></p></li></ol><p><strong>Cluster By：</strong></p><ol><li><p>按照部门编号进行排序</p><p><code>select * from dept_partitions cluster by deptno;</code></p><p>具备Distribute By 和Sort By两者功能，字段一样的时候可以代替</p><p>如：</p><p><code>select * from dept_partitions distribute by deptno sort by deptno;</code></p><p>等价于：<code>select * from dept_partitions cluster by deptno;</code></p></li></ol><h2 id="2、分桶"><a href="#2、分桶" class="headerlink" title="2、分桶"></a>2、分桶</h2><font color="red"><strong>区分：</strong>分桶分的是文件，分区分的是文件夹</font><p><strong>使用场景：</strong>用户需要统计一个具有代表性的结果时，并不是全部结果!</p><p>​        数据量十分大的时候，需要进行<strong>抽样</strong>的时候会用到</p><p><strong>设置支持分桶</strong>：<code>set hive.enforce.bucketing = true;</code></p><ol><li><p>如何创建分桶表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; create table emptable_buck(id int, name string)</div><div class="line">              &gt; clustered by(id) into 4 buckets</div><div class="line">              &gt; row format</div><div class="line">              &gt; delimited fields</div><div class="line">              &gt; terminated by &apos;\t&apos;;</div></pre></td></tr></table></figure></li></ol><ol><li><p>抽样</p><p><code>select * from emptable_buck tablesample(bucket 1 out of 2 on id);</code></p><p>tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y)</p><p>​        y必须是table总bucket数的<strong>倍数或者因子</strong>。hive根据y的大小，决定抽样的比例。例如，table总共分了64份，当y=32时，抽取 (64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。例 如，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</p></li></ol><h2 id="3、UDF自定义函数"><a href="#3、UDF自定义函数" class="headerlink" title="3、UDF自定义函数"></a>3、UDF自定义函数</h2><p>​    UDF：一进一出</p><p>​    UDAF：聚合函数，多进一出(count/max/avg)</p><p>​    UDTF：一进多出</p><p>添加jar包：</p><p>​    <code>add jar /root/myjar.jar</code></p><p>加载为临时的方法：</p><p>​    <code>create temporary function my_cat as &quot;包名.类名&quot;</code></p><p>加载为永久的方法：</p><p>hive-site.xml 中添加以下属性<code>hive.aux.jars.path</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hive.aux.jars.path&lt;/name&gt;</div><div class="line">&lt;value&gt;file:///root/hd/hive/lib/hive.jar&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure><h2 id="4、Hive压缩"><a href="#4、Hive压缩" class="headerlink" title="4、Hive压缩"></a>4、Hive压缩</h2><p>​    存储：hdfs</p><p>​    计算：mapreduce</p><h3 id="4-1Map端输出阶段压缩："><a href="#4-1Map端输出阶段压缩：" class="headerlink" title="4.1Map端输出阶段压缩："></a>4.1Map端输出阶段压缩：</h3><p>开启传输数据压缩功能：</p><p>​    <code>set hive.exec.compress.intermediate=true;</code></p><p>开启map输出压缩功能：</p><p>​    <code>set mapreduce.map.output.compress=true;</code><br>设置snappy压缩方式：<br>​    <code>set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</code></p><h3 id="4-2Reduce端输出压缩："><a href="#4-2Reduce端输出压缩：" class="headerlink" title="4.2Reduce端输出压缩："></a>4.2Reduce端输出压缩：</h3><p>设置hive输出数据压缩功能<br>    <code>set hive.exec.compress.output=true;</code><br>设置mr输出数据压缩<br>    <code>set mapreduce.output.fileoutputformat.compress=true;</code><br>指定压缩编码：<br>    <code>set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</code><br>指定压缩类型块压缩<br>    <code>set mapreduce.output.fileoutputformat.compress.type=BLOCK;</code><br>测试结果</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive基础三&quot;&gt;&lt;a href=&quot;#Hive基础三&quot; class=&quot;headerlink&quot; title=&quot;Hive基础三&quot;&gt;&lt;/a&gt;Hive基础三&lt;/h1&gt;&lt;h2 id=&quot;1、排序&quot;&gt;&lt;a href=&quot;#1、排序&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
      <category term="Hive" scheme="rabbitluluu.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive基础指令二</title>
    <link href="rabbitluluu.github.io/2019/03/08/Hive%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4%E4%BA%8C/"/>
    <id>rabbitluluu.github.io/2019/03/08/Hive基础指令二/</id>
    <published>2019-03-08T10:09:57.000Z</published>
    <updated>2019-03-08T10:14:38.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive基础操作二"><a href="#Hive基础操作二" class="headerlink" title="Hive基础操作二"></a>Hive基础操作二</h1><h2 id="1、-分区表"><a href="#1、-分区表" class="headerlink" title="1、 分区表"></a>1、 分区表</h2><ol><li><p>创建分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partitions(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</div><div class="line"><span class="keyword">partition</span> <span class="keyword">by</span>(<span class="keyword">day</span> <span class="keyword">string</span>)</div><div class="line"><span class="keyword">row</span> <span class="keyword">format</span></div><div class="line"><span class="keyword">delimited</span> <span class="keyword">fields</span></div><div class="line"><span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">''</span>;</div></pre></td></tr></table></figure></li><li><p>导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/temp/dept.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partitions <span class="keyword">partition</span>(<span class="keyword">day</span> = <span class="string">"0302"</span>);</div></pre></td></tr></table></figure><p>得<strong>指明分区字段</strong></p></li><li><p>查询</p></li></ol><ul><li><p>全查询</p><p><code>select * from dept_partitions;</code></p><p>此时查看的是<strong>整个</strong>分区表</p></li><li><p>单表查询</p><p><code>select * from dept_partitions where day = &#39;0302&#39;;</code></p><p>查找<strong>指定</strong>分区的数据</p></li><li><p>联合查询–union</p><p><code>select * from dept_partitions where day = &#39;0302&#39; union select * from dept_partitions where day = &#39;0303&#39;;</code></p><p>查询多个分区</p></li></ul><ol><li><p>添加分区</p><p><code>alter table dept_partitions add partition(day = &quot;0305&quot;);</code></p></li></ol><p>   添加多个分区</p><p>   <code>alter table dept_partitions add partition(day = &quot;0306&quot;) partition(day = &quot;0307&quot;);</code></p><p>   <strong>分区之间用空格分开即可</strong></p><ol><li><p>查询分区个数</p><p><code>show partitions dept_partitions;</code></p></li><li><p>删除分区</p><p><code>alter table dept_partitions drop partition(day = &quot;0307&quot;);</code></p></li><li><p>修复表的关联</p><p>直接创建hdfs路径，将文件上传进去，执行该语句直接将数据关联</p><p><code>msck repair table dept_partitions;</code></p></li></ol><h2 id="2、-DML数据操作"><a href="#2、-DML数据操作" class="headerlink" title="2、 DML数据操作"></a>2、 DML数据操作</h2><ol><li><p>数据导入</p><p><code>load data [local] inpath &#39;/文件名&#39; into table 表名;</code></p></li><li><p>向表中插入数据</p><p><code>insert into table stu_partitions partition(age = 18) values(1,&#39;gakki&#39;);</code></p></li></ol><p>   向现有表中插入sql查询结果(overwrite是覆盖)</p><p>   <code>insert overwriter table stu_partitions partition(age = 18) select * from sean_db.mostlove where id &lt;= 3;</code></p><p>​    直接创建新表插入sql查询结果,会自动创建和被查询表一样的结构</p><p>​    <code>create table if not exists stu_partitions1 as select * from stu_partitions where id =         4;</code></p><ol><li><p>直接创建表的同时加载数据</p><p>`create table stu_partitions2 (id int, name string)</p><pre><code>&gt; row format&gt; delimited fields&gt; terminated by &apos;\t&apos;&gt; location &apos;/temp&apos;;`</code></pre></li></ol><font color="red">注意：location指定的是HDFS中的一个<strong>文件夹</strong>，非文件,并且该文件夹下不能有其他目录</font><ol><li><p>将查询结果导出到本地</p><p><code>insert overwrite local directory &#39;/root/data&#39; select * from dept_partitions;</code></p></li><li><p>将hive表中数据导出到hdfs(拷贝过程)</p><p><code>export table dept_partitions to &#39;/data&#39;;</code></p><p>将hdfs中数据导入到hive(拷贝过程)</p><p><code>inport table test_table from &#39;/data&#39;;</code></p></li><li><p>清空表</p><p><code>truncate table test_table;</code></p></li></ol><h2 id="3-查询操作"><a href="#3-查询操作" class="headerlink" title="3. 查询操作"></a>3. 查询操作</h2><ol><li><p>基础查询</p><p><code>select * from table;</code></p><p><code>select id,name from table;</code></p></li><li><p>设置别名</p><p><code>select ename , sal+1000 as salary from sean_db.emptable;</code></p></li><li><p>创建员工表</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; create table sean_db.emptable (empno int, ename string, job string, mgr int, </div><div class="line">              &gt; birthday string, sal double, comm double, deptno int)</div><div class="line">              &gt; row format</div><div class="line">              &gt; delimited fields</div><div class="line">              &gt; terminated by '\t';</div></pre></td></tr></table></figure></li></ol><ol><li><p>查询公司有多少员工</p><p><code>select count(*) as empnumber from sean_db.emptable;</code></p></li><li><p>查询最高的工资</p><p><code>select max(sal) from sean_db.emptable;</code></p></li><li><p>查询最少的工资</p><p><code>select min(sal) from sean_db.emptable;</code></p></li><li><p>求工资的总和</p><p><code>select sum(sal) sal_sum from emp_table;</code></p></li><li><p>求该公司的员工工资平均值</p><p><code>select avg(sal) sal_avg from emptable;</code></p></li><li><p>查询结果只显示前n条</p><p><code>select * from emptable limit 3;</code></p></li></ol><p><strong>where语句，== 过滤</strong></p><p><strong>使用：where字句后紧接from</strong></p><ol><li><p>求出工资大于1300的员工</p><p><code>select * from emptable where sal &gt; 1300;</code></p></li><li><p>求出工资在1300~3000范围的员工信息</p><p><code>select * from emptable where sal &gt;= 1300 and sal &lt;= 3000;</code></p><p><code>select * from emptable where sal &gt; 1300 and sal &lt; 3000;</code></p><p><strong>is null 和 is not null</strong></p></li><li><p>求出奖金为空的</p><p><code>select * from emptable where comm is null;</code></p></li><li><p>求出奖金为非空的</p><p><code>select * from emptable where comm is not null;</code></p></li><li><p>求出工资在2000~3000员工的姓名</p><p><code>select ename from emptable where sal in(2000,3000);</code></p></li></ol><p><strong>like:模糊查询</strong></p><p><strong>使用：通配符</strong></p><p><code>%</code> : 代表后面零个或多个字符</p><p><code>_</code> : 代表一个字符</p><ol><li><p>查询工资以1开头的员工信息</p><p><code>select * from emptable where sal like &#39;1%&#39;;</code></p></li><li><p>查询工资第二位为1的员工信息</p><p><code>select * from emptable where sal like &#39;_1%&#39;;</code></p></li><li><p>查询工资里面含有5的员工信息</p><p><code>select * from emptable where sal like &#39;%5%&#39;;</code></p></li></ol><p><strong>AND/NOT/OR的使用</strong></p><ol><li><p>查询工资大于两千并且部门号是30</p><p><code>select * from emptable where sal &gt; 2000 and deptno = 30;</code></p></li><li><p>查询工资大于两千或者部门号是30</p><p><code>select * from emptable where sal &gt; 2000 or deptno = 30;</code></p></li><li><p>查询工资不在2000~3000的员工信息</p><p><code>select * from emptable where sal &lt; 2000 or sal &gt; 3000;</code></p></li></ol><p><strong>分组操作</strong></p><p><code>Group By</code>： 通常和一些聚合函数一起使用</p><ol><li><p>求每个部门的平均工资</p><p><code>select avg(sal) as avg_sal , deptno from emptable group by deptno;</code></p></li><li><p>求每个部门工资最低的工资</p><p><code>select min(sal) as min_sal, deptno from emptable group by deptno;</code></p></li></ol><p><code>having</code>： 用于分组的统计语句</p><p><code>where</code>： 后面不可以跟分组函数</p><ol><li><p>求平均工资大于2000的部门</p><p><code>select deptno, avg(sal) as avg_sal from emptable group by deptno having avg_sal &gt; 2000;</code></p></li></ol><h2 id="4-Join操作"><a href="#4-Join操作" class="headerlink" title="4. Join操作"></a>4. Join操作</h2><p>员工表中只有部门编号，没有部门名称</p><p>部门表中有部门标号和部门名称</p><ol><li><p>查询员工编号、员工姓名、员工所在的部门名称（等值join）</p><p><code>select emptable.empno, emptable.ename, dept.dname from emptable join dept on emptable.deptno = dept.deptno;</code></p><p><code>select e.empno, e.ename, d.dname from emptable as e join dept as d where e.deptno = d.deptno;</code></p></li><li><p>查询员工编号、员工姓名、员工所在的部门名称、部门所在地</p><p><strong>内连接</strong>：只有连接的两张表都存在于条件相匹配的数据才会被保留下来</p><p><code>select e.empno, e.ename, d.dname ,d.loc from emptable as e join dept as d on e.deptno = d.deptno;</code></p></li><li><p>查询员工编号、员工姓名、部门名称</p><p><strong>左外链接 left join</strong>：默认用的就是左外连接，可以省略，保留左表数据，右表没有join上 显示null</p><p><code>select e.empno, e.ename, d.dname from emptable e left join dept d on e.deptno = d.deptno;</code></p></li><li><p>查询员工编号、员工姓名、部门名称</p><p><strong>右外链接  right join</strong>：保留右表数据，左表没有join上显示为null</p><p><code>select e.empno, e.ename, d.dname from emptable e right join dept d on e.deptno = d.deptno;</code></p></li><li><p>查询员工编号、员工姓名、部门名称</p><p><strong>满外链接</strong>： 结果会返回所有表中符合条件的所有记录，如果有字段没有符合条件，用null代替</p><p><code>select e.empno, e.ename, d.dname from emptable e full join dept d on e.deptno = d.deptno</code></p></li><li><p>查询员工名、部门名称、地域名称———-多表查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">select</span> e.ename, d.dname, l.loc_name <span class="keyword">from</span> emptable e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno</div><div class="line">              &gt; <span class="keyword">join</span> location l <span class="keyword">on</span> d.loc = l.loc;</div></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive基础操作二&quot;&gt;&lt;a href=&quot;#Hive基础操作二&quot; class=&quot;headerlink&quot; title=&quot;Hive基础操作二&quot;&gt;&lt;/a&gt;Hive基础操作二&lt;/h1&gt;&lt;h2 id=&quot;1、-分区表&quot;&gt;&lt;a href=&quot;#1、-分区表&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="Hive" scheme="rabbitluluu.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive基础指令一</title>
    <link href="rabbitluluu.github.io/2019/03/08/Hive%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4%E4%B8%80/"/>
    <id>rabbitluluu.github.io/2019/03/08/Hive基础指令一/</id>
    <published>2019-03-08T10:09:48.000Z</published>
    <updated>2019-03-08T10:14:09.415Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、基础指令"><a href="#1、基础指令" class="headerlink" title="1、基础指令"></a>1、基础指令</h1><ul><li><p>显示数据库</p><p><code>show databases;</code></p></li><li><p>创建数据库</p><p><code>create database sean_db;</code></p><p><strong>标准写法：</strong></p><p><code>create database if not exists sean_db;</code></p><p><strong>指定hdfs路径为根目录db_cache文件夹下：</strong></p><p><code>create database sean_db location &#39;/db_cache&#39;</code></p><p><code>reate database if not exists sean_db location &#39;/&#39;;</code></p></li><li><p>使用数据库</p><p><code>use sean_db</code></p></li><li><p>创建表</p><p>mysql语法：<code>create table love(id int, name,string);</code></p><p>将查询结果创建为表</p><p><code>create table love2 as select * from love where name = &#39;gakki&#39;;</code></p><p><strong>hive语法：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> mostlove(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</div><div class="line">              &gt; <span class="keyword">row</span> <span class="keyword">format</span></div><div class="line">              &gt; <span class="keyword">delimited</span> <span class="keyword">fields</span></div><div class="line">              &gt; <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</div></pre></td></tr></table></figure></li><li><p>插入数据</p><p><code>insert into love values(1, &quot;gakki&quot;);</code></p></li><li><p>导入数据</p><p><code>load data [local] inpath &#39;文件路径&#39; into table 表名;</code></p><ul><li><p>导入本地文件</p><p><code>load data local inpath &#39;/root/temp/sean.txt&#39; into table love;</code></p></li><li><p>导入HDFS文件–根目录下的sean.txt到love表中</p><p><code>load data inpath &#39;/sean.txt&#39; into table love;</code></p></li></ul></li><li><p>删除表</p><p><code>drop table love;</code></p></li><li><p>查看库结构</p><p><code>desc database sean_db;</code></p></li><li><p>给库添加额外描述信息</p><p><code>alter database sean_db set dbproperties(&#39;created&#39; = &#39;sean&#39;);</code></p></li><li><p>查看库额外信息</p><p><code>desc database extended sean_db;</code></p></li><li><p>查看指定的通配库:过滤–找出所有s开头的库</p><p><code>show databases like &quot;s*&quot;;</code></p></li><li><p>删除库</p><ul><li>drop database sean_db;</li></ul><p>删除非空库</p><ul><li>drop database sean_db cascade;</li></ul><p>删除非空库标准写法</p><ul><li>drop databases if exists sean_db cascade;</li></ul></li></ul><h1 id="2、数据类型"><a href="#2、数据类型" class="headerlink" title="2、数据类型"></a>2、数据类型</h1><table><thead><tr><th style="text-align:center">Java数据类型</th><th style="text-align:center">HIve数据类型</th><th style="text-align:center">长度</th></tr></thead><tbody><tr><td style="text-align:center">byte</td><td style="text-align:center">TINYINT</td><td style="text-align:center">1byte有符号整数</td></tr><tr><td style="text-align:center">short</td><td style="text-align:center">SMALLINT</td><td style="text-align:center">2byte有符号整数</td></tr><tr><td style="text-align:center">int</td><td style="text-align:center">INT</td><td style="text-align:center">4byte有符号整数</td></tr><tr><td style="text-align:center">long</td><td style="text-align:center">BIGINT</td><td style="text-align:center">8byte有符号整数</td></tr><tr><td style="text-align:center">boolean</td><td style="text-align:center">BOOLEAN</td><td style="text-align:center">true/false</td></tr><tr><td style="text-align:center">float</td><td style="text-align:center">FLOAT</td><td style="text-align:center">单精度浮点</td></tr><tr><td style="text-align:center">double</td><td style="text-align:center">DOUBLE</td><td style="text-align:center">双精度浮点</td></tr><tr><td style="text-align:center">String</td><td style="text-align:center">STRING</td><td style="text-align:center">字符</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">BINARY</td><td style="text-align:center">字节数组</td></tr></tbody></table><h1 id="3、操作补充"><a href="#3、操作补充" class="headerlink" title="3、操作补充"></a>3、操作补充</h1><h2 id="3-1-创建表"><a href="#3-1-创建表" class="headerlink" title="3.1 创建表"></a>3.1 创建表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> [<span class="keyword">external</span>] <span class="keyword">table</span> [<span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span>] table_name(字段信息) [partitioned <span class="keyword">by</span>(字段信息)]</div><div class="line">[clustered <span class="keyword">by</span>(字段信息)] [sorted <span class="keyword">by</span>(字段信息)]</div><div class="line"><span class="keyword">row</span> <span class="keyword">format</span> </div><div class="line"><span class="keyword">delimited</span> <span class="keyword">fields</span></div><div class="line"><span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'切割符'</span>;</div></pre></td></tr></table></figure><h2 id="3-2-管理表（内部表）"><a href="#3-2-管理表（内部表）" class="headerlink" title="3.2 管理表（内部表）"></a>3.2 管理表（内部表）</h2><p>默认不加external创建的就是<strong>管理表</strong>，也称为<strong>内部表</strong>。<br>MANAGED_TABLE管理表。<br>Table Type:MANAGED_TABLE<br>查看表类型：<br><code>desc formatted itstar;</code></p><h2 id="3-3-外部表"><a href="#3-3-外部表" class="headerlink" title="3.3 外部表"></a>3.3 外部表</h2><p>EXTERNAL_TABLE外部表<br>创建方式：<br><code>create external table student(id int,name string)</code></p><p><strong>区别</strong>：如果是管理表，删除hdfs中数据删除，</p><p>​            如果是外部表删除，hdfs数据<strong>不删除</strong>！外部表删除后，重新创建同名表后，表中数据还在</p><h1 id="4、Hive常用命令"><a href="#4、Hive常用命令" class="headerlink" title="4、Hive常用命令"></a>4、Hive常用命令</h1><ul><li><p>不登录hive客户端操作hive指令</p><p><code>bin/hive -e &quot;select * from sean_db.love;&quot;</code></p></li><li><p>直接把sql写入到文件中</p><p><code>bin/hive -f /root/temp/hql.sql</code></p></li><li><p>在hive中可以直接查看hdfs中文件(省去hdfs)</p><p>dfs -ls /</p></li><li><p>清空表内数据,保留表结构</p><p><code>truncate table sean_buck;</code></p></li><li><p>设置Reducer个数</p><p><code>set mapreduce.job.reduces</code></p></li><li><p>设置支持分桶</p><p><code>set hive.enforce.bucketing = true;</code></p></li><li><p>显示方法</p><p><code>show functions;</code></p></li><li><p>查询方法使用</p><p><code>desc function extended pow;</code></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、基础指令&quot;&gt;&lt;a href=&quot;#1、基础指令&quot; class=&quot;headerlink&quot; title=&quot;1、基础指令&quot;&gt;&lt;/a&gt;1、基础指令&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;显示数据库&lt;/p&gt;
&lt;p&gt;&lt;code&gt;show databases;&lt;/code&gt;&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="Hive" scheme="rabbitluluu.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive安装及配置</title>
    <link href="rabbitluluu.github.io/2019/03/08/Hive%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE/"/>
    <id>rabbitluluu.github.io/2019/03/08/Hive安装及配置/</id>
    <published>2019-03-08T10:09:38.000Z</published>
    <updated>2019-03-08T11:05:30.080Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>官网：<a href="http://hive.apache.org/" target="_blank" rel="external">http://hive.apache.org/</a><br>Apache Hive™数据仓库软件有助于使用SQL读取，编写和管理驻留在分布式存储中的<br>大型数据集。可以将结构投影到已存储的数据中。提供了命令行工具和JDBC驱动程序以<br>将用户连接到Hive。<br>hive提供了SQL查询功能 hdfs分布式存储。<br>hive本质HQL转化为MapReduce程序。<br>环境前提：</p><ul><li><p>启动hdfs集群</p></li><li><p>启动yarn集群</p><p>如果想用hive的话，需要提前安装部署好hadoop集群。</p></li></ul><h1 id="2、特点"><a href="#2、特点" class="headerlink" title="2、特点"></a>2、特点</h1><p>  <strong>优势：</strong></p><ul><li>操作接口采用类sql语法，select * from stu;<br>简单、上手快！</li><li>hive可以替代mr程序，sqoop。</li><li>hive可以处理海量数据。</li><li>hive支持UDF，自定义函数。</li></ul><p><strong>劣势：</strong></p><ul><li><p>处理数据延迟高，慢。<br>引擎：1.2.2以前版本都是用的mr引擎</p><pre><code>2.x之后用的是spark引擎</code></pre></li><li><p>HQL的表达能力有限<br>一些sql无法解决的场景，依然需要我们写mapreduce.</p><h1 id="3、-架构原理"><a href="#3、-架构原理" class="headerlink" title="3、 架构原理"></a>3、 架构原理</h1></li></ul><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/hive%20%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.png" alt=""></p><blockquote><p>sql-&gt;转换-&gt;mapreduce-&gt;job</p></blockquote><h1 id="4、-安装部署"><a href="#4、-安装部署" class="headerlink" title="4、 安装部署"></a>4、 安装部署</h1><ol><li>下载</li><li>上传</li><li>解压</li><li>重命名conf中配置文件<br>mv hive-env.sh.template hive-env.sh</li><li>修改配置文件<br>vi hive-env.sh<br>HADOOP_HOME=/opt/modules/hadoop-2.8.4<br>export HIVE_CONF_DIR=/opt/modules/hive/conf</li><li>启动</li></ol><h1 id="5、-配置元数据库为Mysql"><a href="#5、-配置元数据库为Mysql" class="headerlink" title="5、 配置元数据库为Mysql"></a>5、 配置元数据库为Mysql</h1><p>因为HIVE自带元数据库derby不支持多客户端同时访问，故此更换Mysql为元数据库</p><p>前提：安装好Mysql</p><p>参考资料：<a href="https://blog.csdn.net/pengjunlee/article/details/81212250" target="_blank" rel="external">https://blog.csdn.net/pengjunlee/article/details/81212250</a></p><ol><li>驱动拷贝<br>拷贝mysql-connector-java-5.1.39-bin.jar到/root/opt/modules/hive/lib/下</li><li>配置Metastore到MySql</li></ol><ul><li><p>在/opt/modules/hive/conf目录下创建一个hive-site.xml</p></li><li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中（hive/conf/下创建文件）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line">&lt;configuration&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</div><div class="line">    &lt;value&gt;jdbc:mysql://node01:3306/metastorecreateDatabaseIfNotExist=true&lt;/value&gt;</div><div class="line">  &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</div><div class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</div><div class="line">  &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</div><div class="line">  &lt;value&gt;root&lt;/value&gt;</div><div class="line">  &lt;description&gt;username to use against metastore database&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</div><div class="line">  &lt;value&gt;root密码&lt;/value&gt;</div><div class="line">  &lt;description&gt;password to use against metastore database&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure><p>配置完之后需要重启集群生效。</p></li></ul><h1 id="6、-设置显示字段名与当前状态"><a href="#6、-设置显示字段名与当前状态" class="headerlink" title="6、 设置显示字段名与当前状态"></a>6、 设置显示字段名与当前状态</h1><h1 id="6-1-更改默认的元数据路径"><a href="#6-1-更改默认的元数据路径" class="headerlink" title="6.1 更改默认的元数据路径"></a>6.1 更改默认的元数据路径</h1><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</div><div class="line">  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</div><div class="line">  &lt;description&gt;location of default database for the warehouse&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure><h1 id="6-2-设置显示头信息和当前库"><a href="#6-2-设置显示头信息和当前库" class="headerlink" title="6.2 设置显示头信息和当前库"></a>6.2 设置显示头信息和当前库</h1><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.cli.print.header&lt;/name&gt;</div><div class="line">  &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</div><div class="line">  &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h1&gt;&lt;p&gt;官网：&lt;a href=&quot;http://hive.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;exte
      
    
    </summary>
    
    
      <category term="Hive" scheme="rabbitluluu.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper监听API操作</title>
    <link href="rabbitluluu.github.io/2019/03/08/Zookeeper%E7%9B%91%E5%90%ACAPI%E6%93%8D%E4%BD%9C/"/>
    <id>rabbitluluu.github.io/2019/03/08/Zookeeper监听API操作/</id>
    <published>2019-03-08T10:09:26.000Z</published>
    <updated>2019-03-08T10:13:22.587Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-对目录监听"><a href="#1-对目录监听" class="headerlink" title="1.对目录监听"></a>1.对目录监听</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.watch_;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.KeeperException;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/2/24/14:56</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> * 实现对节点的监听</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">watchDemo_</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</div><div class="line">        <span class="comment">//设置集群地址</span></div><div class="line">        String connected = <span class="string">"node01:2181,node02:2181,node03:2181,node04:2181"</span>;</div><div class="line">        <span class="comment">//设置超时时间</span></div><div class="line">        <span class="keyword">int</span> timeout = <span class="number">2000</span>;</div><div class="line">        <span class="comment">//创建客户端</span></div><div class="line">        ZooKeeper zkCli = <span class="keyword">new</span> ZooKeeper(connected, timeout, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</div><div class="line">                System.out.println(<span class="string">"正在监听：..."</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line"></div><div class="line">        List&lt;String&gt; children = zkCli.getChildren(<span class="string">"/"</span>, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</div><div class="line">                System.out.println(<span class="string">"正在监听路径为："</span> + event.getPath());</div><div class="line">                System.out.println(<span class="string">"正在监听类型为:"</span> + event.getType());</div><div class="line">            &#125;</div><div class="line">        &#125;, <span class="keyword">null</span>);</div><div class="line"></div><div class="line">        <span class="keyword">for</span>(String s:children)&#123;</div><div class="line">            System.out.println(s);</div><div class="line">        &#125;</div><div class="line">        Thread.sleep(Long.MAX_VALUE);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="2-对节点内容监听"><a href="#2-对节点内容监听" class="headerlink" title="2.对节点内容监听"></a>2.对节点内容监听</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.watch_;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.KeeperException;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/2/24/15:08</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> * 实现对节点内容的监听</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">watchDemo1_</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</div><div class="line">        <span class="comment">//设置集群地址</span></div><div class="line">        String connected = <span class="string">"node01:2181,node02:2181,node03:2181,node04:2181"</span>;</div><div class="line">        <span class="comment">//设置超时时间</span></div><div class="line">        <span class="keyword">int</span> timeout = <span class="number">2000</span>;</div><div class="line">        ZooKeeper zkCli = <span class="keyword">new</span> ZooKeeper(connected, timeout, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</div><div class="line">                System.out.println(<span class="string">"正在监听....请注意言行"</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line"></div><div class="line">        <span class="keyword">byte</span>[] data = zkCli.getData(<span class="string">"/gakki"</span>, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</div><div class="line">                System.out.println(<span class="string">"监听路径为："</span> + event.getPath());</div><div class="line">                System.out.println(<span class="string">"监听类型为："</span> + event.getType());</div><div class="line">            &#125;</div><div class="line">        &#125;, <span class="keyword">null</span>);</div><div class="line"></div><div class="line">        System.out.println(<span class="keyword">new</span> String(data));</div><div class="line">        Thread.sleep(Long.MAX_VALUE);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="3-实现持续监听"><a href="#3-实现持续监听" class="headerlink" title="3.实现持续监听"></a>3.实现持续监听</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.qq_;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.KeeperException;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/2/24/15:13</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> * 实现持续监听</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">zkClient_</span> </span>&#123;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</div><div class="line">        <span class="comment">//1.获取客户端</span></div><div class="line">        zkClient_ zkClient_ = <span class="keyword">new</span> zkClient_();</div><div class="line">        zkClient_.getConnect();</div><div class="line">        <span class="comment">//2.指定监听路径</span></div><div class="line">        zkClient_.getServer();</div><div class="line">        <span class="comment">//3.线程休眠</span></div><div class="line">        zkClient_.getWatchh();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">    * 获取zookeeper客户端连接</div><div class="line">    * */</div><div class="line">    <span class="comment">//设置集群地址</span></div><div class="line">    <span class="keyword">final</span> String connected = <span class="string">"node01:2181,node02:2181,node03:2181,node04:2181"</span>;</div><div class="line">    <span class="comment">//设置超时时间</span></div><div class="line">    <span class="keyword">final</span> <span class="keyword">int</span> timeout = <span class="number">2000</span>;</div><div class="line"></div><div class="line">    ZooKeeper zkCli;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        zkCli = <span class="keyword">new</span> ZooKeeper(connected, timeout, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</div><div class="line">                <span class="comment">//System.out.println("当前正在监听...");</span></div><div class="line">                List&lt;String&gt; children;</div><div class="line"></div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    children = zkCli.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</div><div class="line">                    <span class="comment">//服务器列表</span></div><div class="line">                    ArrayList&lt;String&gt; serverList_ = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line">                    <span class="comment">//获取每个节点数据</span></div><div class="line">                    <span class="keyword">for</span>(String s:children)&#123;</div><div class="line">                        <span class="keyword">byte</span>[] data = zkCli.getData(<span class="string">"/"</span> + s, <span class="keyword">true</span>, <span class="keyword">null</span>);</div><div class="line">                        serverList_.add(<span class="keyword">new</span> String(data));</div><div class="line">                    &#125;</div><div class="line"></div><div class="line">                    <span class="comment">//查看服务器列表</span></div><div class="line">                    System.out.println(serverList_);</div><div class="line">                &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 指定监听路径</div><div class="line">     * */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getServer</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        List&lt;String&gt; children = zkCli.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</div><div class="line"></div><div class="line">        <span class="comment">//存储服务器列表</span></div><div class="line">        ArrayList&lt;String&gt; serverList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line"></div><div class="line">        <span class="keyword">for</span>(String s: children)&#123;</div><div class="line">            <span class="keyword">byte</span>[] data = zkCli.getData(<span class="string">"/"</span> + s, <span class="keyword">true</span>, <span class="keyword">null</span>);</div><div class="line">            serverList.add(<span class="keyword">new</span> String(data));</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//打印服务器列表</span></div><div class="line">        System.out.println(serverList);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 实现一直监听</div><div class="line">     * */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getWatchh</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</div><div class="line">        Thread.sleep(Long.MAX_VALUE);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-对目录监听&quot;&gt;&lt;a href=&quot;#1-对目录监听&quot; class=&quot;headerlink&quot; title=&quot;1.对目录监听&quot;&gt;&lt;/a&gt;1.对目录监听&lt;/h2&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;
      
    
    </summary>
    
    
      <category term="Zookeeper" scheme="rabbitluluu.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper命令行以及API操作</title>
    <link href="rabbitluluu.github.io/2019/03/08/Zookeeper%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%A5%E5%8F%8AAPI%E6%93%8D%E4%BD%9C/"/>
    <id>rabbitluluu.github.io/2019/03/08/Zookeeper命令行以及API操作/</id>
    <published>2019-03-08T10:09:15.000Z</published>
    <updated>2019-03-08T10:12:38.556Z</updated>
    
    <content type="html"><![CDATA[<h1 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h1><p>配置环境变量：vi /etc/profile<br>export ZOOKEEPER_HOME=/root/hd/zookeeper-3.4.10<br>export PATH=$PATH:$ZOOKEEPER_HOME/bin<br>声明环境变量：source /etc/profile</p><ul><li><p>启动客户端<br>$ bin/zkCli.sh</p></li><li><p>连接其它机器客户端操作<br>$ connect node02:2181</p></li><li><p>查看历史操作记录<br>$ history</p></li><li><p>查看当前节点的内容<br>$ ls /</p></li><li><p>存储：创建节点<br>$ create /sean 10</p></li><li><p>查看节点的值<br>$ get /sean</p></li><li><p>创建节点的可选项<br>create [-s] [-e] path data acl<br>[-e] <strong>短暂</strong>节点<br>[-s] 带<strong>序号</strong><br>create -e /test 100<br><strong>注意：此时-e创建的是临时的短暂节点，退出客户端后消失。</strong>退出客户端：quit<br>create -s /test 100<br><strong>注意：此时-s创建是带序号的节点，可以创建节点名相同的，序号依次累加。</strong><br>创建短暂带序号节点<br>create -e -s /test 100</p></li><li><p>修改节点值<br>set path data [version]<br>[version] 版本<br>注意：设置版本号 必须从0开始</p></li><li><p>删除节点<br>delete path</p></li><li><p>创建子节点<br>create /sean/xyz</p></li><li><p>递归删除<br>rmr /sean</p></li><li><p>监听<br>获得监听：get path watch<br>获得当前节点下增减变化：ls path watch</p></li><li><p>查看当前节点状态<br>stat /test</p><h1 id="节点状态信息"><a href="#节点状态信息" class="headerlink" title="节点状态信息"></a>节点状态信息</h1><p>cZxid:zookeeper事务id<br>cTime:节点创建时间<br>mZxid：最后更新的czxid<br>mtime:最后修改的时间<br>pZxid：最后更新子节点的czxid<br>cversion:子节点的变化号、子节点修改次数<br>dataVersion：数据变化号<br>aclVersion：访问控制列表的变化号<br>ephemeralOwner：临时节点判断<br>dataLength：节点数据长度<br>numChildren：子节点个数</p></li></ul><h1 id="API操作"><a href="#API操作" class="headerlink" title="API操作"></a>API操作</h1><p><strong>EPHEMERAL</strong>（短暂的znode）、</p><p><strong>EPHEMERAL_SEQUENTIAL</strong>（短暂的顺序znode）、</p><p><strong>PERSISTENT</strong>（持久的znode）</p><p><strong>PERSISTENT_SEQUENTIAL</strong>（持久的顺序znode）</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line">package xyz.llsean.zk;</div><div class="line"></div><div class="line">import org.apache.zookeeper.*;</div><div class="line">import org.junit.Before;</div><div class="line">import org.junit.Test;</div><div class="line"></div><div class="line">import java.io.IOException;</div><div class="line">import java.util.List;</div><div class="line"></div><div class="line">/**</div><div class="line"> * @author: Sean</div><div class="line"> * @date: 2019/2/22/22:06</div><div class="line"> * @version: 1.0</div><div class="line"> */</div><div class="line">public class zkClient &#123;</div><div class="line">    //集群地址：端口号</div><div class="line">    private String connected = "node01:2181,node02:2181,node03:2181,node04:2181";</div><div class="line"></div><div class="line">    //超时设置（毫秒）</div><div class="line">    private int timeout = 2000;</div><div class="line"></div><div class="line">    ZooKeeper zkCli = null;</div><div class="line"></div><div class="line">    @Before</div><div class="line">    //连接zookeeper集群</div><div class="line">    public void init() throws IOException &#123;</div><div class="line">        //String:连接集群的ip地址端口 Int：超时设置 Wacher：监听</div><div class="line">        zkCli = new ZooKeeper(connected, timeout, new Watcher() &#123;</div><div class="line">            //回调方法   显示根节点</div><div class="line">            public void process(WatchedEvent event) &#123;</div><div class="line">                System.out.println("已经触发" + event.getType() + "事件！");</div><div class="line">                List&lt;String&gt; chilren;</div><div class="line"></div><div class="line">                //获得节点get</div><div class="line">                try &#123;</div><div class="line">                    chilren = zkCli.getChildren("/", true);</div><div class="line">                &#125; catch (KeeperException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125; catch (InterruptedException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    //测试 是否联通集群  创建节点</div><div class="line">    @Test</div><div class="line">    public void createNode() throws KeeperException, InterruptedException &#123;</div><div class="line">        String p = zkCli.create("/sean", "2333".getBytes(),</div><div class="line">                ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</div><div class="line">        //System.out.println("目前节点状态：["+zkCli.exists("/", true)+"]");</div><div class="line">        System.out.println(p);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    /**</div><div class="line">     * 获取节点列表</div><div class="line">     * */</div><div class="line">    @Test</div><div class="line">    public void getChild() throws KeeperException, InterruptedException &#123;</div><div class="line">        List&lt;String&gt; children = zkCli.getChildren("/", true);</div><div class="line">        for(String s:children)&#123;</div><div class="line">            System.out.println(s);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /**</div><div class="line">     * 删除节点</div><div class="line">     * */</div><div class="line">    @Test</div><div class="line">    public void deleteData() throws KeeperException, InterruptedException &#123;</div><div class="line">        zkCli.delete("/sean0000000016", -1);</div><div class="line">        getChild();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /**</div><div class="line">     * 修改数据</div><div class="line">     * */</div><div class="line">    @Test</div><div class="line">    public void setData() throws KeeperException, InterruptedException &#123;</div><div class="line">        zkCli.setData("/sean", "sb".getBytes(), -1);</div><div class="line"></div><div class="line">        //查看</div><div class="line">        byte[] data = zkCli.getData("/sean", false, new Stat());</div><div class="line">        System.out.println(new String (data));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /**</div><div class="line">     * 指定的节点是否存在</div><div class="line">     * */</div><div class="line">    @Test</div><div class="line">    public void testExist() throws KeeperException, InterruptedException &#123;</div><div class="line">        Stat exists = zkCli.exists("/llsean", false);</div><div class="line">        System.out.println(exists == null ? "不存在"":"存在");</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;命令行操作&quot;&gt;&lt;a href=&quot;#命令行操作&quot; class=&quot;headerlink&quot; title=&quot;命令行操作&quot;&gt;&lt;/a&gt;命令行操作&lt;/h1&gt;&lt;p&gt;配置环境变量：vi /etc/profile&lt;br&gt;export ZOOKEEPER_HOME=/root/hd/z
      
    
    </summary>
    
    
      <category term="Zookeeper" scheme="rabbitluluu.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper简介和安装</title>
    <link href="rabbitluluu.github.io/2019/03/08/Zookeeper%E7%AE%80%E4%BB%8B%E5%92%8C%E5%AE%89%E8%A3%85/"/>
    <id>rabbitluluu.github.io/2019/03/08/Zookeeper简介和安装/</id>
    <published>2019-03-08T10:09:03.000Z</published>
    <updated>2019-03-08T10:12:17.593Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Zookeeper"><a href="#1-Zookeeper" class="headerlink" title="1.Zookeeper"></a>1.Zookeeper</h1><p>官网：<a href="http://zookeeper.apache.org/" target="_blank" rel="external">http://zookeeper.apache.org/</a><br>介绍：Apache ZooKeeper致力于开发和维护开源服务器，实现高度可靠的分布式协<br>调。<br>ZooKeeper是一种集中式服务，用于维护配置信息，命名，提供分布式同步和提供组服<br>务。所有这些类型的服务都以分布式应用程序的某种形式使用。每次实施它们都需要做<br>很多工作来修复不可避免的错误和竞争条件。由于难以实现这些类型的服务，应用程序<br>最初通常会吝啬它们，这使得它们在变化的情况下变得脆弱并且难以管理。即使正确完<br>成，这些服务的不同实现也会在部署应用程序时导致管理复杂性。</p><h1 id="2-Zookeeper工作原理"><a href="#2-Zookeeper工作原理" class="headerlink" title="2.Zookeeper工作原理"></a>2.Zookeeper工作原理</h1><p>zookeeper功能：<strong>存储+监听</strong></p><h1 id="3-Zookeeper角色"><a href="#3-Zookeeper角色" class="headerlink" title="3.Zookeeper角色"></a>3.Zookeeper角色</h1><p>主从结构</p><ol><li>Leader领导者—-&gt;主</li><li>Follower追随者—&gt;从</li><li>zookeeper由一个领导者多个追随者组成<br>zk集群中只要有<strong>半数以上</strong>的节点存活，zk集群就能正常工作。所以搭建zk集群最好搭建<br><strong>奇数</strong>台（3,5,11）</li></ol><h1 id="4-zookeeper功能"><a href="#4-zookeeper功能" class="headerlink" title="4.zookeeper功能"></a>4.zookeeper功能</h1><p>大数据中使用zookeeper业务：</p><ul><li>做统一的<strong>配置管理</strong></li><li>做统一的<strong>命名服务</strong></li><li>做统一的<strong>集群管理</strong></li><li>做服务器的<strong>动态上下线感知</strong>（代码）</li></ul><h1 id="5-单节点安装部署"><a href="#5-单节点安装部署" class="headerlink" title="5.单节点安装部署"></a>5.单节点安装部署</h1><ol><li><p>下载安装包</p></li><li><p>上传安装到linux<br>alt+p</p></li><li><p>解压<br>$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/moduls</p></li><li><p>修改配置文件<br>重命名：mv zoo_sample.cfg zoo.cfg</p></li><li><p>创建文件夹zkData<br>添加到配置文件：zoo.cfg<br>dataDir=/opt/moduls/zookeeper-3.4.10/zkData</p></li><li><p>启动zookeeper</p><p>bin/zkServer.sh start</p></li><li><p>启动zookeeper客户端<br>bin/zkCli.sh</p></li></ol><h1 id="6-集群安装部署"><a href="#6-集群安装部署" class="headerlink" title="6.集群安装部署"></a>6.集群安装部署</h1><ol><li>下载安装包</li><li>上传安装到linux<br>alt+p</li><li>解压<br><code>$ tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/moduls</code></li><li>修改配置文件名<br>重命名：mv zoo_sample.cfg zoo.cfg</li><li>修改配置<br>dataDir=/opt/moduls/zookeeper-3.4.10/zkData</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">###### ########zkconfig##############</div><div class="line"></div><div class="line">server.1=node01:2888:3888</div><div class="line">server.2=node02:2888:3888</div><div class="line">server.3=node03:2888:3888</div><div class="line">server.4=node04:2888:3888</div></pre></td></tr></table></figure><p>创建文件myid</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$touch myid</div><div class="line"></div><div class="line">$vi myid</div></pre></td></tr></table></figure><p>添加服务器编号：1</p><p><strong>注意：不小心敲入空格会造成启动失败</strong></p><ol><li><p>拷贝zookeeper到其它机器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ scp -r zookeeper-3.4.10/ node02:$PWD</div><div class="line"></div><div class="line">$ scp -r zookeeper-3.4.10/ node02:$PWD</div><div class="line"></div><div class="line">$ scp -r zookeeper-3.4.10/ node02:$PWD</div></pre></td></tr></table></figure></li></ol><ol><li><p>注意需要修改每台机器的myid文件 设置为当前的机器编号即可</p></li><li><p>启动zookeeper集群<br>$ bin/zkServer.sh start</p></li><li><p>查看zookeeper状态<br>$ bin/zkServer.sh status</p></li></ol><h1 id="7-工作流程"><a href="#7-工作流程" class="headerlink" title="7.工作流程"></a>7.工作流程</h1><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Zookeeper%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.jpg" alt="工作流程"></p><h1 id="7-选举机制"><a href="#7-选举机制" class="headerlink" title="7.选举机制"></a>7.选举机制</h1><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Zookeeper%E9%80%89%E4%B8%BE%E6%9C%BA%E5%88%B6.jpg" alt="投票机制"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Zookeeper&quot;&gt;&lt;a href=&quot;#1-Zookeeper&quot; class=&quot;headerlink&quot; title=&quot;1.Zookeeper&quot;&gt;&lt;/a&gt;1.Zookeeper&lt;/h1&gt;&lt;p&gt;官网：&lt;a href=&quot;http://zookeeper.apach
      
    
    </summary>
    
    
      <category term="Zookeeper" scheme="rabbitluluu.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Mapreduce优化总结</title>
    <link href="rabbitluluu.github.io/2019/03/08/Mapreducer%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"/>
    <id>rabbitluluu.github.io/2019/03/08/Mapreducer优化总结/</id>
    <published>2019-03-08T10:08:48.000Z</published>
    <updated>2019-03-08T10:11:51.012Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-优化目的及方案"><a href="#1-优化目的及方案" class="headerlink" title="1. 优化目的及方案"></a>1. 优化目的及方案</h2><p><strong>优化目的：</strong>提高程序运行的效率</p><p><strong>优化方案：</strong></p><p>影响MR程序的因素</p><ul><li><strong>硬件</strong><br>CPU/磁盘(固态、机械)/内存/网络…</li><li><strong>I/O优化</strong><br>传输<ul><li>maptask与reducetask合理设置个数</li><li>数据倾斜（reducetask-》merge）,避免出现数据倾斜</li><li>大量小文件情况 （combineTextInputFormat）</li><li>combiner优化（不影响业务逻辑）</li></ul></li></ul><h2 id="2-具体操作方式"><a href="#2-具体操作方式" class="headerlink" title="2. 具体操作方式"></a>2. 具体操作方式</h2><p>MR包含流程（数据接入、Map、Reduce、IO传输、处理倾斜、参数优化）</p><p><strong>1. 数据接入：</strong>小文件进行合并 ，namenode存储元数据信息。<br>解决方式：CombineTextInputFormat</p><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">*</div><div class="line">* 小文件优化</div><div class="line">*</div><div class="line">* */</div><div class="line"><span class="comment">//设置读取数据切片的类</span></div><div class="line">job.setInputFormatClass(CombineTextInputFormat.class);</div><div class="line"><span class="comment">//最大切片大小8M</span></div><div class="line">CombineTextInputFormat.setMaxInputSplitSize(job,<span class="number">8388608</span>);</div><div class="line"><span class="comment">//最小切片大小为6M</span></div><div class="line">CombineTextInputFormat.setMinInputSplitSize(job,<span class="number">6291456</span>);</div></pre></td></tr></table></figure><p><strong>2. Map:</strong>会发生溢写，如果减少溢写次数也能达到优化，溢写内存增加这样就减少了溢写次数。<br>解决方式：修改mapred-site.xml</p><ul><li><p>新增属性：mapreduce.task.io.sort.mb<br>默认值为100，调大即可。</p></li><li><p>新增属性：mapreduce.map.sort.spill.percent<br>默认值为0.8，调大即可。</p><p>本质都是提高环形缓冲区内存，减少溢写次数从而达到调优的目的</p></li></ul><p><strong>3. combiner:</strong>map后优化,在Driver中添加在Reducer输出类型之后即可。</p><p>如:<code>job.setCombinerClass(WordCountReducer.class);</code></p><p><strong>4. Reduce:</strong>reduceTask设置合理的个数<br>写mr程序可以合理避免写reduce阶段，如：mapjoin<br>设置<strong>map/reduce共存（非常实用）</strong><br>解决方式：修改mapred-site.xml<br>新增属性：mapreduce.job.reduce.slowstart.completedmaps<br>默认值为0.05，减少即可。</p><p><strong>5.IO传输：</strong>压缩<br>数据倾斜：避免出现数据倾斜，map端合并。手动的对数据进行分段处理，合理的分区。</p><p>压缩代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">*</div><div class="line">* 在此处开启map端的输出压缩</div><div class="line">* 此处压缩不常用</div><div class="line">*</div><div class="line">* */</div><div class="line">conf.setBoolean(<span class="string">"mapreduce.map.output.compress"</span>, <span class="keyword">true</span>);</div><div class="line"><span class="comment">//设置压缩方式</span></div><div class="line">conf.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec.class, CompressionCodec.class);</div></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">*</div><div class="line">* 设置Reducer端输出压缩</div><div class="line">* 常在此处进行压缩，改变最后的结果后缀</div><div class="line">* */</div><div class="line"><span class="comment">//设置压缩方式</span></div><div class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);<span class="comment">//后缀为bz</span></div><div class="line">FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);<span class="comment">//后缀为gz</span></div></pre></td></tr></table></figure><p><strong>6.JVM重用（非常实用）：</strong><br>不关JVM<br>一个map运行一个jvm,开启重用，在运行完这个map后JVM继续运行其它map。<br>添加属性：mapreduce.job.jvm.numtasks<br>设置为：20<br>有效节省40%运行时间</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-优化目的及方案&quot;&gt;&lt;a href=&quot;#1-优化目的及方案&quot; class=&quot;headerlink&quot; title=&quot;1. 优化目的及方案&quot;&gt;&lt;/a&gt;1. 优化目的及方案&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;优化目的：&lt;/strong&gt;提高程序运行的效率&lt;/p&gt;
&lt;p&gt;&lt;
      
    
    </summary>
    
    
      <category term="Mapreduce" scheme="rabbitluluu.github.io/tags/Mapreduce/"/>
    
  </entry>
  
  <entry>
    <title>Combiner与shuffle原理</title>
    <link href="rabbitluluu.github.io/2019/03/08/Combiner%E4%B8%8Eshuffle%E5%8E%9F%E7%90%86/"/>
    <id>rabbitluluu.github.io/2019/03/08/Combiner与shuffle原理/</id>
    <published>2019-03-08T10:08:33.000Z</published>
    <updated>2019-03-08T10:10:52.552Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-combiner-合并"><a href="#1-combiner-合并" class="headerlink" title="1.combiner 合并"></a>1.combiner 合并</h2><ol><li><p>combiner是一个组件<br>注意：是Mapper和Reducer之外的一种组件<br>但是这个组件的父类是Reduer</p></li><li><p>如果想使用combiner继承Reduer即可</p></li><li><p>通过编写combiner发现与reducer代码<strong>相同</strong><br>只需在Driver端指定<br>setCombinerClass(WordCountReduer.class)</p><p><strong>注意：前提是不能影响业务逻辑</strong><a,1><c,1> <a,2><a,1> = <a,3><br>数学运算： (3 + 5 + 7)/3 = 5<br>(2 + 6)/2 = 4<br>不进行局部累加：（3 + 5 + 7 + 2 + 6）/5 = 23/5<br>进行了局部累加：（5+4）/2 = 9/2=4.5 不等于 23/5=4.6</a,3></a,1></a,2></c,1></a,1></p></li></ol><p>代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">*</div><div class="line">* 指定Combiner--用来优化合并</div><div class="line">* 在不影响结果的业务场景下，可以直接用Reducer的类进行优化</div><div class="line">*</div><div class="line">* */</div><div class="line">job.setCombinerClass(WordCountReducer.class);</div><div class="line">job.setCombinerClass(WordCountCombiner.class);</div><div class="line"><span class="comment">//两句等价，实际中不需要单独写Combiner直接使用Reducer即可</span></div></pre></td></tr></table></figure><p>发生时机：</p><p>Mapreducer整体流程：</p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/MapReduce%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90%20(1" alt="Mapreducer整体流程">.jpg)</p><p>具体时机：</p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Combiner%E6%97%B6%E6%9C%BA.png" alt="Combiner时机"></p><p>合并结果：</p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/combiner%E7%BB%93%E6%9E%9C.jpg" alt="结果"></p><h2 id="2-shuffle过程"><a href="#2-shuffle过程" class="headerlink" title="2.shuffle过程"></a>2.shuffle过程</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/shuffle%E8%BF%87%E7%A8%8B.jpg" alt="shuffle过程"></p><p><strong>注意：</strong></p><p>分区方式：hash分区</p><p>排序方式：字典快排</p><p>归并(combiner)和合并(merge)的区别:</p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/combiner%E5%92%8Cmerge%E7%9A%84%E5%8C%BA%E5%88%AB.jpg" alt="归并和合并"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-combiner-合并&quot;&gt;&lt;a href=&quot;#1-combiner-合并&quot; class=&quot;headerlink&quot; title=&quot;1.combiner 合并&quot;&gt;&lt;/a&gt;1.combiner 合并&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;combiner是一个组件&lt;br&gt;
      
    
    </summary>
    
    
      <category term="MapReduce" scheme="rabbitluluu.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>HDFS基础与操作三</title>
    <link href="rabbitluluu.github.io/2019/03/08/HDFS%E5%9F%BA%E7%A1%80%E4%B8%8E%E6%93%8D%E4%BD%9C%E4%B8%89/"/>
    <id>rabbitluluu.github.io/2019/03/08/HDFS基础与操作三/</id>
    <published>2019-03-08T07:42:34.000Z</published>
    <updated>2019-03-08T07:53:25.135Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、HDFS权限问题"><a href="#1、HDFS权限问题" class="headerlink" title="1、HDFS权限问题"></a>1、HDFS权限问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">针对用户操作没有权限 permission denied：</div></pre></td></tr></table></figure><p>  （1）修改 hdfs-site.xml 去掉权限检查（关闭HDFS服务 stop-all.sh;修改后 重新 Start-all.sh）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"> &lt;property&gt;</div><div class="line">       &lt;name&gt;dfs.permissions&lt;/name&gt;</div><div class="line">       &lt;value&gt;false&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure><p>   (2)通过设定用户名字 root</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);</div></pre></td></tr></table></figure><p>   (3)通过java的-D参数传递。 HADOOP_USER_NAME=root  （命令行的方式）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">public static void main(String[] args) </div><div class="line">https://www.cnblogs.com/-wangjiannan/p/3626965.html</div></pre></td></tr></table></figure><p>  （4）hdfs dfs -chmod 777 /input  让所有用户访问。</p><p>  （5）针对HDFS权限问题，有kerberos认证。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"> Kerberos: The Network Authentication Protocol</div><div class="line">https://www.cnblogs.com/wukenaihe/p/3732141.html</div></pre></td></tr></table></figure><h2 id="2、IDEA-Maven工程实现HDFS的文件上传与下载"><a href="#2、IDEA-Maven工程实现HDFS的文件上传与下载" class="headerlink" title="2、IDEA Maven工程实现HDFS的文件上传与下载"></a>2、IDEA Maven工程实现HDFS的文件上传与下载</h2><p><strong>Maven环境中 只有当 POM文件中所有的依赖包全部变成白色。</strong></p><p>  （1）HDFS文件上传</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> 查看源码：crtl+鼠标左键</div><div class="line">## Failed to locate the winutils binary in the hadoop binary path  java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries. ##</div><div class="line">step1:</div><div class="line">   下载：hadoop2.7.3  winutils binary </div><div class="line">   https://github.com/rucyang/hadoop.dll-and-winutils.exe-for-hadoop2.7.3-on-windows_X64</div><div class="line">step2: 配置环境变量 拷贝进入 D:\hadoop-2.7.3\bin文件下。</div><div class="line"> hadoop.home.dir ---bin/winutils.exe</div><div class="line"> HADOOP_HOME:D:\hadoop-2.7.3,然后再path里面增加 %HADOOP_HOME%\bin</div><div class="line"> 或者：System.setProperty(&quot;hadoop.home.dir&quot;, &quot;D:\\hadoop-2.7.3&quot;);</div></pre></td></tr></table></figure><p>  （2）HDFS文件下载</p><pre><code> ###  使用IOUtils 输入路径 输出路径###IOUtils.copyBytes(input,output,1024); </code></pre><h2 id="3、HDFS上传与下载原理"><a href="#3、HDFS上传与下载原理" class="headerlink" title="3、HDFS上传与下载原理"></a>3、HDFS上传与下载原理</h2><p>（1）上传原理<br> <img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/HDFS_Upload.png" alt=""><br>（2）下载原理<br><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/HDFS_DownLoad.png" alt=""></p><h2 id="4、安全模式-safe-mode"><a href="#4、安全模式-safe-mode" class="headerlink" title="4、安全模式 safe mode"></a>4、安全模式 safe mode</h2><p> 检查副本率是否满足配置要求。副本率不够的时候，会水平复制，当下次那个挂掉的节点如果又活过来的话，副本数就会超过N了，就超了，系统会自动选一个多余的副本删掉</p><p> (1)冗余度：dfs.replication 3.有几个冗余的副本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">  hdfs-site.xml</div><div class="line">&lt;!--注释配置数据块的冗余度，默认是3--&gt;</div><div class="line"> &lt;property&gt;</div><div class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</div><div class="line">        &lt;value&gt;1&lt;/value&gt;</div><div class="line"> &lt;/property&gt;</div></pre></td></tr></table></figure><p>（2）副本率：数据块实际冗余度（M），HDFS配置的数据块应该具有的冗余度（N），</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">       M/N*100%;</div><div class="line">      例如：知否知否.avi M=2；HDFS配置的 N=3；</div><div class="line">        2/3=0.667。要求的副本率为 0.99，系统会水平复制数据块到其他节点。</div><div class="line">        如果是副本率过高，M=6，N=3，副本率=2；大于0.99.系统会删除多余的数据块。</div><div class="line">在安全模式下 无法操作HDFS，因为正在进行副本率的检查工作。   </div><div class="line">进入或查看安全模式的命令：</div><div class="line"> hdfs dfsadmin -safemode get/enter/leave/wait</div></pre></td></tr></table></figure><h2 id="5、快照：是一种备份，默认：HDFS快照是关闭"><a href="#5、快照：是一种备份，默认：HDFS快照是关闭" class="headerlink" title="5、快照：是一种备份，默认：HDFS快照是关闭"></a>5、快照：是一种备份，默认：HDFS快照是关闭</h2><p><strong>一般不建议使用. </strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">快照的本质：将需要备份的数据放到一个隐藏目录下。</div><div class="line">(1)开启和关闭快照：</div><div class="line">   hdfs dfsadmin -allowSnapshot &lt;snapshotDir&gt;</div><div class="line">   hdfs dfsadmin -disallowSnapshot</div><div class="line">hdfs lsSnapshottableDir 查看开启快照的所有文件夹</div></pre></td></tr></table></figure><p>   （2）创建快照：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">                           需要创建快照的目录   快照目录的名字</div><div class="line"> hdfs dfs -createSnapshot /folder111          backup_folder111_20190118</div><div class="line">快照打出的日志：</div><div class="line"> Created snapshot /folder111/.snapshot/backup_folder111_20190118</div><div class="line">（3）删除快照</div><div class="line">hdfs dfs -deleteSnapshot /folder111  backup_folder111_20190118</div><div class="line">(4）恢复快照：</div><div class="line">hdfs dfs -cp /folder111/.snapshot/backup_folder111_20190118/a.png /folder111</div></pre></td></tr></table></figure><h2 id="6、回收站：默认HDFS的回收站禁用"><a href="#6、回收站：默认HDFS的回收站禁用" class="headerlink" title="6、回收站：默认HDFS的回收站禁用"></a>6、回收站：默认HDFS的回收站禁用</h2><p>  （1）回收站的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">core-site.xml  fs.trash.interval(时间间隔 分钟)</div><div class="line">关闭集群后才能起作用。</div></pre></td></tr></table></figure><p>  （2）本质是剪切：回收站开启之后，会把删除的文件放到一个/user/root/.Trash/Current</p><p>  （3）回收站回复也就是粘贴的过程。</p><h2 id="7、配额：Quota"><a href="#7、配额：Quota" class="headerlink" title="7、配额：Quota"></a>7、配额：Quota</h2><p>（1）名称配额</p><p>限定HDFS目录下，存放文件（目录）的个数&gt;1，最多存放N-1个。</p><p>  setQuota–指定名称配额</p><p>  clrQuota–清除名称配额</p><p>  例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">hdfs dfs -mkdir /myquota1</div><div class="line">hdfs dfsadmin -setQuota 3 /myquota1</div><div class="line">hdfs dfs -put ~/test.txt  /myquota1----第1个</div><div class="line">hdfs dfs -put ~/students.txt /myquota1--第2个。</div><div class="line">hdfs dfs -put ~/students01.txt /myquota1---第3个 无法放。</div></pre></td></tr></table></figure><p>错误：put: The NameSpace quota (directories and files) of directory /myquota1 is exceeded: quota=3 file count=4</p><p>（2）空间配额    –必须要大于 默认数据块大小。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">setSpaceQuota</div><div class="line"></div><div class="line">clrSpaceQuota</div></pre></td></tr></table></figure><h2 id="8、HDFS底层原理-RPC"><a href="#8、HDFS底层原理-RPC" class="headerlink" title="8、HDFS底层原理-RPC"></a>8、HDFS底层原理-RPC</h2><p>  Remote Procedure Call：远程过程调用，调用代码不在本地执行，实现调用者与被调用者之间的连接和通信。<br>   基于Client server，相当于 DFSClient 相当于客户端。Namenode集群相当于Server。</p><h2 id="9、HDFS底层原理-代理对象Proxy"><a href="#9、HDFS底层原理-代理对象Proxy" class="headerlink" title="9、HDFS底层原理-代理对象Proxy"></a>9、HDFS底层原理-代理对象Proxy</h2><p>（1）代理—明星的经纪人</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">是一种设计模式，提供了对目标对象的另一种访问方式。通过代理对象访问目标对象。</div></pre></td></tr></table></figure><p>（2）代理分为静态代理和动态代理。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">a、静态代理：接口的定义 实现接口。被代理对象与对象实现相同的接口。</div><div class="line">b、动态代理：接口的定义 不需要实现接口（匿名内部类+反射 invoke）</div></pre></td></tr></table></figure><h2 id="10、RPC与Proxy程序示例"><a href="#10、RPC与Proxy程序示例" class="headerlink" title="10、RPC与Proxy程序示例"></a>10、RPC与Proxy程序示例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">针对log4j warn</div><div class="line">log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).</div><div class="line">log4j:WARN Please initialize the log4j system properly.</div><div class="line">log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</div><div class="line">可以在src/resource/通过 增加 log4j.properties解决。</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1、HDFS权限问题&quot;&gt;&lt;a href=&quot;#1、HDFS权限问题&quot; class=&quot;headerlink&quot; title=&quot;1、HDFS权限问题&quot;&gt;&lt;/a&gt;1、HDFS权限问题&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;t
      
    
    </summary>
    
    
      <category term="HDFS" scheme="rabbitluluu.github.io/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Yarn的概念及流程</title>
    <link href="rabbitluluu.github.io/2019/03/08/Yarn%E7%9A%84%E6%A6%82%E5%BF%B5%E5%8F%8A%E6%B5%81%E7%A8%8B/"/>
    <id>rabbitluluu.github.io/2019/03/08/Yarn的概念及流程/</id>
    <published>2019-03-08T07:42:17.000Z</published>
    <updated>2019-03-08T07:44:11.660Z</updated>
    
    <content type="html"><![CDATA[<h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><p><strong>ResourceManager：</strong></p><ul><li>进行客户端任务请求的处理</li><li>对NodeManager进行监控</li><li>启动ApplicationMaster</li><li>对资源的调度和分配</li></ul><p><strong>NodeManager：</strong></p><ul><li>管理当前节点上的资源</li><li>处理来自RM的任务请求</li><li>处理ApplicationMaster命令</li></ul><p><strong>applicationMaster：</strong></p><ul><li>负责数据切片</li><li>申请资源分配给当前的任务  </li></ul><p><strong>container：</strong></p><ul><li>内存、磁盘、cpu、等资源</li></ul><h2 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Yarn.jpg" alt="Yarn"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;名词解释&quot;&gt;&lt;a href=&quot;#名词解释&quot; class=&quot;headerlink&quot; title=&quot;名词解释&quot;&gt;&lt;/a&gt;名词解释&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;ResourceManager：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;进行客户端任务请求的处理&lt;/l
      
    
    </summary>
    
    
      <category term="Yarn" scheme="rabbitluluu.github.io/tags/Yarn/"/>
    
  </entry>
  
  <entry>
    <title>数据压缩以及数据倾斜处理技巧</title>
    <link href="rabbitluluu.github.io/2019/03/08/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E4%BB%A5%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%8A%80%E5%B7%A7/"/>
    <id>rabbitluluu.github.io/2019/03/08/数据压缩以及数据倾斜处理技巧/</id>
    <published>2019-03-08T07:42:00.000Z</published>
    <updated>2019-03-08T07:44:41.499Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-压缩背景"><a href="#1-压缩背景" class="headerlink" title="1.压缩背景"></a>1.压缩背景</h2><p>为什么对数据进行压缩？<br>mapreduce操作需要对大量数据进行传输<br>压缩技术有效的减少底层存储系统读写字节数，hdfs。<br>压缩提高网络带宽和磁盘空间效率。<br>数据压缩节省资源，减少网络I/O。<br>通过压缩可以影响到mapreduce性能。(小文件优化，combiner)代码角度进行优化。<br><strong>注意：利用好压缩提高性能，运用不好会降低性能。</strong></p><h2 id="2-常用压缩格式"><a href="#2-常用压缩格式" class="headerlink" title="2.常用压缩格式"></a>2.常用压缩格式</h2><table><thead><tr><th style="text-align:center">压缩格式</th><th style="text-align:center">是否需要安装</th><th style="text-align:center">文件拓展名</th><th style="text-align:center">是否可以切分</th></tr></thead><tbody><tr><td style="text-align:center">DEFAULT</td><td style="text-align:center">直接使用</td><td style="text-align:center">.deflate</td><td style="text-align:center">否</td></tr><tr><td style="text-align:center">bzip2</td><td style="text-align:center">直接使用</td><td style="text-align:center">.bz2</td><td style="text-align:center">是</td></tr><tr><td style="text-align:center">Gzip</td><td style="text-align:center">直接使用</td><td style="text-align:center">.gz</td><td style="text-align:center">否</td></tr><tr><td style="text-align:center">LZO</td><td style="text-align:center">需要安装</td><td style="text-align:center">.lzo</td><td style="text-align:center">是</td></tr><tr><td style="text-align:center">Snappy</td><td style="text-align:center">需要安装</td><td style="text-align:center">.snappy</td><td style="text-align:center">否</td></tr></tbody></table><h2 id="3-性能测试"><a href="#3-性能测试" class="headerlink" title="3.性能测试"></a>3.性能测试</h2><table><thead><tr><th style="text-align:center">压缩格式</th><th style="text-align:center">原文件大小(GB)</th><th style="text-align:center">压缩后大小(GB)</th><th style="text-align:center">压缩速度(MB/s)</th><th style="text-align:center">解压速度(MB/s)</th></tr></thead><tbody><tr><td style="text-align:center">gzip</td><td style="text-align:center">8.3</td><td style="text-align:center">1.8</td><td style="text-align:center">20</td><td style="text-align:center">60</td></tr><tr><td style="text-align:center">LZO</td><td style="text-align:center">8.3</td><td style="text-align:center">3</td><td style="text-align:center">50</td><td style="text-align:center">70</td></tr><tr><td style="text-align:center">bzip2</td><td style="text-align:center">8.3</td><td style="text-align:center">1.1</td><td style="text-align:center">3</td><td style="text-align:center">10</td></tr></tbody></table><h2 id="4-发生阶段"><a href="#4-发生阶段" class="headerlink" title="4.发生阶段"></a>4.发生阶段</h2><p><strong>发生时机</strong></p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/%E5%8E%8B%E7%BC%A9%E6%97%B6%E6%9C%BA.jpg" alt="发生时机"></p><p><strong>具体操作：</strong></p><ol><li><p>设置map端输出压缩</p><ul><li><p>开启压缩 conf.setBoolean</p></li><li><p>设置具体压缩编码 conf.setClass</p><p>示例代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">conf.setBoolean(<span class="string">"mapreduce.map.output.compress"</span>, <span class="keyword">true</span>);</div><div class="line"><span class="comment">//设置压缩方式</span></div><div class="line">conf.setClass(<span class="string">"mapreduce.map.output.compress.codec"</span>, BZip2Codec.class, CompressionCodec.class);</div></pre></td></tr></table></figure></li></ul></li><li><p>设置reduce端输出压缩（常用）</p><ul><li><p>设置reduce输出压缩  FileOutputFormat.setCompressOutput</p></li><li><p>设置具体压缩编码 FileOutputFormat.setOutputCompressorClass</p><p>示例代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">FileOutputFormat.setCompressOutput(job,<span class="keyword">true</span>);</div><div class="line"><span class="comment">//设置压缩方式</span></div><div class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);<span class="comment">//后缀为bz</span></div></pre></td></tr></table></figure></li></ul></li></ol><h2 id="5-压缩编码使用场景"><a href="#5-压缩编码使用场景" class="headerlink" title="5.压缩编码使用场景"></a>5.压缩编码使用场景</h2><ol><li>Gzip压缩方式<br>压缩率比较高，并且压缩解压缩速度很快；<br>hadoop自身支持的压缩方式，用gzip格式处理数据就像直接处理文本数据是完全一样<br>的；<br>在linux系统自带gzip命令，使用很方便简洁。<br>不支持split<br>使用每个文件压缩之后大小需要在128M以下（块大小）<br>倘若文件200M可以通过设置块大小来解决conf.set(“blocksize”, “256M”)来解决</li></ol><ol><li><p>LZO压缩方式<br>压缩解压速度比较快并且压缩率比较合理；<br>支持split<br>在linux系统不可以直接使用，但是可以进行安装。<br>压缩率比gzip和bzip2要弱，hadoop本身不支持。</p></li><li><p>Bzip2压缩方式<br>支持压缩，具有很强的压缩率。hadoop本身支持。<br>linux中可以安装。<br>压缩解压缩速度很慢。</p></li></ol><ol><li>Snappy压缩方式<br>压缩解压缩速度很快。而且有合理的压缩率。<br>不支持split</li></ol><h2 id="6-数据倾斜"><a href="#6-数据倾斜" class="headerlink" title="6.数据倾斜"></a>6.数据倾斜</h2><p><strong>发生场景</strong></p><p><strong>代码模板</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">//1.创建job任务</div><div class="line">//2.指定jar包位置</div><div class="line">//3.关联Mapper类</div><div class="line">//4.设置最终输出的数据类型</div><div class="line">//5.设置数据输入的路径--自行指定</div><div class="line">//6.设置数据输出的路径</div><div class="line">//7.加载缓存数据</div><div class="line">//8.注意：没有跑reducer，需要指定reduceTask为0</div><div class="line">//9.提交任务</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-压缩背景&quot;&gt;&lt;a href=&quot;#1-压缩背景&quot; class=&quot;headerlink&quot; title=&quot;1.压缩背景&quot;&gt;&lt;/a&gt;1.压缩背景&lt;/h2&gt;&lt;p&gt;为什么对数据进行压缩？&lt;br&gt;mapreduce操作需要对大量数据进行传输&lt;br&gt;压缩技术有效的减少底层存储
      
    
    </summary>
    
    
      <category term="MapReduce" scheme="rabbitluluu.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce小文件切片技巧</title>
    <link href="rabbitluluu.github.io/2019/03/08/MapReduce%E5%B0%8F%E6%96%87%E4%BB%B6%E5%88%87%E7%89%87%E6%8A%80%E5%B7%A7/"/>
    <id>rabbitluluu.github.io/2019/03/08/MapReduce小文件切片技巧/</id>
    <published>2019-03-08T07:41:37.000Z</published>
    <updated>2019-03-08T07:45:19.351Z</updated>
    
    <content type="html"><![CDATA[<p><strong>背景</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">如果企业中存在海量的小文件数据</div><div class="line">TextInputFormat按照文件规划切片，文件不管多小都是一个单独的切片，启动mapt</div><div class="line">ask任务去执行，这样会产生大量的maptask，浪费资源。</div><div class="line">优化手段：</div><div class="line">小文件合并大文件，如果不动这个小文件内容。</div></pre></td></tr></table></figure><p><strong>代码</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">在Driver中添加如下代码：</div><div class="line"></div><div class="line"><span class="comment">//设置读取数据切片的类</span></div><div class="line">job.setInputFormatClass(CombineTextInputFormat.class);</div><div class="line"><span class="comment">//最大切片大小8M</span></div><div class="line">CombineTextInputFormat.setMaxInputSplitSize(job,<span class="number">8388608</span>);</div><div class="line"><span class="comment">//最小切片大小为6M</span></div><div class="line">CombineTextInputFormat.setMinInputSplitSize(job,<span class="number">6291456</span>);</div></pre></td></tr></table></figure><p><strong>Driver模板代码步骤</strong><br>    //1.创建job任务</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//2.制定jar包位置</span></div><div class="line"><span class="comment">//3.关联Mapper类</span></div><div class="line"><span class="comment">//4.关联Reducer类</span></div><div class="line"><span class="comment">//5.设置Mapper输出的数据类型</span></div><div class="line"><span class="comment">//6.设置Redecer输出的数据类型</span></div><div class="line"><span class="comment">//设置读取数据切片的类</span></div><div class="line"><span class="comment">//加入自定义分区</span></div><div class="line"><span class="comment">//设置结果文件个数</span></div><div class="line"><span class="comment">//7.设置数据输入的路径--自行指定</span></div><div class="line"><span class="comment">//8.设置数据输出的路径</span></div><div class="line"><span class="comment">//9.提交任务</span></div></pre></td></tr></table></figure><p>如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//1.创建job任务</span></div><div class="line">Configuration conf = <span class="keyword">new</span> Configuration();</div><div class="line"></div><div class="line"><span class="comment">//2.制定jar包位置</span></div><div class="line">Job job = Job.getInstance(conf);</div><div class="line"></div><div class="line"><span class="comment">//3.关联Mapper类</span></div><div class="line">job.setMapperClass(FlowSortMapper_.class);</div><div class="line"></div><div class="line"><span class="comment">//4.关联Reducer类</span></div><div class="line">job.setReducerClass(FlowSortReducer_.class);</div><div class="line"></div><div class="line"><span class="comment">//5.设置Mapper输出的数据类型</span></div><div class="line">job.setMapOutputKeyClass(FlowSortBean_.class);</div><div class="line">job.setMapOutputValueClass(Text.class);</div><div class="line"></div><div class="line"><span class="comment">//6.设置Redecer输出的数据类型</span></div><div class="line">job.setOutputKeyClass(Text.class);</div><div class="line">job.setOutputValueClass(FlowSortBean_.class);</div><div class="line"></div><div class="line"><span class="comment">//设置读取数据切片的类</span></div><div class="line"><span class="comment">//加入自定义分区</span></div><div class="line">job.setPartitionerClass(FlowSortPartitioner_.class);</div><div class="line"></div><div class="line"><span class="comment">//设置结果文件个数</span></div><div class="line">job.setNumReduceTasks(<span class="number">5</span>);</div><div class="line"></div><div class="line"><span class="comment">//7.设置数据输入的路径--自行指定</span></div><div class="line">FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"H:\\temp\\out\\logs_self1"</span>));</div><div class="line"></div><div class="line"><span class="comment">//8.设置数据输出的路径</span></div><div class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"H:\\temp\\out\\logs_self5"</span>));</div><div class="line"></div><div class="line"><span class="comment">//9.提交任务</span></div><div class="line"><span class="keyword">boolean</span> rs = job.waitForCompletion(<span class="keyword">true</span>);</div><div class="line">System.exit(rs?<span class="number">0</span>:<span class="number">1</span>);</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;背景&lt;/strong&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;
      
    
    </summary>
    
    
      <category term="MapReduce" scheme="rabbitluluu.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce分区和排序</title>
    <link href="rabbitluluu.github.io/2019/03/08/MapReduce%E5%88%86%E5%8C%BA%E5%92%8C%E6%8E%92%E5%BA%8F/"/>
    <id>rabbitluluu.github.io/2019/03/08/MapReduce分区和排序/</id>
    <published>2019-03-08T07:40:58.000Z</published>
    <updated>2019-03-08T07:45:45.925Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、分区"><a href="#1、分区" class="headerlink" title="1、分区"></a>1、分区</h2><p>需求：统计结果进行分区，根据手机号前三位来进行分区，如：135的为一个区域，136为第二个区域，以此类推。</p><p><strong>步骤</strong>：</p><ol><li>自定义类继承partitioner<key,value></key,value></li><li>重写方法<strong>getPartition()</strong></li><li>业务逻辑</li><li>在driver类中加入setPartitionerClass</li><li>注意：需要指定setNumReduceTasks(<strong>个数=分区数+1</strong>)</li></ol><p><strong>代码</strong></p><ol><li><p>新建类PhonenumPatitioner_</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhonenumPatitioner_</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean_</span>&gt;</span>&#123;</div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, FlowBean_ value, <span class="keyword">int</span> numPartitioner)</span> </span>&#123;</div><div class="line">​    <span class="comment">//获取前三位作为分区依据</span></div><div class="line">​    String phoneNum = key.toString().substring(<span class="number">0</span>,<span class="number">3</span>);</div><div class="line"></div><div class="line">​    <span class="comment">//分区</span></div><div class="line">​    <span class="keyword">int</span> Partition = <span class="number">5</span>;</div><div class="line"></div><div class="line">​    <span class="keyword">if</span>(<span class="string">"135"</span>.equals(phoneNum))&#123;</div><div class="line">​        <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">​    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">"136"</span>.equals(phoneNum))&#123;</div><div class="line">​        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">​    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">"137"</span>.equals(phoneNum))&#123;</div><div class="line">​        <span class="keyword">return</span> <span class="number">2</span>;</div><div class="line">​    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(<span class="string">"138"</span>.equals(phoneNum))&#123;</div><div class="line">​        <span class="keyword">return</span> <span class="number">3</span>;</div><div class="line">​    &#125;</div><div class="line"></div><div class="line">​    <span class="keyword">return</span> Partition;</div><div class="line">     &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ol><ol><li><p>修改Driver文件FlowCountDriver</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">//加入自定义分区`</div><div class="line">job.setPartitionerClass(PhonenumPatitioner_.class);</div><div class="line"></div><div class="line">//设置结果文件个数</div><div class="line">job.setNumReduceTasks(5);</div></pre></td></tr></table></figure><p><strong>注意：设置文件个数应为分区数+1，倘若设置个数较小，不会报错但是无结果。倘若设置个数过大，程序正常执行，但是会多出一部分空文件</strong></p></li></ol><h2 id="2、排序"><a href="#2、排序" class="headerlink" title="2、排序"></a>2、排序</h2><p>需求：每个分区内进行排序？</p><p><strong>步骤</strong>：</p><ol><li>实现WritableComparable接口</li><li>重写compareTo方法</li></ol><p><strong>代码</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">public int compareTo(FlowSortBean_ o) &#123;</div><div class="line">    //倒序排序</div><div class="line">    return this.flowSum &gt; o.getFlowSum() ? -1 : 1;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>区别于Writable接口，WritableComparable接口需要实现compaerTo方法，进行数据的正序或者逆序排列</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1、分区&quot;&gt;&lt;a href=&quot;#1、分区&quot; class=&quot;headerlink&quot; title=&quot;1、分区&quot;&gt;&lt;/a&gt;1、分区&lt;/h2&gt;&lt;p&gt;需求：统计结果进行分区，根据手机号前三位来进行分区，如：135的为一个区域，136为第二个区域，以此类推。&lt;/p&gt;
&lt;p&gt;&lt;
      
    
    </summary>
    
    
      <category term="MapReduce" scheme="rabbitluluu.github.io/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>HDFS基础与操作二</title>
    <link href="rabbitluluu.github.io/2019/03/08/HDFS%E5%9F%BA%E7%A1%80%E4%B8%8E%E6%93%8D%E4%BD%9C%E4%BA%8C/"/>
    <id>rabbitluluu.github.io/2019/03/08/HDFS基础与操作二/</id>
    <published>2019-03-08T07:40:26.000Z</published>
    <updated>2019-03-08T07:54:18.420Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1、HDFS权限问题"><a href="#1、HDFS权限问题" class="headerlink" title="1、HDFS权限问题"></a>1、HDFS权限问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">针对用户操作没有权限 permission denied：</div></pre></td></tr></table></figure><p>  （1）修改 hdfs-site.xml 去掉权限检查（关闭HDFS服务 stop-all.sh;修改后 重新 Start-all.sh）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"> &lt;property&gt;</div><div class="line">       &lt;name&gt;dfs.permissions&lt;/name&gt;</div><div class="line">       &lt;value&gt;false&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure><p>  （2）通过设定用户名字 root</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">System.setProperty(&quot;HADOOP_USER_NAME&quot;,&quot;root&quot;);</div></pre></td></tr></table></figure><p>  （3）通过java的-D参数传递。 HADOOP_USER_NAME=root  （命令行的方式）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">public static void main(String[] args)</div></pre></td></tr></table></figure><p>   <a href="https://www.cnblogs.com/-wangjiannan/p/3626965.html" target="_blank" rel="external">-D参数参考网址</a></p><p>  （4）hdfs dfs -chmod 777 /input  让所有用户访问。</p><p>  （5）针对HDFS权限问题，有kerberos认证。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Kerberos: The Network Authentication Protocol</div></pre></td></tr></table></figure><p>   <a href="https://www.cnblogs.com/wukenaihe/p/3732141.html" title="参考网址" target="_blank" rel="external">Kerberos参考网址</a></p><h2 id="2、IDEA-Maven工程实现HDFS的文件上传与下载"><a href="#2、IDEA-Maven工程实现HDFS的文件上传与下载" class="headerlink" title="2、IDEA Maven工程实现HDFS的文件上传与下载"></a>2、IDEA Maven工程实现HDFS的文件上传与下载</h2><p><strong>Maven环境中 只有当 POM文件中所有的依赖包全部变成白色。</strong></p><p><strong>查看源码：crtl+鼠标左键</strong></p><p>  （1）HDFS文件上传</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.FileSystem;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.IOUtils;</div><div class="line"></div><div class="line">import java.io.*;</div><div class="line"></div><div class="line">/**</div><div class="line"> * Created by Sean on 2019/1/14/20:38</div><div class="line"> * 实现文件上传</div><div class="line"> */</div><div class="line">public class HdfsUpload &#123;</div><div class="line">public static void main(String[] args) throws IOException &#123;</div><div class="line"></div><div class="line">    //step 1 建立客户端</div><div class="line">    Configuration conf = new Configuration();</div><div class="line">    conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://node01:9000&quot;);</div><div class="line">    //创建客户端</div><div class="line">    FileSystem client = FileSystem.get(conf);</div><div class="line"></div><div class="line">    //step 2 实现文件上传</div><div class="line">    //hdfs dfs -put /copyFromLocal</div><div class="line">    File file = new File(&quot;H:\\大数据实验手册\\大数据课程\\testFile.txt&quot;);</div><div class="line">    InputStream input = new FileInputStream(file);</div><div class="line"></div><div class="line">    //step 3 创建本地输出流</div><div class="line">    OutputStream output = client.create(new Path(&quot;/input/0114_2.txt&quot;));</div><div class="line"></div><div class="line">    //step 4 开始写入HDFS</div><div class="line">    /*方法一：*/</div><div class="line">    /*int len = 0;</div><div class="line">    byte[] buffer = new byte[1024];</div><div class="line">    //因为read读到文件尾会返回-1</div><div class="line">    while((len = input.read(buffer)) != -1)&#123;</div><div class="line">        output.write(buffer,0,len);</div><div class="line">    &#125;//循环写入数据</div><div class="line">    input.close();</div><div class="line">    output.flush();</div><div class="line">    buffer.close();*/</div><div class="line"></div><div class="line">    /*方法二*/</div><div class="line">    IOUtils.copyBytes(input,output,1024);</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">System.out.println(&quot;上传成功！&quot;);</div></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>  （2）HDFS文件下载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import org.apache.hadoop.conf.Configuration;</div><div class="line">import org.apache.hadoop.fs.FileSystem;</div><div class="line">import org.apache.hadoop.fs.Path;</div><div class="line">import org.apache.hadoop.io.IOUtils;</div><div class="line"></div><div class="line">import java.io.*;</div><div class="line"></div><div class="line">/**</div></pre></td></tr></table></figure><ul><li><p>Created by Sean on 2019/1/14/21:57</p></li><li><p>Hdfs下载文件<br>*/<br>public class HdfsDownLoad {<br>public static void main(String[] args) throws IOException {</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">//step 1 建立客户端</div><div class="line">Configuration conf = new Configuration();</div><div class="line">conf.set(&quot;fs.defaultFS&quot;,&quot;hdfs://node01:9000&quot;);</div><div class="line">//创建客户端</div><div class="line">FileSystem client = FileSystem.get(conf);</div><div class="line"></div><div class="line">//step 2 实现文件下载,创建数据输入流 指向hdfs 从hdfs读取数据</div><div class="line">//hdfs dfs -get /copyToLocal</div><div class="line">InputStream input = client.open(new Path(&quot;/input/0114_1.txt&quot;));</div><div class="line"></div><div class="line">//step 3 创建本地输出流</div><div class="line">OutputStream output = new FileOutputStream(&quot;H:/大数据实验手册/大数据课程/b.txt&quot;);</div><div class="line"></div><div class="line">//step 4 开始写入本地</div><div class="line">/*IOUtils*/</div><div class="line">IOUtils.copyBytes(input,output,1024);</div></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#125;</div></pre></td></tr></table></figure><p>}</p><p><strong>使用IOUtils 输入路径 输出路径</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">IOUtils.copyBytes(input,output,1024);</div></pre></td></tr></table></figure><h2 id="3、HDFS上传与下载原理"><a href="#3、HDFS上传与下载原理" class="headerlink" title="3、HDFS上传与下载原理"></a>3、HDFS上传与下载原理</h2><p><img src="https://i.imgur.com/ZByPVDT.png" alt=""></p><h2 id="4、安全模式-safe-mode"><a href="#4、安全模式-safe-mode" class="headerlink" title="4、安全模式 safe mode"></a>4、安全模式 safe mode</h2><h2 id="5、快照：是一种备份，默认：HDFS快照是关闭"><a href="#5、快照：是一种备份，默认：HDFS快照是关闭" class="headerlink" title="5、快照：是一种备份，默认：HDFS快照是关闭"></a>5、快照：是一种备份，默认：HDFS快照是关闭</h2><h2 id="6、配额：Quota"><a href="#6、配额：Quota" class="headerlink" title="6、配额：Quota"></a>6、配额：Quota</h2><h2 id="7、回收站：默认HDFS的回收站禁用"><a href="#7、回收站：默认HDFS的回收站禁用" class="headerlink" title="7、回收站：默认HDFS的回收站禁用"></a>7、回收站：默认HDFS的回收站禁用</h2><h2 id="8、idea控制上传下载遇到的问题及其解决"><a href="#8、idea控制上传下载遇到的问题及其解决" class="headerlink" title="8、idea控制上传下载遇到的问题及其解决"></a>8、idea控制上传下载遇到的问题及其解决</h2><p>  错误信息：<br>      <code>Failed to locate the winutils binary in the hadoop binary path  java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.</code></p><p>  step1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">下载：hadoop2.7.3  winutils binary </div><div class="line">https://github.com/rucyang/hadoop.dll-and-winutils.exe-for-hadoop2.7.3-on-windows_X64</div></pre></td></tr></table></figure><p>  step2:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">配置环境变量 拷贝进入 D:\hadoop-2.7.3\bin文件下。</div><div class="line">hadoop.home.dir ---bin/winutils.exe</div><div class="line">HADOOP_HOME:D:\hadoop-2.7.3,然后再path里面增加 %HADOOP_HOME%\bin</div><div class="line">或者：System.setProperty(&quot;hadoop.home.dir&quot;, &quot;D:\\hadoop-2.7.3&quot;);</div></pre></td></tr></table></figure><p>​            </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1、HDFS权限问题&quot;&gt;&lt;a href=&quot;#1、HDFS权限问题&quot; class=&quot;headerlink&quot; title=&quot;1、HDFS权限问题&quot;&gt;&lt;/a&gt;1、HDFS权限问题&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;t
      
    
    </summary>
    
    
      <category term="HDFS" scheme="rabbitluluu.github.io/tags/HDFS/"/>
    
  </entry>
  
</feed>
