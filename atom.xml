<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sean</title>
  
  <subtitle>Keep looking.Don&#39;t settle</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="rabbitluluu.github.io/"/>
  <updated>2019-03-22T02:51:16.853Z</updated>
  <id>rabbitluluu.github.io/</id>
  
  <author>
    <name>刘生</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ELK之K--Kibana</title>
    <link href="rabbitluluu.github.io/2019/03/22/ELK%E4%B9%8BK-Kibana/"/>
    <id>rabbitluluu.github.io/2019/03/22/ELK之K-Kibana/</id>
    <published>2019-03-22T02:46:16.000Z</published>
    <updated>2019-03-22T02:51:16.853Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>​    Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。</p><p>​    你用Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。</p><p>​    你可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。</p><p>​    Kibana使得理解大量数据变得很容易。它简单的、基于浏览器的界面使你能够快速创建和共享动态仪表板，实时显示Elasticsearch查询的变化。</p><h1 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h1><p>解压：tar -zxvf kibana-6.6.2-linux-x86_64.tar.gz</p><p>修改 kibana.yml 配置文件：</p><p>server.port: 5601</p><p>server.host: “node01”   ———-部署kinana服务器的ip</p><p>elasticsearch.hosts: [“<a href="http://node01:9200" target="_blank" rel="external">http://node01:9200</a>“]</p><p>kibana.index: “.kibana”</p><h1 id="3、问题排查"><a href="#3、问题排查" class="headerlink" title="3、问题排查"></a>3、问题排查</h1><ol><li><p>启动kibana，报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[error[plugin:remote_clusters@6.6.2] Status changed from red to red - X-Pack plugin is not installed on the [data] Elasticsearch cluster.</div></pre></td></tr></table></figure></li></ol><p>​    解决：卸载x-pack插件。</p><p>​    <code>elasticsearch-plugin remove x-pack</code></p><p>​    <code>kibana-plugin remove x-pack</code></p><ol><li><p>启动kibana，报错 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">log   [09:53:07.332] error[plugin:elasticsearch@6.1.1] Status changed from yellow to red - This version of Kibana requires Elasticsearch v6.1.1 on all nodes. I found the following incompatible nodes in your cluster: v5.6.2 @ 192.168.10.202:9200 (192.168.10.202), v5.6.2 @ 192.168.10.201:9200 (192.168.10.201), v5.6.2 @ 192.168.10.204:9200 (192.168.10.204), v5.6.2 @ 192.168.10.203:9200 (192.168.10.203)</div></pre></td></tr></table></figure><p>解决：重装对应版本的kibana即可</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h1&gt;&lt;p&gt;​    Kibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。&lt;/p&gt;
&lt;p&gt;​    
      
    
    </summary>
    
    
      <category term="Kibana" scheme="rabbitluluu.github.io/tags/Kibana/"/>
    
  </entry>
  
  <entry>
    <title>ELK之L--Logtash</title>
    <link href="rabbitluluu.github.io/2019/03/22/ELK%E4%B9%8BL-Logtash/"/>
    <id>rabbitluluu.github.io/2019/03/22/ELK之L-Logtash/</id>
    <published>2019-03-22T02:46:08.000Z</published>
    <updated>2019-03-22T02:50:58.167Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>​    logstash是一个数据分析软件，主要目的是分析log日志。整一套软件可以当作一个MVC模型，logstash是controller层，Elasticsearch是一个model层，kibana是view层。</p><p>​    首先将数据传给logstash，它将数据进行过滤和格式化（转成JSON格式），然后传给Elasticsearch进行存储、建搜索的索引，kibana提供前端的页面再进行搜索和图表可视化，它是调用Elasticsearch的接口返回的数据进行可视化。logstash和Elasticsearch是用Java写的，kibana使用node.js框架。</p><p>常用架构：L–&gt;redis/kafka–&gt;L–&gt;E–&gt;K</p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Logtash.png" alt=""></p><h1 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h1><ol><li>解压缩</li><li>直接用</li></ol><font color="red"><strong>注意：比较消耗资源，注意资源的分配</strong></font><p>内存过低会有如下错误：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c5330000, 986513408, 0) failed; error='Cannot allocate memory' (errno=12)</div><div class="line"><span class="meta">#</span></div><div class="line"><span class="meta">#</span> There is insufficient memory for the Java Runtime Environment to continue.</div><div class="line"><span class="meta">#</span> Native memory allocation (mmap) failed to map 986513408 bytes for committing reserved memory.</div><div class="line"><span class="meta">#</span> An error report file with more information is saved as:</div><div class="line"><span class="meta">#</span> /usr/local/logstash-6.6.2/confs_test/hs_err_pid3910.log</div></pre></td></tr></table></figure><ol><li><p>测试</p><p><code>bin/logstash -e &#39;input{stdin{}}output{stdout{codec=&gt;rubydebug}}&#39;</code></p><p>机器性能各异，个别可能会启动较慢，耐心等待即可</p></li></ol><h1 id="3、测试"><a href="#3、测试" class="headerlink" title="3、测试"></a>3、测试</h1><h2 id="3-1、Input"><a href="#3-1、Input" class="headerlink" title="3.1、Input"></a>3.1、Input</h2><ol><li><p>监听文件(File)</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    file &#123;</div><div class="line">        path =&gt; ["/opt/modules/logstash-6.6.2/data_test/message"]</div><div class="line">        type =&gt; "system"</div><div class="line">        start_position =&gt; "beginning"</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">output&#123;stdout&#123;codec=&gt;rubydebug&#125;&#125;</div></pre></td></tr></table></figure><p>有一些比较有用的配置项，可以用来指定 FileWatch 库的行为：</p><ul><li><p>discover_interval</p><p>logstash 每隔多久去检查一次被监听的 path 下是否有新文件。默认值是 15 秒。</p></li><li><p>exclude</p><p>不想被监听的文件可以排除出去，这里跟 path 一样支持 glob 展开。</p></li><li><p>close_older</p><p>一个已经监听中的文件，如果超过这个值的时间内没有更新内容，就关闭监听它的文件句柄。默认是 3600 秒，即一小时 </p></li><li><p>ignore_older</p><p>在每次检查文件列表的时候，如果一个文件的最后修改时间超过这个值，就忽略这个文件。默认是 86400 秒，即一天。</p></li><li><p>sincedb_path</p><p>如果你不想用默认的 $HOME/.sincedb(Windows 平台上在 C:\Windows\System32\config\systemprofile.sincedb)，可以通过这个配置定义 sincedb 文件到其他位置。 </p></li><li><p>sincedb_write_interval</p><p>logstash 每隔多久写一次 sincedb 文件，默认是 15 秒。</p></li><li><p>stat_interval</p><p>logstash 每隔多久检查一次被监听文件状态（是否有更新），默认是 1 秒。</p></li><li><p>start_position</p><p>logstash 从什么位置开始读取文件数据，默认是结束位置，也就是说 logstash 进程会以类似 tail -F 的形式运行。如果你是要导入原有数据，把这个设定改成 “beginning”，logstash 进程就从头开始读取，类似 less +F 的形式运行。</p></li></ul></li><li><p>标准输入/输出(Stdin)</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    stdin &#123;</div><div class="line">        add_field =&gt; &#123;"key" =&gt; "value"&#125;</div><div class="line">        codec =&gt; "plain"</div><div class="line">        tags =&gt; ["add"]</div><div class="line">        type =&gt; "std"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output&#123;stdout&#123;codec=&gt;rubydebug&#125;&#125;</div></pre></td></tr></table></figure><p>​    type 和 tags 是 logstash 事件中两个特殊的字段。通常来说我们会在输入区段中通过 type 来标记事件类型。而 tags 则是在数据处理过程中，由具体的插件来添加或者删除的。</p><p>常常有以下用法：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    stdin &#123;</div><div class="line">        type =&gt; "web"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">filter &#123;</div><div class="line">    if [type] == "web" &#123;</div><div class="line">        grok &#123;</div><div class="line">            match =&gt; ["message", %&#123;COMBINEDAPACHELOG&#125;]</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line">    if "_grokparsefailure" in [tags] &#123;</div><div class="line">        nagios_nsca &#123;</div><div class="line">            nagios_status =&gt; "1"</div><div class="line">        &#125;</div><div class="line">    &#125; else &#123;</div><div class="line">        elasticsearch &#123;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ol><h2 id="3-2、Codec"><a href="#3-2、Codec" class="headerlink" title="3.2、Codec"></a>3.2、Codec</h2><p>​    Codec 是 logstash 从 1.3.0 版开始新引入的概念(Codec 来自 Coder/decoder 两个单词的首字母缩写)。</p><p>在此之前，logstash 只支持纯文本形式输入，然后以过滤器处理它。但现在，我们可以在输入期处理不同类型的数据，这全是因为有了 codec 设置。所以，这里需要纠正之前的一个概念。Logstash 不只是一个input | filter | output 的数据流，而是一个 <strong>input | decode | filter | encode | output</strong> 的数据流！codec 就是用来 decode、encode 事件的。codec 的引入，使得 logstash 可以更好更方便的与其他有自定义数据格式的运维产品共存，比如 graphite、fluent、netflow、collectd，以及使用 msgpack、json、edn 等通用数据格式的其他产品等。</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    stdin &#123;</div><div class="line">        add_field =&gt; &#123;"key" =&gt; "value"&#125;</div><div class="line">        codec =&gt; "json"</div><div class="line">        type =&gt; "std"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output&#123;stdout&#123;codec=&gt;rubydebug&#125;&#125;</div></pre></td></tr></table></figure><p>输入测试</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;<span class="attr">"simCar"</span>:<span class="number">18074045598</span>,<span class="attr">"validityPeriod"</span>:<span class="string">"1996-12-06"</span>,<span class="attr">"unitPrice"</span>:<span class="number">9</span>,<span class="attr">"quantity"</span>:<span class="number">19</span>,<span class="attr">"amount"</span>:<span class="number">35</span>,<span class="attr">"imei"</span>:<span class="number">887540376467915</span>,<span class="attr">"user"</span>:<span class="string">"test"</span>&#125;</div></pre></td></tr></table></figure><p>输出如下</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">        "@timestamp" =&gt; 2019-03-21T07:11:22.457Z,</div><div class="line">              "imei" =&gt; 887540376467915,</div><div class="line">    "validityPeriod" =&gt; "1996-12-06",</div><div class="line">              "user" =&gt; "test",</div><div class="line">              "host" =&gt; "node01",</div><div class="line">            "amount" =&gt; 35,</div><div class="line">               "key" =&gt; "value",</div><div class="line">            "simCar" =&gt; 18074045598,</div><div class="line">         "unitPrice" =&gt; 9,</div><div class="line">          "quantity" =&gt; 19,</div><div class="line">              "type" =&gt; "std",</div><div class="line">          "@version" =&gt; "1"</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="3-3、Filter"><a href="#3-3、Filter" class="headerlink" title="3.3、Filter"></a>3.3、Filter</h2><h3 id="3-3-1、Grok插件"><a href="#3-3-1、Grok插件" class="headerlink" title="3.3.1、Grok插件"></a>3.3.1、Grok插件</h3><p>​    logstash拥有丰富的filter插件,它们扩展了进入过滤器的原始数据，进行复杂的逻辑处理，甚至可以无中生有的添加新的 logstash 事件到后续的流程中去！Grok 是 Logstash 最重要的插件之一。也是迄今为止使蹩脚的、无结构的日志结构化和可查询的最好方式。Grok在解析 syslog logs、apache and other webserver logs、mysql logs等任意格式的文件上表现完美。</p><p>​    这个工具非常适用于系统日志，Apache和其他网络服务器日志，MySQL日志等。</p><p>测试：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    stdin &#123;</div><div class="line">        type =&gt; "std"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">filter &#123;</div><div class="line">  grok &#123;</div><div class="line">    match=&gt;&#123;"message"=&gt; "%&#123;IP:client&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125;" &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">output&#123;stdout&#123;codec=&gt;rubydebug&#125;&#125;</div></pre></td></tr></table></figure><p>输入：</p><p><code>55.3.244.1 GET /index.html 15824 0.043</code></p><p>输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    "@timestamp" =&gt; 2019-03-21T07:18:44.268Z,</div><div class="line">       "request" =&gt; "/index.html",</div><div class="line">         "bytes" =&gt; "15824",</div><div class="line">      "duration" =&gt; "0.043",</div><div class="line">          "host" =&gt; "node01",</div><div class="line">        "method" =&gt; "GET",</div><div class="line">        "client" =&gt; "55.3.244.1",</div><div class="line">       "message" =&gt; "55.3.244.1 GET /index.html 15824 0.043",</div><div class="line">      "@version" =&gt; "1",</div><div class="line">          "type" =&gt; "std"</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="3-3-2、Grok语法"><a href="#3-3-2、Grok语法" class="headerlink" title="3.3.2、Grok语法"></a>3.3.2、Grok语法</h3><p>grok模式的语法如下：</p><p><code>%{SYNTAX:SEMANTIC}</code></p><p><strong>SYNTAX</strong>：代表匹配值的类型,例如3.44可以用NUMBER类型所匹配,127.0.0.1可以使用IP类型匹配。</p><p><strong>SEMANTIC</strong>：代表存储该值的一个变量名称,例如 3.44 可能是一个事件的持续时间,127.0.0.1可能是请求的client地址。所以这两个值可以用 %{NUMBER:duration} %{IP:client} 来匹配。</p><p>​    你也可以选择将数据类型转换添加到Grok模式。默认情况下，所有语义都保存为字符串。如果您希望转换语义的数据类型，例如将字符串更改为整数，则将其后缀为目标数据类型。例如%{NUMBER:num:int}将num语义从一个字符串转换为一个整数。目前唯一支持的转换是int和float。</p><p>​    Logstash附带约120个模式。你可以在这里找到它们<a href="https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns" target="_blank" rel="external">https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns</a></p><h3 id="3-3-3、自定义Grok模式"><a href="#3-3-3、自定义Grok模式" class="headerlink" title="3.3.3、自定义Grok模式"></a>3.3.3、自定义Grok模式</h3><p>​    更多时候logstash grok没办法提供你所需要的匹配类型，这个时候我们可以使用自定义。</p><p>创建自定义 patterns 文件。</p><ol><li><p>创建一个名为patterns其中创建一个文件postfix （文件名无关紧要,随便起）,在该文件中，将需要的模式写为模式名称，空格，然后是该模式的正则表达式。例如：</p><p><code>POSTFIX_QUEUEID [0-9A-F]{10,11}</code></p></li><li><p>然后使用这个插件中的patterns_dir设置告诉logstash目录是你的自定义模式。</p></li></ol><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    stdin &#123;</div><div class="line">        type =&gt; "std"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">filter &#123;</div><div class="line">  grok &#123;</div><div class="line">    patterns_dir =&gt; ["./patterns"]</div><div class="line">    match =&gt; &#123; "message" =&gt; "%&#123;SYSLOGBASE&#125; %&#123;POSTFIX_QUEUEID:queue_id&#125;: %&#123;GREEDYDATA:syslog_message&#125;" &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">output&#123;stdout&#123;codec=&gt;rubydebug&#125;&#125;</div></pre></td></tr></table></figure><p>输入：</p><p><code>Jan 1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965:message-id=&lt;20130101142543.5828399CCAF@mailserver1</code></p><p>输出</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">          "queue_id" =&gt; "BEF25A72965",</div><div class="line">           "message" =&gt; "Jan  1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965: message-id=&lt;20130101142543.5828399CCAF@mailserver1",</div><div class="line">               "pid" =&gt; "21403",</div><div class="line">           "program" =&gt; "postfix/cleanup",</div><div class="line">          "@version" =&gt; "1",</div><div class="line">              "type" =&gt; "std",</div><div class="line">         "logsource" =&gt; "mailserver14",</div><div class="line">              "host" =&gt; "zzc-203",</div><div class="line">         "timestamp" =&gt; "Jan  1 06:25:43",</div><div class="line">    "syslog_message" =&gt; "message-id=&lt;20130101142543.5828399CCAF@mailserver1",</div><div class="line">        "@timestamp" =&gt; 2019-03-19T05:31:37.405Z</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="3-3-GeoIP"><a href="#3-3-GeoIP" class="headerlink" title="3.3.GeoIP"></a>3.3.GeoIP</h3><p>​    GeoIP 是最常见的免费 IP 地址归类查询库，同时也有收费版可以采购。GeoIP 库可以根据 IP 地址提供对应的地域信息，包括国别，省市，经纬度等，对于可视化地图和区域统计非常有用。</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    stdin &#123;</div><div class="line">        type =&gt; "std"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">filter &#123;</div><div class="line">    geoip &#123;</div><div class="line">        source =&gt; "message"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output&#123;stdout&#123;codec=&gt;rubydebug&#125;&#125;</div></pre></td></tr></table></figure><p>输入：</p><p><code>39.65.228.148</code></p><p>输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">       "message" =&gt; "39.65.228.148",</div><div class="line">          "host" =&gt; "node01",</div><div class="line">          "type" =&gt; "std",</div><div class="line">         "geoip" =&gt; &#123;</div><div class="line">              "location" =&gt; &#123;</div><div class="line">            "lon" =&gt; 116.9972,</div><div class="line">            "lat" =&gt; 36.6683</div><div class="line">        &#125;,</div><div class="line">                    "ip" =&gt; "39.65.228.148",</div><div class="line">         "country_code3" =&gt; "CN",</div><div class="line">             "longitude" =&gt; 116.9972,</div><div class="line">         "country_code2" =&gt; "CN",</div><div class="line">           "region_name" =&gt; "Shandong",</div><div class="line">           "region_code" =&gt; "37",</div><div class="line">              "timezone" =&gt; "Asia/Shanghai",</div><div class="line">          "country_name" =&gt; "China",</div><div class="line">              "latitude" =&gt; 36.6683,</div><div class="line">        "continent_code" =&gt; "AS",</div><div class="line">             "city_name" =&gt; "Jinan"</div><div class="line">    &#125;,</div><div class="line">    "@timestamp" =&gt; 2019-03-21T08:16:10.813Z,</div><div class="line">      "@version" =&gt; "1"</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="3-4、Output"><a href="#3-4、Output" class="headerlink" title="3.4、Output"></a>3.4、Output</h2><h3 id="3-4-1、保存成本地文件"><a href="#3-4-1、保存成本地文件" class="headerlink" title="3.4.1、保存成本地文件"></a>3.4.1、保存成本地文件</h3><p>配置</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    stdin &#123;</div><div class="line">        type =&gt; "std"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line">    file &#123;</div><div class="line">        path =&gt; "../data_test/%&#123;+yyyy&#125;/%&#123;+MM&#125;/%&#123;+dd&#125;/%&#123;host&#125;.log"</div><div class="line">        codec =&gt; line &#123; format =&gt; "custom format: %&#123;message&#125;"&#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>输入：</p><p><code>Gakki真漂亮</code></p><p>输出到：<code>/opt/modules/logstash-6.6.2/data_test/2019/03/21</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[sean@node01 21]$ cat node01.log </div><div class="line">custom format: Gakki真漂亮`</div></pre></td></tr></table></figure><h3 id="3-4-2、服务器间传文件"><a href="#3-4-2、服务器间传文件" class="headerlink" title="3.4.2、服务器间传文件"></a>3.4.2、服务器间传文件</h3><p>发送端</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">input&#123;</div><div class="line">    file &#123;</div><div class="line">        path =&gt; ["/opt/modules/logstash-6.6.2/data_test/send.log"]</div><div class="line">        type =&gt; "ecolog"</div><div class="line">        start_position =&gt; "beginning"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">filter &#123;</div><div class="line">    if [type] =~ /^ecolog/ &#123;</div><div class="line">        ruby &#123;</div><div class="line">            code =&gt; "file_name = event.get('path').split('/')[-1]</div><div class="line">                                         event.set('file_name',file_name)</div><div class="line">                                         event.set('servip','客户端IP')"</div><div class="line">        &#125;</div><div class="line">        mutate &#123;</div><div class="line">            rename =&gt; &#123;"file_name" =&gt; "filename"&#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line">        tcp &#123;</div><div class="line">        host  =&gt; "node01"</div><div class="line">        port  =&gt; 9600</div><div class="line">        codec =&gt; json_lines</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>接收端</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">  tcp &#123;</div><div class="line">        mode =&gt; "server"</div><div class="line">        port =&gt; 9600</div><div class="line">        ssl_enable =&gt; false</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">filter &#123;</div><div class="line">    json &#123;</div><div class="line">        source =&gt; "message"</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line">    file &#123;</div><div class="line">        path =&gt; "/opt/modules/logstash-6.6.2/data_test/%&#123;+YYYY-MM-dd&#125;/%&#123;servip&#125;-%&#123;filename&#125;"</div><div class="line">        codec =&gt; line &#123; format =&gt; "%&#123;message&#125;"&#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="3-4-3、写入到es"><a href="#3-4-3、写入到es" class="headerlink" title="3.4.3、写入到es"></a>3.4.3、写入到es</h3><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">input &#123;</div><div class="line">    file &#123;</div><div class="line">        path =&gt; ["/opt/modules/logstash-6.6.2/data_test/run_error.log"]</div><div class="line">        type =&gt; "error"</div><div class="line">        start_position =&gt; "beginning"</div><div class="line">&#125;</div><div class="line">&#125;</div><div class="line">output &#123;</div><div class="line">    elasticsearch &#123;</div><div class="line">        hosts =&gt; ["node01:9200"]</div><div class="line">        index =&gt; "logstash-%&#123;type&#125;-%&#123;+YYYY.MM.dd&#125;"</div><div class="line">        document_type =&gt; "%&#123;type&#125;"</div><div class="line">        sniffing =&gt; true</div><div class="line">        template_overwrite =&gt; true</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>sniffing ：寻找其他es节点，建议打开</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h1&gt;&lt;p&gt;​    logstash是一个数据分析软件，主要目的是分析log日志。整一套软件可以当作一个MVC模型，logstash是
      
    
    </summary>
    
    
      <category term="Logtash" scheme="rabbitluluu.github.io/tags/Logtash/"/>
    
  </entry>
  
  <entry>
    <title>IK分词器</title>
    <link href="rabbitluluu.github.io/2019/03/22/IK%E5%88%86%E8%AF%8D%E5%99%A8/"/>
    <id>rabbitluluu.github.io/2019/03/22/IK分词器/</id>
    <published>2019-03-22T02:46:00.000Z</published>
    <updated>2019-03-22T02:50:29.320Z</updated>
    
    <content type="html"><![CDATA[<h1 id="IK分词器"><a href="#IK分词器" class="headerlink" title="IK分词器"></a>IK分词器</h1><h2 id="1、安装"><a href="#1、安装" class="headerlink" title="1、安装"></a>1、安装</h2><ol><li><p>下载</p><p><code>https://github.com/medcl/elasticsearch-analysis-ik</code>找到对应es的版本下载zip包</p></li><li><p>安装</p><ol><li><p>cp elasticsearch-analysis-ik-6.1.1.zip ./elasticsearch-6.1.1/plugins/</p><p>unzip elasticsearch-analysis-ik-6.1.1.zip -d ik-analyzer</p><p>推荐使用</p></li><li><p>./elasticsearch-plugin<br>install <a href="https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.1.1/elasticsearch-analysis-ik-6.1.1.zip" target="_blank" rel="external">https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.1.1/elasticsearch-analysis-ik-6.1.1.zip</a></p><p>不推荐此法</p></li><li><p>elasticsearch-plugin install -f<br>file:///usr/local/elasticsearch-analysis-ik-6.1.1.zip</p></li></ol><font color="red"><strong>注意：不能有内层目录，否则启动报错</strong></font></li><li><p>测试</p><p><code>curl -XGET &#39;http://node01:9200/_analyze?pretty&amp;analyzer=standard&#39; -d &#39;Gakki真是一个可爱的女孩子&#39;</code></p></li></ol><h2 id="2、模式介绍"><a href="#2、模式介绍" class="headerlink" title="2、模式介绍"></a>2、模式介绍</h2><p><strong>ik_smart模式</strong></p><p>测试：</p><p><code>curl -XGET &#39;http://node01:9200/_analyze?pretty&amp;analyzer=ik_smart&#39; -d &#39;Gakki真是一个可爱的女孩子&#39;</code></p><p>结果：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"tokens"</span> : [</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"gakki"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">0</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">5</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"ENGLISH"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">0</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"真是"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">5</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">7</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">1</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"一个"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">7</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">9</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">2</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"可爱"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">9</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">11</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">3</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"的"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">11</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">12</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_CHAR"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">4</span></div><div class="line">    &#125;,</div><div class="line">&#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"女孩子"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">12</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">15</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">5</span></div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure><p><strong>ik_max_word模式</strong></p><p><code>curl -XGET &#39;http://node01:9200/_analyze?pretty&amp;analyzer=ik_max_word&#39; -d &#39;Gakki真是一个可爱的女孩子&#39;</code></p><p>结果：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">  <span class="attr">"tokens"</span> : [</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"gakki"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">0</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">5</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"ENGLISH"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">0</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"真是"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">5</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">7</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">1</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"一个"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">7</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">9</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">2</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"一"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">7</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">8</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"TYPE_CNUM"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">3</span></div><div class="line">    &#125;,</div><div class="line">&#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"个"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">8</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">9</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"COUNT"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">4</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"可爱"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">9</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">11</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">5</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"的"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">11</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">12</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_CHAR"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">6</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"女孩子"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">12</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">15</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">7</span></div><div class="line">    &#125;,</div><div class="line">    &#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"女孩"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">12</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">14</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">8</span></div><div class="line">    &#125;,</div><div class="line">&#123;</div><div class="line">      <span class="attr">"token"</span> : <span class="string">"孩子"</span>,</div><div class="line">      <span class="attr">"start_offset"</span> : <span class="number">13</span>,</div><div class="line">      <span class="attr">"end_offset"</span> : <span class="number">15</span>,</div><div class="line">      <span class="attr">"type"</span> : <span class="string">"CN_WORD"</span>,</div><div class="line">      <span class="attr">"position"</span> : <span class="number">9</span></div><div class="line">    &#125;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>区别：</p><p><code>ik_max_word</code>分词更为细腻，会重复使用句子中的字，将所有可能的结果都会分出来</p><p><code>ik_smart</code>则每个字只使用一次，不会重复使用</p><h2 id="3、API使用"><a href="#3、API使用" class="headerlink" title="3、API使用"></a>3、API使用</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;IK分词器&quot;&gt;&lt;a href=&quot;#IK分词器&quot; class=&quot;headerlink&quot; title=&quot;IK分词器&quot;&gt;&lt;/a&gt;IK分词器&lt;/h1&gt;&lt;h2 id=&quot;1、安装&quot;&gt;&lt;a href=&quot;#1、安装&quot; class=&quot;headerlink&quot; title=&quot;1、安装&quot;
      
    
    </summary>
    
    
      <category term="Elasticsearch" scheme="rabbitluluu.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>ELK之E--Elasticsearch的API使用</title>
    <link href="rabbitluluu.github.io/2019/03/22/ELK%E4%B9%8BE-Elasticsearch%E7%9A%84API%E4%BD%BF%E7%94%A8/"/>
    <id>rabbitluluu.github.io/2019/03/22/ELK之E-Elasticsearch的API使用/</id>
    <published>2019-03-22T02:45:50.000Z</published>
    <updated>2019-03-22T02:49:35.917Z</updated>
    
    <content type="html"><![CDATA[<p>package xyz.llsean.esTest;</p><p>import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;<br>import org.elasticsearch.action.delete.DeleteResponse;<br>import org.elasticsearch.action.get.GetResponse;<br>import org.elasticsearch.action.get.MultiGetItemResponse;<br>import org.elasticsearch.action.get.MultiGetResponse;<br>import org.elasticsearch.action.index.IndexRequest;<br>import org.elasticsearch.action.index.IndexResponse;<br>import org.elasticsearch.action.search.SearchResponse;<br>import org.elasticsearch.action.update.UpdateRequest;<br>import org.elasticsearch.action.update.UpdateResponse;<br>import org.elasticsearch.client.Requests;<br>import org.elasticsearch.client.transport.TransportClient;<br>import org.elasticsearch.common.settings.Settings;<br>import org.elasticsearch.common.transport.InetSocketTransportAddress;<br>import org.elasticsearch.common.xcontent.XContentBuilder;<br>import org.elasticsearch.common.xcontent.XContentFactory;<br>import org.elasticsearch.index.query.QueryBuilders;<br>import org.elasticsearch.search.SearchHit;<br>import org.elasticsearch.search.SearchHits;<br>import org.elasticsearch.transport.client.PreBuiltTransportClient;<br>import org.junit.Before;<br>import org.junit.Test;</p><p>import java.io.IOException;<br>import java.net.InetAddress;<br>import java.net.UnknownHostException;<br>import java.util.HashMap;<br>import java.util.Iterator;<br>import java.util.concurrent.ExecutionException;</p><p>/**</p><ul><li>@author: Sean</li><li>@date: 2019/3/19/12:52</li><li><p>@version: 1.0<br> */<br> public class esTest {<br> //声明客户端<br> private TransportClient client;</p><p> /**</p><ul><li><p>初始化连接<br> */<br> @Before<br> public void getClient() throws UnknownHostException {<br> //1.设置连接的集群名称<br> Settings settings = Settings.builder().</p><pre><code>put(&quot;cluster.name&quot;, &quot;my-application&quot;).build();</code></pre><p> //2.连接集群<br> client = new PreBuiltTransportClient(settings);<br> client.addTransportAddress(</p><pre><code>new InetSocketTransportAddress(        InetAddress.getByName(&quot;node01&quot;), 9300));</code></pre><p>}</p><p>/**</p></li><li><p>创建索引–名字只能用小写<br> */<br> @Test<br> public void createIndex() {<br> //1.创建索引<br> client.admin().indices().prepareCreate(“jojo”).get();</p><p> //关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>删除索引<br> */<br> @Test<br> public void deleteIndex() {<br> //1.删除索引<br> client.admin().indices().prepareDelete(“gakki”).get();</p><p> //2.关闭连接<br> client.close();<br> }</p><p>/**</p></li><li>新建文档–方式1：源数据json串</li><li><p>(不推荐)<br> */<br> @Test<br> public void createIndexByJson() {<br> //1.文档数据准备<br> String json = “{“ + “\”id\”:\”1\”,” + “\”title\”:\”基于Lucene的搜索服务器\”,”</p><pre><code>+ &quot;\&quot;content\&quot;:\&quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口\&quot;&quot; + &quot;}&quot;;</code></pre><p> //2.创建文档<br> IndexResponse indexResponse = client.prepareIndex(“gakki”, “article”, “1”)</p><pre><code>.setSource(json).execute().actionGet();</code></pre><p> //3.打印结果<br> System.out.println(“index:” + indexResponse.getIndex());<br> System.out.println(“type:” + indexResponse.getType());<br> System.out.println(“id:” + indexResponse.getId());<br> System.out.println(“version:” + indexResponse.getVersion());<br> System.out.println(“result:” + indexResponse.getResult());</p><p> //4.关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>创建文档–方式2：源数据map方式添加json<br> */<br> @Test<br> public void createIndexByMap() {<br> //1.文档数据准备<br> HashMap<string, object=""> json = new HashMap&lt;&gt;();<br> json.put(“id”, 2);<br> json.put(“title”, “基于Lucene的搜索服务器”);<br> json.put(“content”, “它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口”);</string,></p><p> //2.创建文档<br> IndexResponse indexResponse = client.prepareIndex(“gakki”, “article”, “2”).setSource(json).execute().actionGet();</p><p> // 3 打印返回的结果<br> System.out.println(“index:” + indexResponse.getIndex());<br> System.out.println(“type:” + indexResponse.getType());<br> System.out.println(“id:” + indexResponse.getId());<br> System.out.println(“version:” + indexResponse.getVersion());<br> System.out.println(“result:” + indexResponse.getResult());</p><p> // 4 关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>创建文档–方式3：源数据es构建器添加json<br> */<br> @Test<br> public void createIndexByEs() throws IOException {<br> //1.通过es自带的帮助类，构建json数据<br> XContentBuilder builder = XContentFactory.jsonBuilder().startObject()</p><pre><code>.field(&quot;id&quot;, 3).field(&quot;title&quot;, &quot;基于Lucene的搜索服务器&quot;).field(&quot;content&quot;, &quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。&quot;).endObject();</code></pre><p> // 2 创建文档<br> IndexResponse indexResponse = client.prepareIndex(“gakki”, “article”, “3”).setSource(builder).get();</p><p> // 3 打印返回的结果<br> System.out.println(“index:” + indexResponse.getIndex());<br> System.out.println(“type:” + indexResponse.getType());<br> System.out.println(“id:” + indexResponse.getId());<br> System.out.println(“version:” + indexResponse.getVersion());<br> System.out.println(“result:” + indexResponse.getResult());</p><p> // 4 关闭连接<br> client.close();</p><p>}</p><p>/**</p></li><li><p>查询一条信息<br> */<br> @Test<br> public void getData() {<br> //1.查询文档<br> GetResponse response = client.prepareGet(“gakki”, “article”, “2”).get();</p><p> //2.打印搜索的结果<br> System.out.println(response.getSourceAsString());</p><p> //3.关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>查询多条信息<br> */<br> @Test<br> public void getMultiData() {</p><p> //1.查询多个文档<br> MultiGetResponse responses = client.prepareMultiGet()</p><pre><code>.add(&quot;gakki&quot;, &quot;article&quot;, &quot;3&quot;).add(&quot;gakki&quot;, &quot;article&quot;, &quot;1&quot;, &quot;2&quot;).get();</code></pre><p> //2.遍历返回结果<br> for (MultiGetItemResponse m : responses) {</p><pre><code>GetResponse response = m.getResponse();//判断结果是否存在if (response.isExists()) {    String sourceAsString = response.getSourceAsString();    System.out.println(sourceAsString);}</code></pre><p> }<br> }</p><p>/**</p></li><li><p>更新数据<br> */<br> @Test<br> public void updateData() throws ExecutionException, InterruptedException, IOException {</p><p> //1.创建更新数据的请求对象<br> UpdateRequest updateRequest = new UpdateRequest();<br> updateRequest.index(“gakki”);<br> updateRequest.type(“article”);<br> updateRequest.id(“3”);</p><p> //设置json字符串–法1–使用map集合<br> HashMap<string, object=""> json1 = new HashMap&lt;&gt;();<br> json1.put(“title”, “基于Lucene的搜索服务器”);<br> json1.put(“content”, “它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。大数据前景无限”);<br> json1.put(“createDate”, “2019-3-21”);</string,></p><p> //设置json字符串–法2–使用自带类<br> XContentBuilder json2 = XContentFactory.jsonBuilder().startObject()</p><pre><code>.field(&quot;title&quot;, &quot;基于Lucene的搜索服务器&quot;).field(&quot;content&quot;, &quot;它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。大数据前景无限,并且薪资水平高&quot;).field(&quot;createDate&quot;, &quot;2019-3-22&quot;).endObject();</code></pre><p> //添加<br> //updateRequest.doc(json1);<br> updateRequest.doc(json2);</p><p> //2.获取更新后的值<br> UpdateResponse indexResponse = client.update(updateRequest).get();</p><p> //3.打印结果<br> System.out.println(“index:” + indexResponse.getIndex());<br> System.out.println(“type:” + indexResponse.getType());<br> System.out.println(“id:” + indexResponse.getId());<br> System.out.println(“version:” + indexResponse.getVersion());<br> System.out.println(“create:” + indexResponse.getResult());<br> }</p><p>/**</p></li><li><p>更新数据，倘若不存在则添加<br> */<br> @Test<br> public void testUpdata() throws IOException, ExecutionException, InterruptedException {</p><p> //准备json<br> XContentBuilder json1 = XContentFactory.jsonBuilder().startObject()</p><pre><code>.field(&quot;title&quot;, &quot;搜索服务器&quot;).field(&quot;content&quot;,        &quot;它提供了一个分布式多用户能力的全文搜索引擎，&quot; +                &quot;基于RESTful web接口。Elasticsearch是用Java开发的，&quot; +                &quot;并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。&quot; +                &quot;设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。&quot;).endObject();</code></pre><p> //1.设置查询条件<br> IndexRequest indexRequest = new IndexRequest(“gakki”, “article”, “5”).source(json1);</p><p> XContentBuilder json2 = XContentFactory.jsonBuilder().startObject()</p><pre><code>.field(&quot;user&quot;, &quot;sean&quot;).endObject();</code></pre><p> //2.设置更新查找到更新下面的设置<br> UpdateRequest upsert = new UpdateRequest(“gakki”, “article”, “5”)</p><pre><code>.doc(json2).upsert(indexRequest);</code></pre><p> client.update(upsert).get();<br> client.close();<br> }</p><p>/**</p></li><li><p>删除数据<br> */<br> @Test<br> public void deleeteData() {</p><p> //1.删除文档数据<br> DeleteResponse indexResponse = client.prepareDelete(“gakki”, “article”, “5”).get();</p><p> //2.打印结果<br> // 2 打印返回的结果<br> System.out.println(“index:” + indexResponse.getIndex());<br> System.out.println(“type:” + indexResponse.getType());<br> System.out.println(“id:” + indexResponse.getId());<br> System.out.println(“version:” + indexResponse.getVersion());<br> System.out.println(“found:” + indexResponse.getResult());</p><p> // 3 关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>全表查询–不推荐<br> */<br> @Test<br> public void matchAllQuery() {</p><p> //1.执行查询<br> SearchResponse searchResponse = client.prepareSearch(“gakki”)</p><pre><code>.setTypes(&quot;article&quot;).setQuery(QueryBuilders.matchAllQuery()).get();</code></pre><p> //2.打印查询结果<br> SearchHits hits = searchResponse.getHits();//获取命中次数<br> System.out.println(“查询结果有：” + hits.getTotalHits());</p><p> for (SearchHit hit : hits) {</p><pre><code>//打印信息System.out.println(hit.getSourceAsString());</code></pre><p> }</p><p> //关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>条件查询<br> */<br> @Test<br> public void query() {</p><p> //1.条件查询<br> SearchResponse searchResponse = client.prepareSearch(“gakki”)</p><pre><code>.setTypes(&quot;article&quot;).setQuery(QueryBuilders.queryStringQuery(&quot;大数据&quot;)).get();</code></pre><p> //2.打印查询结果<br> SearchHits hits = searchResponse.getHits();<br> System.out.println(“共有” + hits.getTotalHits() + “条”);</p><p> for (SearchHit hit : hits) {</p><pre><code>System.out.println(hit.getSourceAsString());</code></pre><p> }</p><p> //3.关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>通配符查询<br> */<br> @Test<br> public void wildcardQuery() {</p><p> //1.通配符查询<br> SearchResponse searchResponse = client.prepareSearch(“gakki”)</p><pre><code>.setTypes(&quot;article&quot;).setQuery(QueryBuilders.wildcardQuery(&quot;content&quot;, &quot;*全*&quot;)).get();</code></pre><p> //2.打印查询结果<br> SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象<br> System.out.println(“查询结果有：” + hits.getTotalHits() + “条”);</p><p> for (SearchHit hit : hits) {</p><pre><code>System.out.println(hit.getSourceAsString());// 打印出每条结果</code></pre><p> }</p><p> // 3 关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>精准查询<br> */<br> @Test<br> public void termQuery() {</p><p> //1.设置查询<br> SearchResponse searchResponse = client.prepareSearch(“gakki”)</p><pre><code>.setTypes(&quot;article&quot;).setQuery(QueryBuilders.termQuery(&quot;content&quot;, &quot;全文&quot;)).get();</code></pre><p> //2.打印信息<br> SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象<br> System.out.println(“查询结果有：” + hits.getTotalHits() + “条”);</p><p> for (SearchHit hit : hits) {</p><pre><code>System.out.println(hit.getSourceAsString());// 打印出每条结果</code></pre><p> }</p><p> // 3 关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>模糊查询<br> */<br> @Test<br> public void fuzzy() {<br> SearchResponse searchResponse = client.prepareSearch(“gakki”)</p><pre><code>.setTypes(&quot;article&quot;).setQuery(QueryBuilders.fuzzyQuery(&quot;title&quot;, &quot;lucene&quot;)).get();</code></pre><p> // 2 打印查询结果<br> SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象<br> System.out.println(“查询结果有：” + hits.getTotalHits() + “条”);</p><p> Iterator<searchhit> iterator = hits.iterator();</searchhit></p><p> while (iterator.hasNext()) {</p><pre><code>SearchHit searchHit = iterator.next(); // 每个查询对象System.out.println(searchHit.getSourceAsString()); // 获取字符串格式打印</code></pre><p> }</p><p> // 3 关闭连接<br> client.close();<br> }</p><p>/**</p></li><li><p>创建mapping<br> */<br> @Test<br> public void createMapping() throws Exception {</p><p> // 1设置mapping<br> XContentBuilder builder = XContentFactory.jsonBuilder().startObject().startObject(“article”)</p><pre><code>.startObject(&quot;properties&quot;).startObject(&quot;id1&quot;).field(&quot;type&quot;, &quot;text&quot;).field(&quot;store&quot;, &quot;true&quot;).endObject().startObject(&quot;title2&quot;).field(&quot;type&quot;, &quot;text&quot;).field(&quot;store&quot;, &quot;false&quot;).endObject().startObject(&quot;content&quot;).field(&quot;type&quot;, &quot;text&quot;).field(&quot;store&quot;, &quot;true&quot;).endObject().endObject().endObject().endObject();</code></pre><p> // 2 添加mapping<br> PutMappingRequest mapping = Requests.putMappingRequest(“jojo”).type(“article”).source(builder);</p><p> client.admin().indices().putMapping(mapping).get();</p><p> // 3 关闭资源<br> client.close();<br> }<br> }</p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;package xyz.llsean.esTest;&lt;/p&gt;
&lt;p&gt;import org.elasticsearch.action.admin.indices.mapping.put.PutMappingRequest;&lt;br&gt;import org.elasticsearc
      
    
    </summary>
    
    
      <category term="Elasticsearch" scheme="rabbitluluu.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>ELK之E--Elasticsearch安装和配置</title>
    <link href="rabbitluluu.github.io/2019/03/22/ELK%E4%B9%8BE-Elasticsearch%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/"/>
    <id>rabbitluluu.github.io/2019/03/22/ELK之E-Elasticsearch安装和配置/</id>
    <published>2019-03-22T02:45:26.000Z</published>
    <updated>2019-03-22T02:48:57.084Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>​    Elasticsearch，基于lucene，隐藏复杂性，提供简单易用的restful api接口、java api接口（还有其他语言的api接口）。</p><p>​    关于elasticsearch的一个传说，有一个程序员失业了，陪着自己老婆去英国伦敦学习厨师课程。程序员在失业期间想给老婆写一个菜谱搜索引擎，觉得lucene实在太复杂了，就开发了一个封装了lucene的开源项目，compass。后来程序员找到了工作，是做分布式的高性能项目的，觉得compass不够，就写了elasticsearch，让lucene变成分布式的系统。</p><p>​    Elasticsearch是一个实时分布式搜索和分析引擎。它用于全文搜索、结构化搜索、分析。</p><h1 id="2、概念区分"><a href="#2、概念区分" class="headerlink" title="2、概念区分"></a>2、概念区分</h1><ol><li><p>近实时</p><p>近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级。</p></li><li><p>Cluster（集群）</p><p>集群包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。</p></li><li><p>Node（节点）</p><p>集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为“elasticsearch”的集群，如果直接启动一堆节点，那么它们会自动组成一个elasticsearch集群，当然一个节点也可以组成一个elasticsearch集群。</p></li><li><p>Index（索引-数据库）</p><p>索引包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document。</p></li><li><p>Type（类型-表）</p><p>每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户数据type，博客数据type，评论数据type。</p></li><li><p>Document（文档-行）</p><p>文档是es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。</p></li><li><p>Field（字段-列）</p><p>Field是Elasticsearch的最小单位。一个document里面有多个field，每个field就是一个数据字段</p></li><li><p>mapping（映射-约束）</p><p>数据如何存放到索引对象上，需要有一个映射配置，包括：数据类型、是否存储、是否分词等。</p></li></ol><p><strong>elasticsearch与数据库的类比:</strong></p><table><thead><tr><th style="text-align:center">关系型数据库（比如Mysql）</th><th style="text-align:center">非关系型数据库（Elasticsearch）</th></tr></thead><tbody><tr><td style="text-align:center">数据库Database</td><td style="text-align:center">索引Index</td></tr><tr><td style="text-align:center">表Table</td><td style="text-align:center">类型Type</td></tr><tr><td style="text-align:center">数据行Row</td><td style="text-align:center">文档Document</td></tr><tr><td style="text-align:center">数据列Column</td><td style="text-align:center">字段Field</td></tr><tr><td style="text-align:center">约束 Schema</td><td style="text-align:center">映射Mapping</td></tr></tbody></table><h1 id="3、存数据和搜索机制"><a href="#3、存数据和搜索机制" class="headerlink" title="3、存数据和搜索机制"></a>3、存数据和搜索机制</h1><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/ES%E5%AD%98%E6%95%B0%E6%8D%AE%E5%92%8C%E6%90%9C%E7%B4%A2%E6%9C%BA%E5%88%B6.png" alt=""></p><h1 id="4、安装"><a href="#4、安装" class="headerlink" title="4、安装"></a>4、安装</h1><p><strong>单节点安装：</strong></p><ol><li><p>下载</p></li><li><p>解压</p></li><li><p>修改配置文件<code>elasticsearch.yml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"># ---------------------------------- Cluster -------------------------------------</div><div class="line">cluster.name: my-application</div><div class="line"># ------------------------------------ Node --------------------------------------</div><div class="line">node.name: node-1</div><div class="line"># ----------------------------------- Paths ---------------------------------------</div><div class="line">path.data: /opt/modules/elasticsearch-5.6.1/data</div><div class="line">path.logs: /opt/modules/elasticsearch-5.6.1/logs</div><div class="line"># ----------------------------------- Memory -----------------------------------</div><div class="line">bootstrap.memory_lock: false</div><div class="line">bootstrap.system_call_filter: false</div><div class="line"># ---------------------------------- Network ------------------------------------</div><div class="line">network.host: 192.168.10.201 </div><div class="line"># --------------------------------- Discovery ------------------------------------</div><div class="line">discovery.zen.ping.unicast.hosts: [&quot;node01&quot;]</div></pre></td></tr></table></figure><p>注意事项：</p><ol><li>cluster.name</li></ol><p>如果要配置集群需要两个节点上的elasticsearch配置的cluster.name相同，都启动可以自动组成集群，这里如果不改cluster.name则默认是cluster.name=my-application，</p><ol><li>nodename随意取但是集群内的各节点不能相同</li><li>修改后的每行前面不能有空格，修改后的“：”后面必须有一个空格</li></ol></li><li><p>修改服务器相关配置</p><ol><li><p><code>/etc/security/limits.conf</code>中添加如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">* soft nofile 65536</div><div class="line">* hard nofile 131072</div><div class="line">* soft nproc 4096</div><div class="line">* hard nproc 4096</div></pre></td></tr></table></figure></li><li><p><code>/etc/security/limits.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">* soft nproc 4096</div></pre></td></tr></table></figure></li><li><p><code>/etc/sysctl.conf</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vm.max_map_count=655360</div></pre></td></tr></table></figure><p>添加完后运行<code>sudo sysctl -p</code>查看是否生效</p></li></ol></li><li><p>安装web监控端</p><p>谷歌浏览器添加<code>elasticsearch-head.crx</code>即可</p></li><li><p>启动</p><p><code>bin/elasticsearch</code></p></li></ol><p><strong>集群安装：</strong></p><p>将安装包scp到其他机器</p><p>再在主节点配置中添加如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">node.master: true</div><div class="line">node.data: true</div></pre></td></tr></table></figure><p>其他节点配置中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">node.master: false</div><div class="line">node.data: true</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h1&gt;&lt;p&gt;​    Elasticsearch，基于lucene，隐藏复杂性，提供简单易用的restful api接口、java ap
      
    
    </summary>
    
    
      <category term="Elasticsearch" scheme="rabbitluluu.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Kafka的API及其使用</title>
    <link href="rabbitluluu.github.io/2019/03/22/Kafka%E7%9A%84API%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8/"/>
    <id>rabbitluluu.github.io/2019/03/22/Kafka的API及其使用/</id>
    <published>2019-03-22T02:45:13.000Z</published>
    <updated>2019-03-22T02:48:00.678Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、命令行操作"><a href="#1、命令行操作" class="headerlink" title="1、命令行操作"></a>1、命令行操作</h1><ol><li><p>查看当前集群中已存在的主题topic<br><code>bin/kafka-topics.sh --zookeeper node01:2181 --list</code></p></li><li><p>创建topic<br><code>bin/kafka-topics.sh --zookeeper node01:2181 --create --replication-factor 3 --partitions 1 --topic gakki</code></p><p>–zookeeper 连接zk集群<br>–create 创建<br>–replication-factor 副本<br>–partitions 分区<br>–topic 主题名</p></li><li><p>删除主题<br><code>bin/kafka-topics.sh --zookeeper node01:2181 --delete --topic gakki</code></p></li><li><p>发送消息<br>生产者启动：<br><code>bin/kafka-console-producer.sh --broker-list node01:9092 --topic gakki</code><br>消费者启动：<br><code>bin/kafka-console-consumer.sh --bootstrap-server node01:9092 --topic gakki --from-beginning</code><br>5）查看主题详细信息<br><code>bin/kafka-topics.sh --zookeeper node01:2181 --describe --topic gakki</code></p></li></ol><h1 id="2、API操作"><a href="#2、API操作" class="headerlink" title="2、API操作"></a>2、API操作</h1><h2 id="2-1、Producer"><a href="#2-1、Producer" class="headerlink" title="2.1、Producer"></a>2.1、Producer</h2><p>写法一：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.kafka_producer;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.util.Properties;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/3/18/10:13</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Producer</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="comment">//1.配置生产者属性（指定多个参数）</span></div><div class="line">        Properties prpo = <span class="keyword">new</span> Properties();</div><div class="line"></div><div class="line">        <span class="comment">//参数配置</span></div><div class="line">        <span class="comment">//kafka节点的地址</span></div><div class="line">        prpo.put(<span class="string">"bootstrap.servers"</span>,<span class="string">"node01:9092"</span>);</div><div class="line">        </div><div class="line">        <span class="comment">/**</span></div><div class="line">         * 可选项</div><div class="line">         * */</div><div class="line">        <span class="comment">//发送消息是否等待应答</span></div><div class="line">        prpo.put(<span class="string">"acks"</span>,<span class="string">"all"</span>);</div><div class="line">        <span class="comment">//配置发送消息失败重试</span></div><div class="line">        prpo.put(<span class="string">"retries"</span>,<span class="string">"0"</span>);</div><div class="line">        <span class="comment">//配置批量处理消息大小</span></div><div class="line">        prpo.put(<span class="string">"batch.size"</span>,<span class="string">"10241"</span>);</div><div class="line">        <span class="comment">//配置批量处理数据延迟--ms</span></div><div class="line">        prpo.put(<span class="string">"linger.ms"</span>,<span class="string">"5"</span>);</div><div class="line">        <span class="comment">//配置内存缓冲大小</span></div><div class="line">        prpo.put(<span class="string">"buffer.memory"</span>,<span class="string">"123412"</span>);</div><div class="line">        </div><div class="line">        <span class="comment">//消息在发送前必须序列化</span></div><div class="line">        prpo.put(<span class="string">"key.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</div><div class="line">        prpo.put(<span class="string">"value.serializer"</span>,<span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</div><div class="line"></div><div class="line">        <span class="comment">//2.实例化producer</span></div><div class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(prpo);</div><div class="line"></div><div class="line">        <span class="comment">//3.发送消息</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">99</span>; i++) &#123;</div><div class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"jojo"</span>,<span class="string">"sean's love"</span> + i));</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//4.释放资源</span></div><div class="line">        producer.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>写法二：</p><p>接口回调，可以在idea控制台直接看到运行信息，便于管理者查看任务运行的状态</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.kafka_producer;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Callback;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.util.Properties;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/3/18/10:13</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> * 接口回调--用来监控当前的运行状态</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Producer2</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="comment">//1.配置生产者属性（指定多个参数）</span></div><div class="line">        Properties prpo = <span class="keyword">new</span> Properties();</div><div class="line"></div><div class="line">        <span class="comment">//参数配置</span></div><div class="line">        <span class="comment">//kafka节点的地址</span></div><div class="line">        prpo.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"node01:9092"</span>);</div><div class="line"></div><div class="line">        <span class="comment">/**</span></div><div class="line">         * 可选项</div><div class="line">         * */</div><div class="line">        <span class="comment">//发送消息是否等待应答</span></div><div class="line">        prpo.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</div><div class="line">        <span class="comment">//配置发送消息失败重试</span></div><div class="line">        prpo.put(<span class="string">"retries"</span>, <span class="string">"0"</span>);</div><div class="line">        <span class="comment">//配置批量处理消息大小</span></div><div class="line">        prpo.put(<span class="string">"batch.size"</span>, <span class="string">"10241"</span>);</div><div class="line">        <span class="comment">//配置批量处理数据延迟--ms</span></div><div class="line">        prpo.put(<span class="string">"linger.ms"</span>, <span class="string">"5"</span>);</div><div class="line">        <span class="comment">//配置内存缓冲大小</span></div><div class="line">        prpo.put(<span class="string">"buffer.memory"</span>, <span class="string">"123412"</span>);</div><div class="line"></div><div class="line">        <span class="comment">//消息在发送前必须序列化</span></div><div class="line">        prpo.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</div><div class="line">        prpo.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</div><div class="line">        <span class="comment">//加载分区逻辑</span></div><div class="line">        <span class="comment">//prpo.put("partitioner.class","xyz.llsean.kafka_producer.Partition");</span></div><div class="line"></div><div class="line">        <span class="comment">//2.实例化producer</span></div><div class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(prpo);</div><div class="line"></div><div class="line">        <span class="comment">//3.发送消息</span></div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">99</span>; i++) &#123;</div><div class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"llsean"</span>, <span class="string">"sean's love emmm"</span> + i),</div><div class="line">                    <span class="keyword">new</span> Callback() &#123;</div><div class="line">                        <span class="meta">@Override</span></div><div class="line">                        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata metadata, Exception e)</span> </span>&#123;</div><div class="line">                            <span class="comment">//如果有metadata不为null 拿到当前的数据偏移量和分区</span></div><div class="line">                            <span class="keyword">if</span> (metadata != <span class="keyword">null</span>) &#123;</div><div class="line">                                System.out.println(metadata.topic() + <span class="string">"-----"</span> +</div><div class="line">                                        metadata.offset() + <span class="string">"-----"</span> + metadata.partition());</div><div class="line">                            &#125;</div><div class="line">                        &#125;</div><div class="line">                    &#125;);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//4.释放资源</span></div><div class="line">        producer.close();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>设置加载自定义分区：</p><p>实现Partitioner接口，重写方法即可</p><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.kafka_producer;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Partitioner;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.common.Cluster;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.util.Map;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/3/18/10:56</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Partition</span> <span class="keyword">implements</span> <span class="title">Partitioner</span></span>&#123;</div><div class="line"></div><div class="line">    <span class="comment">//设置</span></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; map)</span> </span>&#123;</div><div class="line"></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//分区逻辑</span></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String s, Object o, <span class="keyword">byte</span>[] bytes, Object o1, <span class="keyword">byte</span>[] bytes1, Cluster cluster)</span> </span>&#123;</div><div class="line"></div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//释放资源</span></div><div class="line">    <span class="meta">@Override</span></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</div><div class="line"></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="2-2、Consumer"><a href="#2-2、Consumer" class="headerlink" title="2.2、Consumer"></a>2.2、Consumer</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.kafka_consumer;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</div><div class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.util.Arrays;</div><div class="line"><span class="keyword">import</span> java.util.Properties;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/3/18/11:16</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Consumer</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</div><div class="line">        <span class="comment">//1.配置消费者属性</span></div><div class="line">        Properties prpo = <span class="keyword">new</span> Properties();</div><div class="line"></div><div class="line">        <span class="comment">//配置属性</span></div><div class="line">        <span class="comment">//服务器地址</span></div><div class="line">        prpo.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"node01:9092"</span>);</div><div class="line">        <span class="comment">//配置消费者组</span></div><div class="line">        prpo.put(<span class="string">"group.id"</span>,<span class="string">"g1"</span>);</div><div class="line">        <span class="comment">//配置是否自动确认offset</span></div><div class="line">        prpo.put(<span class="string">"enable.auto.commit"</span>,<span class="string">"true"</span>);</div><div class="line">        <span class="comment">//序列化和反序列化</span></div><div class="line">        prpo.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</div><div class="line">        prpo.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</div><div class="line"></div><div class="line">        <span class="comment">//2.实例化消费者</span></div><div class="line">        <span class="keyword">final</span> KafkaConsumer consumer = <span class="keyword">new</span> KafkaConsumer&lt;String,String&gt;(prpo);</div><div class="line"></div><div class="line">        <span class="comment">//4.释放资源</span></div><div class="line">        Runtime.getRuntime().addShutdownHook(<span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</div><div class="line">            <span class="meta">@Override</span></div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</div><div class="line">                <span class="keyword">if</span> (consumer != <span class="keyword">null</span>)&#123;</div><div class="line">                    consumer.close();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;));</div><div class="line"></div><div class="line">        <span class="comment">//订阅消息主题</span></div><div class="line">        consumer.subscribe(Arrays.asList(<span class="string">"jojo"</span>));</div><div class="line"></div><div class="line">        <span class="comment">//3.拉取消息</span></div><div class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>)&#123;</div><div class="line">            ConsumerRecords&lt;String,String&gt; record = consumer.poll(<span class="number">500</span>);</div><div class="line">            <span class="comment">//遍历消息</span></div><div class="line">            <span class="keyword">for</span>(ConsumerRecord&lt;String,String&gt; c:record)&#123;</div><div class="line">                System.out.println(c.topic()+<span class="string">"---"</span>+c.value());</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、命令行操作&quot;&gt;&lt;a href=&quot;#1、命令行操作&quot; class=&quot;headerlink&quot; title=&quot;1、命令行操作&quot;&gt;&lt;/a&gt;1、命令行操作&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;查看当前集群中已存在的主题topic&lt;br&gt;&lt;code&gt;bin/kafka-top
      
    
    </summary>
    
    
      <category term="Kafka" scheme="rabbitluluu.github.io/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka概述和安装</title>
    <link href="rabbitluluu.github.io/2019/03/22/kafka%E6%A6%82%E8%BF%B0%E5%92%8C%E5%AE%89%E8%A3%85/"/>
    <id>rabbitluluu.github.io/2019/03/22/kafka概述和安装/</id>
    <published>2019-03-22T02:45:02.000Z</published>
    <updated>2019-03-22T02:48:25.908Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、kafka概述"><a href="#1、kafka概述" class="headerlink" title="1、kafka概述"></a>1、kafka概述</h1><h2 id="1-1、概述"><a href="#1-1、概述" class="headerlink" title="1.1、概述"></a>1.1、概述</h2><p>官网：<br><a href="http://kafka.apache.org/" target="_blank" rel="external">http://kafka.apache.org/</a><br>ApacheKafka®是一个分布式流媒体平台<br>流媒体平台有三个关键功能：</p><ul><li>发布和订阅记录流，类似于消息队列或企业消息传递系统。</li><li>以容错的持久方式存储记录流。</li><li>记录发生时处理流</li></ul><p>Kafka通常用于两大类应用：</p><ul><li>构建可在系统或应用程序之间可靠获取数据的实时流数据管道</li><li>构建转换或响应数据流的实时流应用程序</li></ul><p>在流计算中，kafka主要功能是用来缓存数据，storm可以通过消费kafka中的数据进行流计算。<br>是一套开源的消息系统，由scala写成。支持javaAPI的。<br>kafka最初由LinkedIn公司开发，2011年开源。<br>是一个<strong>分布式消息队列</strong>，kafka读消息保存采用Topic进行归类。</p><h2 id="1-2、角色组成"><a href="#1-2、角色组成" class="headerlink" title="1.2、角色组成"></a>1.2、角色组成</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">发送消息：Producer(生产者)</div><div class="line">接收消息：Consumer(消费者)</div></pre></td></tr></table></figure><h2 id="1-3、作用"><a href="#1-3、作用" class="headerlink" title="1.3、作用"></a>1.3、作用</h2><ul><li>解耦<br>为了避免出现问题，一个组件专业做一件事</li><li>拓展性<br>可增加处理过程</li><li>灵活<br>面对访问量剧增，不会因为超负荷请求而完全瘫痪。</li><li>可恢复<br>一部分组件失效，不会影响整个系统。可以进行恢复。</li><li>缓冲<br>控制数据流经过系统的速度。</li><li>顺序保证<br>对消息进行有序处理。</li><li>异步通信<br>akka,消息队列提供了异步处理的机制。允许用户把消息放到队列，不立刻处理。</li></ul><h1 id="2、架构"><a href="#2、架构" class="headerlink" title="2、架构"></a>2、架构</h1><h1 id="3、安装部署"><a href="#3、安装部署" class="headerlink" title="3、安装部署"></a>3、安装部署</h1><p>提前准备：搭建kafka集群要先搭建zookeeper集群</p><ol><li><p>官网下载安装包</p></li><li><p>上传安装包</p></li><li><p>解压安装包</p></li><li><p>重命名</p></li><li><p>修改配置文件<br><code>config/server.properties</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">broker.id=0 #每台机器的id不同即可</div><div class="line">delete.topic.enable=true #是否允许删除主题</div><div class="line">log.dirs=/root/hd/kafka/logs #运行日志保存位置</div><div class="line">zookeeper.connect=hd09-01:2181,hd09-02:2181,</div></pre></td></tr></table></figure></li><li><p>启动集群<br><code>bin/kafka-server-start.sh config/server.properties &amp;</code></p></li><li><p>关闭<br><code>bin/kafka-server-stop.sh</code></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、kafka概述&quot;&gt;&lt;a href=&quot;#1、kafka概述&quot; class=&quot;headerlink&quot; title=&quot;1、kafka概述&quot;&gt;&lt;/a&gt;1、kafka概述&lt;/h1&gt;&lt;h2 id=&quot;1-1、概述&quot;&gt;&lt;a href=&quot;#1-1、概述&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="kafka" scheme="rabbitluluu.github.io/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Hbase调优</title>
    <link href="rabbitluluu.github.io/2019/03/22/Hbase%E8%B0%83%E4%BC%98/"/>
    <id>rabbitluluu.github.io/2019/03/22/Hbase调优/</id>
    <published>2019-03-22T02:44:48.000Z</published>
    <updated>2019-03-22T02:47:33.648Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hbase优化技巧"><a href="#Hbase优化技巧" class="headerlink" title="Hbase优化技巧"></a>Hbase优化技巧</h1><ol><li><p>预分区问题</p><p>region分片的原因，表十分大</p><p>region存储数据，如果有多个region，每个region负责维护一部分的rowkey{startrowkey，endrowkey}</p><p>分多少片？提前规划，有效提高hbase性能，进行存储数据前做好rowkey的预分区优化hbase</p><p>实际操作：</p><p>​    <code>create &#39;user_pp&#39;,&#39;info&#39;,partition&#39;,SPLITS =&gt;[&#39;201&#39;,&#39;202&#39;,&#39;203&#39;,&#39;204&#39;]</code></p><p>添加上列族名，便于添加数据，下者同理</p><p>提前将分区信息写入文件，</p><p>​    <code>create &#39;user_ppp&#39;,&#39;partition&#39;,SPLITS_FILE=&gt;&#39;/root/temp/partitions.txt&#39;</code></p></li></ol><ol><li><p>rowkey如何设计？</p><p>rowkey是数据的唯一标识，这条数据存储在哪个分区由预分区范围决定</p><p>合理设计rowkey：一份数据分为5个region存储，但是我们需要尽可能的保证每个region中数据量差不多</p><p>尽可能的打散数据，平均分配到每个region中即可</p><p>解决方案：</p><p>​    生成随机数、Hash/散列值、字符串拼接/反转</p><p>如：原来的rowkey是201，hash后104526607，</p><p>​                    202，hash后104526606，</p><p>​    字符串拼接：20190316_dsa1  20190316_gdss</p><p>​    字符串反转：20190316-&gt;61309102</p></li></ol><ol><li><p>hbase基础优化</p><p>因为hbase用的hdfs存储，datanode允许最大文件打开数，默认是4096，将其<strong>调大</strong>，修改<code>hdfs-site.xml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dfs.datanode.max.transfer.threads</div></pre></td></tr></table></figure><p>优化等待时间,默认60000毫秒，将数值<strong>调大</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dfs.image.transfer.timeout</div></pre></td></tr></table></figure><p>内存优化：修改<code>hadoop-env.sh</code>设置内存的堆大小</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export HADOOP_PORTMAP_OPTS=&apos;-Xmx512m $HADOOP_PORTMAP_OPTS&apos;</div></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hbase优化技巧&quot;&gt;&lt;a href=&quot;#Hbase优化技巧&quot; class=&quot;headerlink&quot; title=&quot;Hbase优化技巧&quot;&gt;&lt;/a&gt;Hbase优化技巧&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;预分区问题&lt;/p&gt;
&lt;p&gt;region分片的原因，表十分大&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="Hbase" scheme="rabbitluluu.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hbase的API和MR实例</title>
    <link href="rabbitluluu.github.io/2019/03/22/Hbase%E7%9A%84API%E5%92%8CMR%E5%AE%9E%E4%BE%8B/"/>
    <id>rabbitluluu.github.io/2019/03/22/Hbase的API和MR实例/</id>
    <published>2019-03-22T02:44:34.000Z</published>
    <updated>2019-03-22T02:47:12.659Z</updated>
    
    <content type="html"><![CDATA[<p>package xyz.llsean.HbaseMR2_;</p><p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.hbase.HBaseConfiguration;<br>import org.apache.hadoop.hbase.client.Put;<br>import org.apache.hadoop.hbase.io.ImmutableBytesWritable;<br>import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.util.Tool;<br>import org.apache.hadoop.util.ToolRunner;</p><p>/**</p><ul><li>@author: Sean</li><li>@date: 2019/3/17/9:54</li><li><p>@version: 1.0<br> */<br> public class ReadHdfsDriver implements Tool{<br> private Configuration conf;</p><p> @Override<br> public void setConf(Configuration conf) {</p><pre><code>this.conf = HBaseConfiguration.create(conf);</code></pre><p> }</p><p> @Override<br> public Configuration getConf() {</p><pre><code>return this.conf;</code></pre><p> }</p><p> @Override<br> public int run(String[] args) throws Exception {</p><pre><code>//1.创建jobJob job = Job.getInstance(conf);job.setJarByClass(ReadHdfsDriver.class);//2.设置Mapperjob.setMapperClass(ReadHdfsMapper_.class);job.setMapOutputKeyClass(ImmutableBytesWritable.class);job.setMapOutputValueClass(Put.class);//3.设置ReducerTableMapReduceUtil.initTableReducerJob(&quot;user_hdfs&quot;,        WriterHdfsReducer_.class,        job);//4.输入配置FileInputFormat.addInputPath(job,new Path(&quot;/temp/love.txt&quot;));boolean rs = job.waitForCompletion(true);return rs?0:1;</code></pre><p> }</p><p> public static void main(String[] args) {</p><pre><code>try {    int run = ToolRunner.run(new ReadHdfsDriver(), args);    System.exit(run);} catch (Exception e) {    e.printStackTrace();}</code></pre><p> }<br> }</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;package xyz.llsean.HbaseMR2_;&lt;/p&gt;
&lt;p&gt;import org.apache.hadoop.conf.Configuration;&lt;br&gt;import org.apache.hadoop.fs.Path;&lt;br&gt;import org.apac
      
    
    </summary>
    
    
      <category term="Hbase" scheme="rabbitluluu.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hbase的Shell操作以及读写过程解析</title>
    <link href="rabbitluluu.github.io/2019/03/15/Hbase%E7%9A%84Shell%E6%93%8D%E4%BD%9C%E4%BB%A5%E5%8F%8A%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B%E8%A7%A3%E6%9E%90/"/>
    <id>rabbitluluu.github.io/2019/03/15/Hbase的Shell操作以及读写过程解析/</id>
    <published>2019-03-15T04:50:15.000Z</published>
    <updated>2019-03-15T04:50:50.331Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、基本操作"><a href="#1、基本操作" class="headerlink" title="1、基本操作"></a>1、基本操作</h1><ol><li><p>启动终端</p><p><code>bin/hbase shell</code></p></li><li><p>显示所有表</p><p><code>list</code></p></li><li><p>显示命令介绍</p><p><code>help</code></p></li><li><p>显示当前的服务器状态</p><p><code>status &#39;node01&#39;</code></p><p><code>1 active master, 0 backup masters, 4 servers, 0 dead, 0.5000 average load</code></p><p>1 active master：一个存活的master</p><p>0 backup masters：0个备份master</p><p>4 servers：4个regionserver</p><p>0 dead：0个宕机</p><p>0.5000 average load：平均加载</p></li><li><p>显示当前用户</p><p><code>whoami</code></p></li><li><p>创建表</p><p><code>create &#39;user&#39;,&#39;info1&#39;,&#39;info2&#39;</code></p></li><li><p>全表扫描</p><p><code>scan &#39;user&#39;</code></p></li><li><p>存入数据(put ‘表名’，’Rowkey’，’列族:列名’，’值’)</p><p><code>put &#39;user&#39;,&#39;1001&#39;,&#39;info1:name&#39;,&#39;sean&#39;</code></p><p><code>put &#39;user&#39;,&#39;1001&#39;,&#39;info1:age&#39;,&#39;22&#39;</code></p></li><li><p>覆盖(hbase中无修改)</p><p><code>put &#39;user&#39;,&#39;1001&#39;,&#39;info2:id&#39;,&#39;112&#39;</code></p><p>一步一步深入，对应上表名，Rowkey，列族，列即可</p></li><li><p>查看表结构</p><p><code>describe &#39;user&#39;</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Table user is ENABLED                                                                   </div><div class="line">user                                                                                             </div><div class="line">COLUMN FAMILIES DESCRIPTION                                                                       </div><div class="line">&#123;NAME =&gt; &apos;info1&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS </div><div class="line">=&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =</div><div class="line">&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;                      </div><div class="line">&#123;NAME =&gt; &apos;info2&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, VERSIONS =&gt; &apos;1&apos;, IN_MEMORY =&gt; &apos;false&apos;, KEEP_DELETED_CELLS </div><div class="line">=&gt; &apos;FALSE&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =</div><div class="line">&gt; &apos;0&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;&#125;                      </div><div class="line">2 row(s) in 0.1500 seconds</div></pre></td></tr></table></figure></li><li><p>变更表结构信息</p><p><code>alter &#39;user&#39;,{NAME =&gt; &#39;info1&#39;,VERSIONS =&gt; &#39;5&#39;}</code></p><p>需要指定列族，再指定到版本才能修改</p></li><li><p>过滤信息(get ‘表名’,’Rowkey’)</p><p><code>get &#39;user&#39;,&#39;1001&#39;</code></p><p><code>get &#39;user&#39;,&#39;1001&#39;,&#39;info2:id&#39;</code></p></li><li><p>清空数据</p><p><code>truncate &#39;user&#39;</code></p></li><li><p>删除表</p><p><code>drop &#39;user&#39;</code></p><p>但是返回：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ERROR: Table user is enabled. Disable it first.</div><div class="line"></div><div class="line">Here is some help for this command:</div><div class="line">Drop the named table. Table must first be disabled:</div><div class="line">  hbase&gt; drop &apos;t1&apos;</div><div class="line">  hbase&gt; drop &apos;ns1:t1&apos;</div></pre></td></tr></table></figure><p>正确步骤为：</p><p><code>disable &#39;user&#39;</code>：指定表不可用</p><p><code>drop &#39;user&#39;</code>：删除</p></li><li><p>扫描指定rowkey范围</p><p><code>scan &#39;user&#39;,{STARTROW=&gt;&#39;2202&#39;}</code></p><p>含头不含尾:</p><p><code>scan &#39;user&#39;,{STARTROW=&gt;&#39;2202&#39;,STOPROW=&gt;&#39;2202&#39;}</code></p></li><li><p>统计行数</p><p><code>count &#39;user&#39;</code></p></li></ol><h1 id="2、文件读写具体流程"><a href="#2、文件读写具体流程" class="headerlink" title="2、文件读写具体流程"></a>2、文件读写具体流程</h1><p>客户端访问zk,返回root表元数据位置，根据元数据信息去查找对应regionserver，根据root表去查找到META表，再根据meta表元数据查找region，返回元数据给客户端</p><h2 id="2-1、Hbase读取数据-总体"><a href="#2-1、Hbase读取数据-总体" class="headerlink" title="2.1、Hbase读取数据(总体)"></a>2.1、Hbase读取数据(总体)</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Hbase%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B%EF%BC%88%E6%80%BB%E4%BD%93%EF%BC%89.png" alt="流程"></p><p>过程梳理：</p><ol><li>客户端访问zk，发出请求</li><li>zk返回-ROOT表位置元数据信息</li><li>根据zk返回信息找到具体的HRegionserver，获取到-ROOT表</li><li>根据-ROOT表元数据信息找到.META表</li><li>根据.META表中存储的Region元数据信息找到具体的Region</li><li>返回所需要读取的信息</li></ol><h2 id="2-2、Hbase读取数据-局部"><a href="#2-2、Hbase读取数据-局部" class="headerlink" title="2.2、Hbase读取数据(局部)"></a>2.2、Hbase读取数据(局部)</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Hbase%E5%85%B7%E4%BD%93%E8%AF%BB%E5%8F%96%E6%B5%81%E7%A8%8B.png" alt="Hbase读取数据(局部)"></p><p>过程梳理：</p><ol><li>client从memstore中读取数据</li><li>如果所需要的数据恰好在memstore中，那么直接返回到客户端</li><li>memstore没有找到则会在blockcache中查找</li><li>倘若blockcache中还没找到，那么将通过hdfs客户端对Hfile进行查找</li><li>将读取到的数据缓存到blockcache并返回</li><li>数据返回客户端</li></ol><h2 id="2-3、Hbase写数据过程"><a href="#2-3、Hbase写数据过程" class="headerlink" title="2.3、Hbase写数据过程"></a>2.3、Hbase写数据过程</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/Hbase%E5%86%99%E6%95%B0%E6%8D%AE%E8%BF%87%E7%A8%8B.png" alt="写数据过程"></p><p>过程梳理：</p><ol><li>发出请求给zk</li><li>返回元数据信息</li><li>查找到对应的RegionServer(此处省略掉-ROOT，.META的嵌套查询过程)</li><li>找到对应的Region，同时将操作写入到Hlog</li><li>将数据写入到memstore中</li><li>满后溢写数据到StoreFile中，生成Hfile</li><li>hdfs客户端将Hfile写入到集群</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、基本操作&quot;&gt;&lt;a href=&quot;#1、基本操作&quot; class=&quot;headerlink&quot; title=&quot;1、基本操作&quot;&gt;&lt;/a&gt;1、基本操作&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;启动终端&lt;/p&gt;
&lt;p&gt;&lt;code&gt;bin/hbase shell&lt;/code&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Hbase" scheme="rabbitluluu.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Hbase概述和部署</title>
    <link href="rabbitluluu.github.io/2019/03/14/Hbase%E6%A6%82%E8%BF%B0%E5%92%8C%E9%83%A8%E7%BD%B2/"/>
    <id>rabbitluluu.github.io/2019/03/14/Hbase概述和部署/</id>
    <published>2019-03-14T09:27:57.000Z</published>
    <updated>2019-03-14T09:28:35.628Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p><a href="http://www.apache.org/" target="_blank" rel="external">Apache</a> HBase™是<a href="http://hadoop.apache.org/" target="_blank" rel="external">Hadoop</a>数据库，是一个分布式，可扩展的大数据存储。</p><p>​    当需要对大数据进行随机，实时读/写访问时，请使用Apache HBase™。该项目的目标是托管非常大的表 - 数十亿行X百万列 - 在商品硬件集群上。Apache HBase是一个开源的，分布式的，版本化的非关系数据库，模仿Google的<a href="http://research.google.com/archive/bigtable.html" target="_blank" rel="external">Bigtable：</a> Chang等人的<a href="http://research.google.com/archive/bigtable.html" target="_blank" rel="external">结构化数据分布式存储系统</a>。正如Bigtable利用Google文件系统提供的分布式数据存储一样，Apache HBase在Hadoop和HDFS之上提供类似Bigtable的功能。</p><h1 id="2、集群角色"><a href="#2、集群角色" class="headerlink" title="2、集群角色"></a>2、集群角色</h1><p>hdfs：Namenode + DataNode</p><p>yarn：ResourceManager + NodeManger</p><p>hbase：HMaser + Regionserver</p><p>HMaser作用：</p><ul><li>对Regionserver监控</li><li>处理一些元数据的变更</li><li>对Regionserver进行故障转移</li><li>空闲时对数据进行负载均衡</li><li>对region进行管理</li><li>发布位置到客户端</li></ul><p>Regionserver作用：</p><ul><li>存储hbase的实际数据</li><li>刷新缓存数据到hdfs</li><li>处理Region</li><li>可以进行压缩</li><li>对Hlog进行维护</li><li>对region进行分片</li></ul><h1 id="3、集群安装"><a href="#3、集群安装" class="headerlink" title="3、集群安装"></a>3、集群安装</h1><ol><li><p>安装好zookeeper集群</p></li><li><p>安装好hadoop集群</p></li><li><p>解压，修改配置<code>hbase-env.sh</code> 和 <code>hbase-site.xml</code></p><p>hbase-env.sh:</p><p>export JAVA_HOME=/opt/modules/jdk1.8.0_141</p><p>export HBASE_MANAGES_ZK=false</p><p>hbase-site.xml:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">   &lt;property&gt;</div><div class="line">&lt;name&gt;hbase.rootdir&lt;/name&gt;</div><div class="line">&lt;value&gt;hdfs://node01:9000/hbase&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">   </div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</div><div class="line">&lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">   </div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hbase.master.port&lt;/name&gt;</div><div class="line">&lt;value&gt;16000&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">   </div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</div><div class="line">&lt;value&gt;node01:2181,node02:2181,node03:2181,node04:2181&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">   </div><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</div><div class="line">&lt;value&gt;/opt/modules/zookeeper-3.4.10/zkData&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure><ol><li>拷贝core-site.xml 和 hdfs-site.xml 到conf下，或者制作软连接<br><code>ln -s /opt/modules/hadoop-2.8.4/etc/hadoop/core-site.xml</code></li></ol><p>​       <code>ln -s /opt/modules/hadoop-2.8.4/etc/hadoop/hdfs-site.xml</code></p><ol><li><p>拷贝安装包到其他机器</p></li><li><p>启动测试：</p><p>​    启动Hmaster：bin/hbase-daemon.sh start master</p><p>​    启动Regionserver：bin/hbase-daemon.sh start regionserver</p><p>操作后查看进程出现hmaster和regionserver，且在webUI（端口16010）中可以查看到节点即配置成功</p><font color="red">特别注意：hmaster启动指令中不叫Hmaster！！！！！！</font></li></ol><h1 id="4、集群架构解析"><a href="#4、集群架构解析" class="headerlink" title="4、集群架构解析"></a>4、集群架构解析</h1><p>架构图：</p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/%E5%9B%BE%E7%89%872%EF%BC%9AHbase%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt="hbase架构"></p></li></ol><p>名词解释：</p><p>​    HLog：预写入日志，记录hbase修改记录，倘若系统故障，可以通过log文件进行重建，进行数据恢复。</p><p>​    HRegion：表的分片，根据Rowkey（行键）进行划分</p><p>​    Store：一个store对应hbase中的一个列族</p><p>​    StoreFile和HFile：HFile是真正的物理存储文件，StoreFile是逻辑概念</p><p>​    MemStore：接受来自HLog的数据，相当于内存，满了之后数据存到HFile，可以有效的提高查询效率</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;http://www.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;external
      
    
    </summary>
    
    
      <category term="Hbase" scheme="rabbitluluu.github.io/tags/Hbase/"/>
    
  </entry>
  
  <entry>
    <title>Azkaban概述和使用</title>
    <link href="rabbitluluu.github.io/2019/03/13/Azkaban%E6%A6%82%E8%BF%B0%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>rabbitluluu.github.io/2019/03/13/Azkaban概述和使用/</id>
    <published>2019-03-13T01:58:11.000Z</published>
    <updated>2019-03-13T02:09:22.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、-Azkaban概述"><a href="#1、-Azkaban概述" class="headerlink" title="1、 Azkaban概述"></a>1、 Azkaban概述</h1><p><strong>Azkaban</strong>是在LinkedIn上创建的批处理工作流作业调度程序，用于运行Hadoop作业。Azkaban通过作业<strong>依赖</strong>性解决订单，并提供易于使用的Web用户界面来维护和跟踪您的工作流程。</p><p>工作流作业：</p><p>flume-&gt;hdfs-&gt;mr-&gt;hive建表-&gt;导入load data脚本</p><p><strong>自动化</strong>调度</p><h1 id="2、-环境搭建"><a href="#2、-环境搭建" class="headerlink" title="2、 环境搭建"></a>2、 环境搭建</h1><ol><li>创建相关mysql库并导入脚本</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">create database azkaban;</div><div class="line">use azkaban;</div><div class="line">source /root/hd/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql;</div></pre></td></tr></table></figure><ol><li><p>生成证书</p><p><code>keytool -keystore keystore -alias jetty -genkey -keyalg RSA</code></p><p>牢记密码后续要用</p><p>并将其拷贝到server中</p></li><li><p>时间同步配置</p><p>开启发送指令到所有窗口，保证集群时间同步，任务调度，所以和本地时间保持一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo date -s &apos;当前时间&apos;</div><div class="line">hwclock -w</div></pre></td></tr></table></figure></li><li><p>修改server配置文件<code>azkaban.properties</code>和<code>azkaban-users.xml</code></p><p>前者修改mysql相关配置和时区</p><p>后者添加账户密码<code>&lt;user username=&quot;admin&quot; password=&quot;admin&quot; roles=&quot;admin,metrics&quot;/&gt;</code>权限为管理员</p></li><li><p>修改excutor端配置文件<code>azkaban.properties</code>时区设置和mysql相关配置</p></li></ol><h1 id="3、azkaban实例"><a href="#3、azkaban实例" class="headerlink" title="3、azkaban实例"></a>3、azkaban实例</h1><font color="red">注意1，启动时需在主目录下，否则会有如下错误</font><p><code>Exception in thread &quot;main&quot; java.io.FileNotFoundException: conf/global.properties (No such file or directory)</code></p><font color="red">注意2，打包只能打成zip包，否则会无法识别</font><font color="red">注意3，指令写一行，不换行</font><font color="red">注意4，脚本不识别环境变量，需要将其启动脚本的绝对路径写全</font><ol><li><p>实例1：启动yarn集群</p><p><code>type=commandcommand=/opt/modules/hadoop-2.8.4/sbin/start-yarn.sh</code></p></li><li><p>实例2：启动mr程序</p><p><code>type=commandcommand=/opt/modules/hadoop-2.8.4/bin/hadoop jar /opt/modules/hadoop-2.8.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.4.jar wordcount /input/wc /output/wc2</code></p></li><li><p>实例3：设置任务依赖</p><p><strong>job_a.job:</strong></p><p><code>type=commandcommand=echo &#39;gakki&#39;</code></p><p><strong>job_b.job:</strong></p><p><code>type=commanddependencies=job_a.jobcommand=echo &#39;is my love&#39;</code></p></li><li><p>实例4：创建hive表并从本地导入数据</p><p><strong>hive.sql：</strong></p><p><code>use default;create table azhive(id int, name string, int num) row format delimited fields terminated by &#39;\t&#39;;load data inpath &#39;/temp/dept.txt&#39; into table azhive;</code></p><p><strong>hive-f.job：</strong></p><p><code>type=commandcommand=//opt/modules/hive/bin/hive -f &quot;hive.sql&quot;</code></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、-Azkaban概述&quot;&gt;&lt;a href=&quot;#1、-Azkaban概述&quot; class=&quot;headerlink&quot; title=&quot;1、 Azkaban概述&quot;&gt;&lt;/a&gt;1、 Azkaban概述&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Azkaban&lt;/strong&gt;是在Link
      
    
    </summary>
    
    
      <category term="Azkaban" scheme="rabbitluluu.github.io/tags/Azkaban/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop概述和使用</title>
    <link href="rabbitluluu.github.io/2019/03/13/Sqoop%E6%A6%82%E8%BF%B0%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
    <id>rabbitluluu.github.io/2019/03/13/Sqoop概述和使用/</id>
    <published>2019-03-13T01:57:57.000Z</published>
    <updated>2019-03-13T01:58:43.499Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、sqoop概述"><a href="#1、sqoop概述" class="headerlink" title="1、sqoop概述"></a>1、sqoop概述</h1><p>flume数据采集 采集日志数据</p><p>sqoop数据迁移 hdfs–&gt;mysql</p><p>azkaban任务调度</p><p>流程：flume–&gt;hdfs–&gt;shell–&gt;hive–&gt;sql–&gt;BI</p><p>sqoop数据迁移==mapreduce，处理离线数据</p><p>整个过程就是数据导入处理导出过程，直接使用map清洗</p><p>方法介绍：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">codegen            Generate code to interact with database records</div><div class="line">create-hive-table  Import a table definition into Hive</div><div class="line">eval               Evaluate a SQL statement and display the results</div><div class="line">export             Export an HDFS directory to a database table</div><div class="line">help               List available commands</div><div class="line">import             Import a table from a database to HDFS</div><div class="line">import-all-tables  Import tables from a database to HDFS</div><div class="line">import-mainframe   Import datasets from a mainframe server to HDFS</div><div class="line">job                Work with saved jobs</div><div class="line">list-databases     List available databases on a server</div><div class="line">list-tables        List available tables in a database</div><div class="line">merge              Merge results of incremental imports</div><div class="line">metastore          Run a standalone Sqoop metastore</div><div class="line">version            Display version information</div></pre></td></tr></table></figure><h1 id="2、-安装部署"><a href="#2、-安装部署" class="headerlink" title="2、 安装部署"></a>2、 安装部署</h1><ol><li><p>上传</p></li><li><p>下载</p></li><li><p>解压</p></li><li><p>重命名配置文件模板</p><p><code>mv sqoop-env-template.sh  sqoop-env.sh</code></p></li><li><p>修改配置文件</p></li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>Set path to where bin/hadoop is available</div><div class="line">set HADOOP_COMMON_HOME=</div><div class="line"><span class="meta"></span></div><div class="line">#Set path to where hadoop-*-core.jar is available</div><div class="line">set HADOOP_MAPRED_HOME=</div><div class="line"><span class="meta"></span></div><div class="line">#set the path to where bin/hbase is available</div><div class="line"><span class="meta">#</span>set HBASE_HOME=</div><div class="line"><span class="meta"></span></div><div class="line">#Set the path to where bin/hive is available</div><div class="line">set HIVE_HOME=</div><div class="line"><span class="meta"></span></div><div class="line">#Set the path for where zookeper config dir is</div><div class="line">set ZOOCFGDIR=</div></pre></td></tr></table></figure><h1 id="3、-指令使用"><a href="#3、-指令使用" class="headerlink" title="3、 指令使用"></a>3、 指令使用</h1><p>import带入：mysql–&gt;hdfs</p><p>export导出：hdfs–&gt;mysql</p><h2 id="3-1、使用示例1：从mysql导入hdfs"><a href="#3-1、使用示例1：从mysql导入hdfs" class="headerlink" title="3.1、使用示例1：从mysql导入hdfs"></a>3.1、使用示例1：从mysql导入hdfs</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop import </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop  //指定mysql地址和数据库名</div><div class="line">--username root   //指定用户名</div><div class="line">--password root   //指定密码</div><div class="line">--table emp   //指定表名</div><div class="line">--target-dir /sqoop/data   //指定hdfs路径</div><div class="line">--num-mappers 1   //启动的map个数</div><div class="line">--fields-terminated-by "\t"          //设置数据的切分格式</div></pre></td></tr></table></figure><h2 id="3-2-、使用示例2：使用query对数据进行过滤"><a href="#3-2-、使用示例2：使用query对数据进行过滤" class="headerlink" title="3.2 、使用示例2：使用query对数据进行过滤"></a>3.2 、使用示例2：使用query对数据进行过滤</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop import </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root </div><div class="line">--password root </div><div class="line">--target-dir /sqoop/selectimport </div><div class="line">--num-mappers 1 </div><div class="line">--fields-terminated-by "\t" </div><div class="line">--query 'select * from emp where deptno &gt;= 20 and $CONDITIONS'</div></pre></td></tr></table></figure><p>检索deptno大于等于20的信息写入hdfs，此时运行一个map程序</p><p><code>$CONDITIONS</code>关键字起到索引作用，让map数据处理时不会重复</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop import </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root --password root </div><div class="line">--target-dir /sqoop/selectimport </div><div class="line">--num-mappers 2 </div><div class="line">--fields-terminated-by "\t" </div><div class="line">--query 'select * from emp where deptno &gt;= 20 and $CONDITIONS' </div><div class="line">--split-by empno</div></pre></td></tr></table></figure><p>设置两个map程序，但需要指明按照哪一项进行切分–split-by </p><h2 id="3-3-、使用示例3：导入到hive"><a href="#3-3-、使用示例3：导入到hive" class="headerlink" title="3.3 、使用示例3：导入到hive"></a>3.3 、使用示例3：导入到hive</h2><font color="red">倘若遇到报错import failed错误，可以通过添加环境变量来解决</font><p>解决办法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/modules/hive/lib/*</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">bin/sqoop import </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root </div><div class="line">--password root </div><div class="line">--table user </div><div class="line">--num-mappers 1 </div><div class="line">--hive-import </div><div class="line">--fields-terminated-by "\t" </div><div class="line">--hive-overwrite </div><div class="line">--hive-table user_sqoop</div></pre></td></tr></table></figure><h2 id="3-4、-使用示例4：hive导出mysql"><a href="#3-4、-使用示例4：hive导出mysql" class="headerlink" title="3.4、 使用示例4：hive导出mysql"></a>3.4、 使用示例4：hive导出mysql</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop export </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root</div><div class="line">--password root </div><div class="line">--table user </div><div class="line">--num-mappers 1 </div><div class="line">--export-dir /user/hive/warehouse/user_sqoop </div><div class="line">--input-fields-terminated-by "\t";</div></pre></td></tr></table></figure><p>常用参数：</p><p>import：导入数据到集群</p><p>export：从集群导出数据</p><p>create-hive-table：创建hive表</p><p>import-all-tables：导入所有库中表到hdfs集群</p><p>list-databases：列出所有数据库</p><p>list-tables：列出所有数据库表</p><p>merge：合并hdfs中的不同文件</p><p>codegen：获取某张表数据生成JavaBean并打包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">bin/sqoop codegen </div><div class="line">--connect jdbc:mysql://node01:3306/sqoop </div><div class="line">--username root </div><div class="line">--password root </div><div class="line">--table user </div><div class="line">--bindir /root/sqoopjar/userBean </div><div class="line">--class-name UserBean </div><div class="line">--fields-terminated-by "\t"</div></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">bin/sqoop merge </div><div class="line">--new-data /testmerge/new </div><div class="line">--onto /testmerge/old </div><div class="line">--target-dir /testmerge/merged </div><div class="line">--jar-file /root/sqoopjar/userBean/UserBean.jar </div><div class="line">--class-name UserBean </div><div class="line">--merge-key id</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、sqoop概述&quot;&gt;&lt;a href=&quot;#1、sqoop概述&quot; class=&quot;headerlink&quot; title=&quot;1、sqoop概述&quot;&gt;&lt;/a&gt;1、sqoop概述&lt;/h1&gt;&lt;p&gt;flume数据采集 采集日志数据&lt;/p&gt;
&lt;p&gt;sqoop数据迁移 hdfs–&amp;gt
      
    
    </summary>
    
    
      <category term="Sqoop" scheme="rabbitluluu.github.io/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Flume安装及使用</title>
    <link href="rabbitluluu.github.io/2019/03/08/Flume%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8/"/>
    <id>rabbitluluu.github.io/2019/03/08/Flume安装及使用/</id>
    <published>2019-03-08T10:10:14.000Z</published>
    <updated>2019-03-13T01:57:05.826Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>​    Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日<br>志数据。它具有基于流数据流的简单灵活的架构。它具有可靠的可靠性机制和许多故障<br>转移和恢复机制，具有强大的容错性。它使用简单的可扩展数据模型，允许在线分析应<br>用程序。</p><ol><li>数据采集（爬虫\日志数据\flume）</li><li>数据存储（hdfs/hive/hbase(nosql)）</li><li>数据计算（mapreduce/hive/sparkSQL/sparkStreaming/flink）</li><li>数据可视化(BI)</li></ol><h1 id="2、角色"><a href="#2、角色" class="headerlink" title="2、角色"></a>2、角色</h1><ol><li>source<br>数据源，用户采集数据，source产生数据流，同时会把产生的数据流传输到channel。</li><li>channel<br>传输通道，用于桥接source和sink</li><li>sink<br>下沉，用于收集channel传输的数据，将数据源传递到目标源</li><li>event<br>在flume中使用事件作为传输的基本单元</li></ol><p>图示：</p><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/DevGuide_image00.png" alt="架构"></p><h1 id="3、-安装"><a href="#3、-安装" class="headerlink" title="3、 安装"></a>3、 安装</h1><ol><li>下载</li><li>上传到linux</li><li>解压<br>tar -zxvf flume.jar</li><li>重命名<br>mv apache-flume-1.6.0-bin/ flume<br>mv flume-env.sh.template flume-env.sh</li><li>修改配置<br>vi flume-env.sh<br>export JAVA_HOME=/opt/modules/jdk1.8.0_141</li></ol><h1 id="4、使用案例"><a href="#4、使用案例" class="headerlink" title="4、使用案例"></a>4、使用案例</h1><h2 id="4-1、-官方案例：监听端口"><a href="#4-1、-官方案例：监听端口" class="headerlink" title="4.1、 官方案例：监听端口"></a>4.1、 官方案例：监听端口</h2><p>配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span>smple.conf: A single-node Flume configuration</div><div class="line"><span class="meta"></span></div><div class="line"># Name the components on this agent 定义变量方便调用 加s可以有多个此角色</div><div class="line">a1.sources = r1</div><div class="line">a1.sinks = k1</div><div class="line">a1.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source 描述source角色 进行内容定制</div><div class="line"><span class="meta">#</span> 此配置属于tcp source 必须是netcat类型</div><div class="line">a1.sources.r1.type = netcat </div><div class="line">a1.sources.r1.bind = localhost</div><div class="line">a1.sources.r1.port = 44444</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink 输出日志文件</div><div class="line">a1.sinks.k1.type = logger</div><div class="line"><span class="meta"></span></div><div class="line"># Use a channel which buffers events in memory（file） 使用内存 总大小1000 每次传输100</div><div class="line">a1.channels.c1.type = memory</div><div class="line">a1.channels.c1.capacity = 1000</div><div class="line">a1.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel 一个source可以绑定多个channel </div><div class="line"><span class="meta">#</span> 一个sinks可以只能绑定一个channel  使用的是图二的模型</div><div class="line">a1.sources.r1.channels = c1</div><div class="line">a1.sinks.k1.channel = c1</div></pre></td></tr></table></figure><p>启动指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">bin/flume-ng agent 使用ng启动agent</div><div class="line">--conf conf/ 指定配置所在的文件夹</div><div class="line">--name a1 指定的agent别名</div><div class="line">--conf-file conf/flumejob_telnet.conf 文件</div><div class="line">-Dflume.root.logger=INFO,console 日志级别</div></pre></td></tr></table></figure><h2 id="4-2、案例二：监控hive日志，实时同步到hdfs"><a href="#4-2、案例二：监控hive日志，实时同步到hdfs" class="headerlink" title="4.2、案例二：监控hive日志，实时同步到hdfs"></a>4.2、案例二：监控hive日志，实时同步到hdfs</h2><p>配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> Name the components on this agent agent别名设置</div><div class="line">a1.sources = r1</div><div class="line">a1.sinks = k1</div><div class="line">a1.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source  设置数据源监听本地文件配置</div><div class="line"><span class="meta">#</span> exec 执行一个命令的方式去查看文件 tail -F 实时查看</div><div class="line">a1.sources.r1.type = exec</div><div class="line"><span class="meta">#</span> 要执行的脚本command tail -F 默认10行 man tail  查看帮助</div><div class="line">a1.sources.r1.command = tail -F /tmp/root/hive.log</div><div class="line"><span class="meta">#</span> 执行这个command使用的是哪个脚本 -c 指定使用什么命令</div><div class="line"><span class="meta">#</span> whereis bash</div><div class="line"><span class="meta">#</span> bash: /usr/bin/bash /usr/share/man/man1/bash.1.gz </div><div class="line">a1.sources.r1.shell = /usr/bin/bash -c</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink </div><div class="line">a1.sinks.k1.type = hdfs</div><div class="line">a1.sinks.k1.hdfs.path = hdfs://node01:9000/flume/%Y%m%d/%H</div><div class="line"><span class="meta">#</span>上传文件的前缀</div><div class="line">a1.sinks.k1.hdfs.filePrefix = logs-</div><div class="line"><span class="meta">#</span>是否按照时间滚动文件夹</div><div class="line">a1.sinks.k1.hdfs.round = true</div><div class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹  秒 （默认30s）</div><div class="line">a1.sinks.k1.hdfs.roundValue = 1</div><div class="line"><span class="meta">#</span>重新定义时间单位（每小时滚动一个文件夹）</div><div class="line">a1.sinks.k1.hdfs.roundUnit = minute</div><div class="line"><span class="meta">#</span>是否使用本地时间戳</div><div class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</div><div class="line"><span class="meta">#</span>积攒多少个 Event 才 flush 到 HDFS 一次</div><div class="line">a1.sinks.k1.hdfs.batchSize = 500</div><div class="line"><span class="meta">#</span>设置文件类型，可支持压缩</div><div class="line">a1.sinks.k1.hdfs.fileType = DataStream</div><div class="line"><span class="meta">#</span>多久生成一个新的文件 秒</div><div class="line">a1.sinks.k1.hdfs.rollInterval = 30</div><div class="line"><span class="meta">#</span>设置每个文件的滚动大小 字节（最好128M）</div><div class="line">a1.sinks.k1.hdfs.rollSize = 134217700</div><div class="line"><span class="meta">#</span>文件的滚动与 Event 数量无关</div><div class="line">a1.sinks.k1.hdfs.rollCount = 0</div><div class="line"><span class="meta">#</span>最小冗余数(备份数 生成滚动功能则生效roll hadoop本身有此功能 无需配置) 1份 不冗余</div><div class="line">a1.sinks.k1.hdfs.minBlockReplicas = 1</div><div class="line"><span class="meta"></span></div><div class="line"># Use a channel which buffers events in memory </div><div class="line">a1.channels.c1.type = memory </div><div class="line">a1.channels.c1.capacity = 1000</div><div class="line">a1.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel</div><div class="line">a1.sources.r1.channels = c1</div><div class="line">a1.sinks.k1.channel = c1</div></pre></td></tr></table></figure><p>启动指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">bin/flume-ng agent </div><div class="line">--conf conf/ </div><div class="line">--name a1 </div><div class="line">--conf-file conf/flumejob_hdfs.conf</div></pre></td></tr></table></figure><h2 id="4-3、-案例三：flume监听文件夹"><a href="#4-3、-案例三：flume监听文件夹" class="headerlink" title="4.3、 案例三：flume监听文件夹"></a>4.3、 案例三：flume监听文件夹</h2><p>配置文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> 定义别名</div><div class="line">a1.sources = r1</div><div class="line">a1.sinks = k1</div><div class="line">a1.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source</div><div class="line">a1.sources.r1.type = spooldir</div><div class="line"><span class="meta">#</span> 监控的文件夹</div><div class="line">a1.sources.r1.spoolDir = /root/temp/testdir</div><div class="line"><span class="meta">#</span> 上传成功后显示后缀名 </div><div class="line">a1.sources.r1.fileSuffix = .COMPLETED</div><div class="line"><span class="meta">#</span> 如论如何 加绝对路径的文件名 默认false</div><div class="line">a1.sources.r1.fileHeader = true</div><div class="line"><span class="meta">#</span>忽略所有以.tmp 结尾的文件（正在被写入），不上传</div><div class="line"><span class="meta">#</span> ^以任何开头 出现无限次 以.tmp结尾的</div><div class="line">a1.sources.r1.ignorePattern = ([^ ]*\.tmp)</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink 下沉到hdfs</div><div class="line">a1.sinks.k1.type = hdfs</div><div class="line">a1.sinks.k1.hdfs.path = hdfs://node01:9000/flume/testdir/%Y%m%d/%H</div><div class="line"><span class="meta">#</span>上传文件的前缀</div><div class="line">a1.sinks.k1.hdfs.filePrefix = testdir-</div><div class="line"><span class="meta">#</span>是否按照时间滚动文件夹</div><div class="line">a1.sinks.k1.hdfs.round = true</div><div class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹</div><div class="line">a1.sinks.k1.hdfs.roundValue = 1</div><div class="line"><span class="meta">#</span>重新定义时间单位</div><div class="line">a1.sinks.k1.hdfs.roundUnit = hour</div><div class="line"><span class="meta">#</span>是否使用本地时间戳</div><div class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</div><div class="line"><span class="meta">#</span>积攒多少个 Event 才 flush 到 HDFS 一次</div><div class="line">a1.sinks.k1.hdfs.batchSize = 100</div><div class="line"><span class="meta">#</span>设置文件类型，可支持压缩</div><div class="line">a1.sinks.k1.hdfs.fileType = DataStream</div><div class="line"><span class="meta">#</span>多久生成一个新的文件</div><div class="line">a1.sinks.k1.hdfs.rollInterval = 600</div><div class="line"><span class="meta">#</span>设置每个文件的滚动大小大概是 128M </div><div class="line">a1.sinks.k1.hdfs.rollSize = 134217700</div><div class="line"><span class="meta">#</span>文件的滚动与 Event 数量无关</div><div class="line">a1.sinks.k1.hdfs.rollCount = 0</div><div class="line"><span class="meta">#</span>最小副本数</div><div class="line">a1.sinks.k1.hdfs.minBlockReplicas = 1</div><div class="line"><span class="meta"></span></div><div class="line"># Use a channel which buffers events in memory </div><div class="line">a1.channels.c1.type = memory </div><div class="line">a1.channels.c1.capacity = 1000</div><div class="line">a1.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel</div><div class="line">a1.sources.r1.channels = c1 </div><div class="line">a1.sinks.k1.channel = c1</div></pre></td></tr></table></figure><p>启动指令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">bin/flume-ng agent </div><div class="line">--conf conf/ </div><div class="line">--name a1 </div><div class="line">--conf-file conf/flumejob_dir.conf</div></pre></td></tr></table></figure><h2 id="4-4、案例四：多sink情景，同时监听hive日志到hdfs和local"><a href="#4-4、案例四：多sink情景，同时监听hive日志到hdfs和local" class="headerlink" title="4.4、案例四：多sink情景，同时监听hive日志到hdfs和local"></a>4.4、案例四：多sink情景，同时监听hive日志到hdfs和local</h2><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/%E5%A4%9Asink%E6%83%85%E6%99%AF.jpg" alt="情景"></p><p>配置文件flumejob_1.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> name the components on this agent 别名设置</div><div class="line">a1.sources = r1</div><div class="line">a1.sinks = k1 k2 </div><div class="line">a1.channels = c1 c2</div><div class="line"><span class="meta"></span></div><div class="line"># 将数据流复制给多个 channel</div><div class="line">a1.sources.r1.selector.type = replicating</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source </div><div class="line">a1.sources.r1.type = exec</div><div class="line">a1.sources.r1.command = tail -F /tmp/root/hive.log</div><div class="line">a1.sources.r1.shell = /bin/bash -c</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink</div><div class="line"><span class="meta">#</span> 分两个端口发送数据 </div><div class="line">a1.sinks.k1.type = avro </div><div class="line">a1.sinks.k1.hostname = node01</div><div class="line">a1.sinks.k1.port = 4141</div><div class="line"></div><div class="line">a1.sinks.k2.type = avro </div><div class="line">a1.sinks.k2.hostname = node01</div><div class="line">a1.sinks.k2.port = 4142</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the channel </div><div class="line">a1.channels.c1.type = memory </div><div class="line">a1.channels.c1.capacity = 1000</div><div class="line">a1.channels.c1.transactionCapacity = 100</div><div class="line"></div><div class="line">a1.channels.c2.type = memory </div><div class="line">a1.channels.c2.capacity = 1000</div><div class="line">a1.channels.c2.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel </div><div class="line">a1.sources.r1.channels = c1 c2 </div><div class="line">a1.sinks.k1.channel = c1</div><div class="line">a1.sinks.k2.channel = c2</div></pre></td></tr></table></figure><p>配置文件flumejob_2.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> Name the components on this agent </div><div class="line">a2.sources = r1</div><div class="line">a2.sinks = k1 </div><div class="line">a2.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source</div><div class="line">a2.sources.r1.type = avro </div><div class="line"><span class="meta">#</span> 端口抓取数据</div><div class="line">a2.sources.r1.bind = node01</div><div class="line">a2.sources.r1.port = 4141</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink </div><div class="line">a2.sinks.k1.type = hdfs</div><div class="line">a2.sinks.k1.hdfs.path = hdfs://node01:9000/flume2/%Y%m%d/%H</div><div class="line"><span class="meta"></span></div><div class="line">#上传文件的前缀</div><div class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</div><div class="line"><span class="meta">#</span>是否按照时间滚动文件夹</div><div class="line">a2.sinks.k1.hdfs.round = true</div><div class="line"><span class="meta">#</span>多少时间单位创建一个新的文件夹</div><div class="line">a2.sinks.k1.hdfs.roundValue = 1</div><div class="line"><span class="meta">#</span>重新定义时间单位</div><div class="line">a2.sinks.k1.hdfs.roundUnit = hour</div><div class="line"><span class="meta">#</span>是否使用本地时间戳</div><div class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</div><div class="line"><span class="meta">#</span>积攒多少个 Event 才 flush 到 HDFS 一次</div><div class="line">a2.sinks.k1.hdfs.batchSize = 100</div><div class="line"><span class="meta"></span></div><div class="line">#设置文件类型，可支持压缩</div><div class="line">a2.sinks.k1.hdfs.fileType = DataStream</div><div class="line"><span class="meta">#</span>多久生成一个新的文件</div><div class="line">a2.sinks.k1.hdfs.rollInterval = 600</div><div class="line"><span class="meta">#</span>设置每个文件的滚动大小大概是 128M </div><div class="line">a2.sinks.k1.hdfs.rollSize = 134217700</div><div class="line"><span class="meta">#</span>文件的滚动与 Event 数量无关</div><div class="line">a2.sinks.k1.hdfs.rollCount = 0</div><div class="line"><span class="meta">#</span>最小副本数</div><div class="line">a2.sinks.k1.hdfs.minBlockReplicas = 1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the channel </div><div class="line">a2.channels.c1.type = memory </div><div class="line">a2.channels.c1.capacity = 1000</div><div class="line">a2.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel </div><div class="line">a2.sources.r1.channels = c1</div><div class="line">a2.sinks.k1.channel = c1</div></pre></td></tr></table></figure><p>配置文件flumejob_3.conf</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#</span> Name the components on this agent </div><div class="line">a3.sources = r1</div><div class="line">a3.sinks = k1 </div><div class="line">a3.channels = c1</div><div class="line"><span class="meta"></span></div><div class="line"># Describe/configure the source </div><div class="line">a3.sources.r1.type = avro</div><div class="line">a3.sources.r1.bind = node01</div><div class="line">a3.sources.r1.port = 4142</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the sink </div><div class="line">a3.sinks.k1.type = file_roll</div><div class="line">a3.sinks.k1.sink.directory = /root/flume2</div><div class="line"><span class="meta"></span></div><div class="line"># Describe the channel </div><div class="line">a3.channels.c1.type = memory </div><div class="line">a3.channels.c1.capacity = 1000</div><div class="line">a3.channels.c1.transactionCapacity = 100</div><div class="line"><span class="meta"></span></div><div class="line"># Bind the source and sink to the channel </div><div class="line">a3.sources.r1.channels = c1</div><div class="line">a3.sinks.k1.channel = c1</div></pre></td></tr></table></figure><font color="red"><strong>注意：有启动顺序</strong></font>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h1&gt;&lt;p&gt;​    Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日&lt;br&gt;志数据。它具有基于流数据流的简单
      
    
    </summary>
    
    
      <category term="Flume" scheme="rabbitluluu.github.io/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Hive基础指令三</title>
    <link href="rabbitluluu.github.io/2019/03/08/Hive%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4%E4%B8%89/"/>
    <id>rabbitluluu.github.io/2019/03/08/Hive基础指令三/</id>
    <published>2019-03-08T10:10:05.000Z</published>
    <updated>2019-03-08T10:14:58.540Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive基础三"><a href="#Hive基础三" class="headerlink" title="Hive基础三"></a>Hive基础三</h1><h2 id="1、排序"><a href="#1、排序" class="headerlink" title="1、排序"></a>1、排序</h2><p><strong>Order By：</strong>全局排序</p><ol><li><p>按照员工表的奖金金额进行正序排列</p><p><code>select * from emptable order by emptable.comm;</code></p><p>默认正序，后面加asc， 可以省略</p></li><li><p>按照员工表的奖金金额进行倒序排列</p><p><code>select * from emptable order by emptable.comm desc;</code></p></li><li><p>按照部门和奖金进行升序排列</p><p><code>select * from emptable order by deptno,comm;</code></p></li></ol><p><strong>Sort By：</strong>内部排序，分区表中进行 并设置Reducer个数</p><ol><li><p>在<strong>分区表</strong>表中查询所有信息，启用3个reduce并根据deptno降序</p><p><code>set mapreduce.job.reduces=3;</code></p></li></ol><p>​    <code>select * from dept_partitions sort by deptno desc;</code></p><p><strong>Distribute By：</strong>分区排序</p><ol><li><p>先按照部门编号进行升序排序，再按照地域编号进行降序排序</p><p><code>select * from dept_partitions distribute by deptno sort by loc desc;</code></p></li></ol><p><strong>Cluster By：</strong></p><ol><li><p>按照部门编号进行排序</p><p><code>select * from dept_partitions cluster by deptno;</code></p><p>具备Distribute By 和Sort By两者功能，字段一样的时候可以代替</p><p>如：</p><p><code>select * from dept_partitions distribute by deptno sort by deptno;</code></p><p>等价于：<code>select * from dept_partitions cluster by deptno;</code></p></li></ol><h2 id="2、分桶"><a href="#2、分桶" class="headerlink" title="2、分桶"></a>2、分桶</h2><font color="red"><strong>区分：</strong>分桶分的是文件，分区分的是文件夹</font><p><strong>使用场景：</strong>用户需要统计一个具有代表性的结果时，并不是全部结果!</p><p>​        数据量十分大的时候，需要进行<strong>抽样</strong>的时候会用到</p><p><strong>设置支持分桶</strong>：<code>set hive.enforce.bucketing = true;</code></p><ol><li><p>如何创建分桶表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; create table emptable_buck(id int, name string)</div><div class="line">              &gt; clustered by(id) into 4 buckets</div><div class="line">              &gt; row format</div><div class="line">              &gt; delimited fields</div><div class="line">              &gt; terminated by &apos;\t&apos;;</div></pre></td></tr></table></figure></li></ol><ol><li><p>抽样</p><p><code>select * from emptable_buck tablesample(bucket 1 out of 2 on id);</code></p><p>tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y)</p><p>​        y必须是table总bucket数的<strong>倍数或者因子</strong>。hive根据y的大小，决定抽样的比例。例如，table总共分了64份，当y=32时，抽取 (64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。例 如，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</p></li></ol><h2 id="3、UDF自定义函数"><a href="#3、UDF自定义函数" class="headerlink" title="3、UDF自定义函数"></a>3、UDF自定义函数</h2><p>​    UDF：一进一出</p><p>​    UDAF：聚合函数，多进一出(count/max/avg)</p><p>​    UDTF：一进多出</p><p>添加jar包：</p><p>​    <code>add jar /root/myjar.jar</code></p><p>加载为临时的方法：</p><p>​    <code>create temporary function my_cat as &quot;包名.类名&quot;</code></p><p>加载为永久的方法：</p><p>hive-site.xml 中添加以下属性<code>hive.aux.jars.path</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">&lt;name&gt;hive.aux.jars.path&lt;/name&gt;</div><div class="line">&lt;value&gt;file:///root/hd/hive/lib/hive.jar&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure><h2 id="4、Hive压缩"><a href="#4、Hive压缩" class="headerlink" title="4、Hive压缩"></a>4、Hive压缩</h2><p>​    存储：hdfs</p><p>​    计算：mapreduce</p><h3 id="4-1Map端输出阶段压缩："><a href="#4-1Map端输出阶段压缩：" class="headerlink" title="4.1Map端输出阶段压缩："></a>4.1Map端输出阶段压缩：</h3><p>开启传输数据压缩功能：</p><p>​    <code>set hive.exec.compress.intermediate=true;</code></p><p>开启map输出压缩功能：</p><p>​    <code>set mapreduce.map.output.compress=true;</code><br>设置snappy压缩方式：<br>​    <code>set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</code></p><h3 id="4-2Reduce端输出压缩："><a href="#4-2Reduce端输出压缩：" class="headerlink" title="4.2Reduce端输出压缩："></a>4.2Reduce端输出压缩：</h3><p>设置hive输出数据压缩功能<br>    <code>set hive.exec.compress.output=true;</code><br>设置mr输出数据压缩<br>    <code>set mapreduce.output.fileoutputformat.compress=true;</code><br>指定压缩编码：<br>    <code>set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</code><br>指定压缩类型块压缩<br>    <code>set mapreduce.output.fileoutputformat.compress.type=BLOCK;</code><br>测试结果</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive基础三&quot;&gt;&lt;a href=&quot;#Hive基础三&quot; class=&quot;headerlink&quot; title=&quot;Hive基础三&quot;&gt;&lt;/a&gt;Hive基础三&lt;/h1&gt;&lt;h2 id=&quot;1、排序&quot;&gt;&lt;a href=&quot;#1、排序&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
      <category term="Hive" scheme="rabbitluluu.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive基础指令二</title>
    <link href="rabbitluluu.github.io/2019/03/08/Hive%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4%E4%BA%8C/"/>
    <id>rabbitluluu.github.io/2019/03/08/Hive基础指令二/</id>
    <published>2019-03-08T10:09:57.000Z</published>
    <updated>2019-03-08T10:14:38.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hive基础操作二"><a href="#Hive基础操作二" class="headerlink" title="Hive基础操作二"></a>Hive基础操作二</h1><h2 id="1、-分区表"><a href="#1、-分区表" class="headerlink" title="1、 分区表"></a>1、 分区表</h2><ol><li><p>创建分区表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> dept_partitions(<span class="keyword">id</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>)</div><div class="line"><span class="keyword">partition</span> <span class="keyword">by</span>(<span class="keyword">day</span> <span class="keyword">string</span>)</div><div class="line"><span class="keyword">row</span> <span class="keyword">format</span></div><div class="line"><span class="keyword">delimited</span> <span class="keyword">fields</span></div><div class="line"><span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">''</span>;</div></pre></td></tr></table></figure></li><li><p>导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/temp/dept.txt'</span> <span class="keyword">into</span> <span class="keyword">table</span> dept_partitions <span class="keyword">partition</span>(<span class="keyword">day</span> = <span class="string">"0302"</span>);</div></pre></td></tr></table></figure><p>得<strong>指明分区字段</strong></p></li><li><p>查询</p></li></ol><ul><li><p>全查询</p><p><code>select * from dept_partitions;</code></p><p>此时查看的是<strong>整个</strong>分区表</p></li><li><p>单表查询</p><p><code>select * from dept_partitions where day = &#39;0302&#39;;</code></p><p>查找<strong>指定</strong>分区的数据</p></li><li><p>联合查询–union</p><p><code>select * from dept_partitions where day = &#39;0302&#39; union select * from dept_partitions where day = &#39;0303&#39;;</code></p><p>查询多个分区</p></li></ul><ol><li><p>添加分区</p><p><code>alter table dept_partitions add partition(day = &quot;0305&quot;);</code></p></li></ol><p>   添加多个分区</p><p>   <code>alter table dept_partitions add partition(day = &quot;0306&quot;) partition(day = &quot;0307&quot;);</code></p><p>   <strong>分区之间用空格分开即可</strong></p><ol><li><p>查询分区个数</p><p><code>show partitions dept_partitions;</code></p></li><li><p>删除分区</p><p><code>alter table dept_partitions drop partition(day = &quot;0307&quot;);</code></p></li><li><p>修复表的关联</p><p>直接创建hdfs路径，将文件上传进去，执行该语句直接将数据关联</p><p><code>msck repair table dept_partitions;</code></p></li></ol><h2 id="2、-DML数据操作"><a href="#2、-DML数据操作" class="headerlink" title="2、 DML数据操作"></a>2、 DML数据操作</h2><ol><li><p>数据导入</p><p><code>load data [local] inpath &#39;/文件名&#39; into table 表名;</code></p></li><li><p>向表中插入数据</p><p><code>insert into table stu_partitions partition(age = 18) values(1,&#39;gakki&#39;);</code></p></li></ol><p>   向现有表中插入sql查询结果(overwrite是覆盖)</p><p>   <code>insert overwriter table stu_partitions partition(age = 18) select * from sean_db.mostlove where id &lt;= 3;</code></p><p>​    直接创建新表插入sql查询结果,会自动创建和被查询表一样的结构</p><p>​    <code>create table if not exists stu_partitions1 as select * from stu_partitions where id =         4;</code></p><ol><li><p>直接创建表的同时加载数据</p><p>`create table stu_partitions2 (id int, name string)</p><pre><code>&gt; row format&gt; delimited fields&gt; terminated by &apos;\t&apos;&gt; location &apos;/temp&apos;;`</code></pre></li></ol><font color="red">注意：location指定的是HDFS中的一个<strong>文件夹</strong>，非文件,并且该文件夹下不能有其他目录</font><ol><li><p>将查询结果导出到本地</p><p><code>insert overwrite local directory &#39;/root/data&#39; select * from dept_partitions;</code></p></li><li><p>将hive表中数据导出到hdfs(拷贝过程)</p><p><code>export table dept_partitions to &#39;/data&#39;;</code></p><p>将hdfs中数据导入到hive(拷贝过程)</p><p><code>inport table test_table from &#39;/data&#39;;</code></p></li><li><p>清空表</p><p><code>truncate table test_table;</code></p></li></ol><h2 id="3-查询操作"><a href="#3-查询操作" class="headerlink" title="3. 查询操作"></a>3. 查询操作</h2><ol><li><p>基础查询</p><p><code>select * from table;</code></p><p><code>select id,name from table;</code></p></li><li><p>设置别名</p><p><code>select ename , sal+1000 as salary from sean_db.emptable;</code></p></li><li><p>创建员工表</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt; create table sean_db.emptable (empno int, ename string, job string, mgr int, </div><div class="line">              &gt; birthday string, sal double, comm double, deptno int)</div><div class="line">              &gt; row format</div><div class="line">              &gt; delimited fields</div><div class="line">              &gt; terminated by '\t';</div></pre></td></tr></table></figure></li></ol><ol><li><p>查询公司有多少员工</p><p><code>select count(*) as empnumber from sean_db.emptable;</code></p></li><li><p>查询最高的工资</p><p><code>select max(sal) from sean_db.emptable;</code></p></li><li><p>查询最少的工资</p><p><code>select min(sal) from sean_db.emptable;</code></p></li><li><p>求工资的总和</p><p><code>select sum(sal) sal_sum from emp_table;</code></p></li><li><p>求该公司的员工工资平均值</p><p><code>select avg(sal) sal_avg from emptable;</code></p></li><li><p>查询结果只显示前n条</p><p><code>select * from emptable limit 3;</code></p></li></ol><p><strong>where语句，== 过滤</strong></p><p><strong>使用：where字句后紧接from</strong></p><ol><li><p>求出工资大于1300的员工</p><p><code>select * from emptable where sal &gt; 1300;</code></p></li><li><p>求出工资在1300~3000范围的员工信息</p><p><code>select * from emptable where sal &gt;= 1300 and sal &lt;= 3000;</code></p><p><code>select * from emptable where sal &gt; 1300 and sal &lt; 3000;</code></p><p><strong>is null 和 is not null</strong></p></li><li><p>求出奖金为空的</p><p><code>select * from emptable where comm is null;</code></p></li><li><p>求出奖金为非空的</p><p><code>select * from emptable where comm is not null;</code></p></li><li><p>求出工资在2000~3000员工的姓名</p><p><code>select ename from emptable where sal in(2000,3000);</code></p></li></ol><p><strong>like:模糊查询</strong></p><p><strong>使用：通配符</strong></p><p><code>%</code> : 代表后面零个或多个字符</p><p><code>_</code> : 代表一个字符</p><ol><li><p>查询工资以1开头的员工信息</p><p><code>select * from emptable where sal like &#39;1%&#39;;</code></p></li><li><p>查询工资第二位为1的员工信息</p><p><code>select * from emptable where sal like &#39;_1%&#39;;</code></p></li><li><p>查询工资里面含有5的员工信息</p><p><code>select * from emptable where sal like &#39;%5%&#39;;</code></p></li></ol><p><strong>AND/NOT/OR的使用</strong></p><ol><li><p>查询工资大于两千并且部门号是30</p><p><code>select * from emptable where sal &gt; 2000 and deptno = 30;</code></p></li><li><p>查询工资大于两千或者部门号是30</p><p><code>select * from emptable where sal &gt; 2000 or deptno = 30;</code></p></li><li><p>查询工资不在2000~3000的员工信息</p><p><code>select * from emptable where sal &lt; 2000 or sal &gt; 3000;</code></p></li></ol><p><strong>分组操作</strong></p><p><code>Group By</code>： 通常和一些聚合函数一起使用</p><ol><li><p>求每个部门的平均工资</p><p><code>select avg(sal) as avg_sal , deptno from emptable group by deptno;</code></p></li><li><p>求每个部门工资最低的工资</p><p><code>select min(sal) as min_sal, deptno from emptable group by deptno;</code></p></li></ol><p><code>having</code>： 用于分组的统计语句</p><p><code>where</code>： 后面不可以跟分组函数</p><ol><li><p>求平均工资大于2000的部门</p><p><code>select deptno, avg(sal) as avg_sal from emptable group by deptno having avg_sal &gt; 2000;</code></p></li></ol><h2 id="4-Join操作"><a href="#4-Join操作" class="headerlink" title="4. Join操作"></a>4. Join操作</h2><p>员工表中只有部门编号，没有部门名称</p><p>部门表中有部门标号和部门名称</p><ol><li><p>查询员工编号、员工姓名、员工所在的部门名称（等值join）</p><p><code>select emptable.empno, emptable.ename, dept.dname from emptable join dept on emptable.deptno = dept.deptno;</code></p><p><code>select e.empno, e.ename, d.dname from emptable as e join dept as d where e.deptno = d.deptno;</code></p></li><li><p>查询员工编号、员工姓名、员工所在的部门名称、部门所在地</p><p><strong>内连接</strong>：只有连接的两张表都存在于条件相匹配的数据才会被保留下来</p><p><code>select e.empno, e.ename, d.dname ,d.loc from emptable as e join dept as d on e.deptno = d.deptno;</code></p></li><li><p>查询员工编号、员工姓名、部门名称</p><p><strong>左外链接 left join</strong>：默认用的就是左外连接，可以省略，保留左表数据，右表没有join上 显示null</p><p><code>select e.empno, e.ename, d.dname from emptable e left join dept d on e.deptno = d.deptno;</code></p></li><li><p>查询员工编号、员工姓名、部门名称</p><p><strong>右外链接  right join</strong>：保留右表数据，左表没有join上显示为null</p><p><code>select e.empno, e.ename, d.dname from emptable e right join dept d on e.deptno = d.deptno;</code></p></li><li><p>查询员工编号、员工姓名、部门名称</p><p><strong>满外链接</strong>： 结果会返回所有表中符合条件的所有记录，如果有字段没有符合条件，用null代替</p><p><code>select e.empno, e.ename, d.dname from emptable e full join dept d on e.deptno = d.deptno</code></p></li><li><p>查询员工名、部门名称、地域名称———-多表查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">select</span> e.ename, d.dname, l.loc_name <span class="keyword">from</span> emptable e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno = d.deptno</div><div class="line">              &gt; <span class="keyword">join</span> location l <span class="keyword">on</span> d.loc = l.loc;</div></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hive基础操作二&quot;&gt;&lt;a href=&quot;#Hive基础操作二&quot; class=&quot;headerlink&quot; title=&quot;Hive基础操作二&quot;&gt;&lt;/a&gt;Hive基础操作二&lt;/h1&gt;&lt;h2 id=&quot;1、-分区表&quot;&gt;&lt;a href=&quot;#1、-分区表&quot; class=&quot;head
      
    
    </summary>
    
    
      <category term="Hive" scheme="rabbitluluu.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive基础指令一</title>
    <link href="rabbitluluu.github.io/2019/03/08/Hive%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4%E4%B8%80/"/>
    <id>rabbitluluu.github.io/2019/03/08/Hive基础指令一/</id>
    <published>2019-03-08T10:09:48.000Z</published>
    <updated>2019-03-08T10:14:09.415Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、基础指令"><a href="#1、基础指令" class="headerlink" title="1、基础指令"></a>1、基础指令</h1><ul><li><p>显示数据库</p><p><code>show databases;</code></p></li><li><p>创建数据库</p><p><code>create database sean_db;</code></p><p><strong>标准写法：</strong></p><p><code>create database if not exists sean_db;</code></p><p><strong>指定hdfs路径为根目录db_cache文件夹下：</strong></p><p><code>create database sean_db location &#39;/db_cache&#39;</code></p><p><code>reate database if not exists sean_db location &#39;/&#39;;</code></p></li><li><p>使用数据库</p><p><code>use sean_db</code></p></li><li><p>创建表</p><p>mysql语法：<code>create table love(id int, name,string);</code></p><p>将查询结果创建为表</p><p><code>create table love2 as select * from love where name = &#39;gakki&#39;;</code></p><p><strong>hive语法：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> mostlove(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>)</div><div class="line">              &gt; <span class="keyword">row</span> <span class="keyword">format</span></div><div class="line">              &gt; <span class="keyword">delimited</span> <span class="keyword">fields</span></div><div class="line">              &gt; <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</div></pre></td></tr></table></figure></li><li><p>插入数据</p><p><code>insert into love values(1, &quot;gakki&quot;);</code></p></li><li><p>导入数据</p><p><code>load data [local] inpath &#39;文件路径&#39; into table 表名;</code></p><ul><li><p>导入本地文件</p><p><code>load data local inpath &#39;/root/temp/sean.txt&#39; into table love;</code></p></li><li><p>导入HDFS文件–根目录下的sean.txt到love表中</p><p><code>load data inpath &#39;/sean.txt&#39; into table love;</code></p></li></ul></li><li><p>删除表</p><p><code>drop table love;</code></p></li><li><p>查看库结构</p><p><code>desc database sean_db;</code></p></li><li><p>给库添加额外描述信息</p><p><code>alter database sean_db set dbproperties(&#39;created&#39; = &#39;sean&#39;);</code></p></li><li><p>查看库额外信息</p><p><code>desc database extended sean_db;</code></p></li><li><p>查看指定的通配库:过滤–找出所有s开头的库</p><p><code>show databases like &quot;s*&quot;;</code></p></li><li><p>删除库</p><ul><li>drop database sean_db;</li></ul><p>删除非空库</p><ul><li>drop database sean_db cascade;</li></ul><p>删除非空库标准写法</p><ul><li>drop databases if exists sean_db cascade;</li></ul></li></ul><h1 id="2、数据类型"><a href="#2、数据类型" class="headerlink" title="2、数据类型"></a>2、数据类型</h1><table><thead><tr><th style="text-align:center">Java数据类型</th><th style="text-align:center">HIve数据类型</th><th style="text-align:center">长度</th></tr></thead><tbody><tr><td style="text-align:center">byte</td><td style="text-align:center">TINYINT</td><td style="text-align:center">1byte有符号整数</td></tr><tr><td style="text-align:center">short</td><td style="text-align:center">SMALLINT</td><td style="text-align:center">2byte有符号整数</td></tr><tr><td style="text-align:center">int</td><td style="text-align:center">INT</td><td style="text-align:center">4byte有符号整数</td></tr><tr><td style="text-align:center">long</td><td style="text-align:center">BIGINT</td><td style="text-align:center">8byte有符号整数</td></tr><tr><td style="text-align:center">boolean</td><td style="text-align:center">BOOLEAN</td><td style="text-align:center">true/false</td></tr><tr><td style="text-align:center">float</td><td style="text-align:center">FLOAT</td><td style="text-align:center">单精度浮点</td></tr><tr><td style="text-align:center">double</td><td style="text-align:center">DOUBLE</td><td style="text-align:center">双精度浮点</td></tr><tr><td style="text-align:center">String</td><td style="text-align:center">STRING</td><td style="text-align:center">字符</td></tr><tr><td style="text-align:center"></td><td style="text-align:center">BINARY</td><td style="text-align:center">字节数组</td></tr></tbody></table><h1 id="3、操作补充"><a href="#3、操作补充" class="headerlink" title="3、操作补充"></a>3、操作补充</h1><h2 id="3-1-创建表"><a href="#3-1-创建表" class="headerlink" title="3.1 创建表"></a>3.1 创建表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">create</span> [<span class="keyword">external</span>] <span class="keyword">table</span> [<span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span>] table_name(字段信息) [partitioned <span class="keyword">by</span>(字段信息)]</div><div class="line">[clustered <span class="keyword">by</span>(字段信息)] [sorted <span class="keyword">by</span>(字段信息)]</div><div class="line"><span class="keyword">row</span> <span class="keyword">format</span> </div><div class="line"><span class="keyword">delimited</span> <span class="keyword">fields</span></div><div class="line"><span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'切割符'</span>;</div></pre></td></tr></table></figure><h2 id="3-2-管理表（内部表）"><a href="#3-2-管理表（内部表）" class="headerlink" title="3.2 管理表（内部表）"></a>3.2 管理表（内部表）</h2><p>默认不加external创建的就是<strong>管理表</strong>，也称为<strong>内部表</strong>。<br>MANAGED_TABLE管理表。<br>Table Type:MANAGED_TABLE<br>查看表类型：<br><code>desc formatted itstar;</code></p><h2 id="3-3-外部表"><a href="#3-3-外部表" class="headerlink" title="3.3 外部表"></a>3.3 外部表</h2><p>EXTERNAL_TABLE外部表<br>创建方式：<br><code>create external table student(id int,name string)</code></p><p><strong>区别</strong>：如果是管理表，删除hdfs中数据删除，</p><p>​            如果是外部表删除，hdfs数据<strong>不删除</strong>！外部表删除后，重新创建同名表后，表中数据还在</p><h1 id="4、Hive常用命令"><a href="#4、Hive常用命令" class="headerlink" title="4、Hive常用命令"></a>4、Hive常用命令</h1><ul><li><p>不登录hive客户端操作hive指令</p><p><code>bin/hive -e &quot;select * from sean_db.love;&quot;</code></p></li><li><p>直接把sql写入到文件中</p><p><code>bin/hive -f /root/temp/hql.sql</code></p></li><li><p>在hive中可以直接查看hdfs中文件(省去hdfs)</p><p>dfs -ls /</p></li><li><p>清空表内数据,保留表结构</p><p><code>truncate table sean_buck;</code></p></li><li><p>设置Reducer个数</p><p><code>set mapreduce.job.reduces</code></p></li><li><p>设置支持分桶</p><p><code>set hive.enforce.bucketing = true;</code></p></li><li><p>显示方法</p><p><code>show functions;</code></p></li><li><p>查询方法使用</p><p><code>desc function extended pow;</code></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、基础指令&quot;&gt;&lt;a href=&quot;#1、基础指令&quot; class=&quot;headerlink&quot; title=&quot;1、基础指令&quot;&gt;&lt;/a&gt;1、基础指令&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;显示数据库&lt;/p&gt;
&lt;p&gt;&lt;code&gt;show databases;&lt;/code&gt;&lt;/p&gt;
      
    
    </summary>
    
    
      <category term="Hive" scheme="rabbitluluu.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Hive安装及配置</title>
    <link href="rabbitluluu.github.io/2019/03/08/Hive%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE/"/>
    <id>rabbitluluu.github.io/2019/03/08/Hive安装及配置/</id>
    <published>2019-03-08T10:09:38.000Z</published>
    <updated>2019-03-08T11:05:30.080Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、概述"><a href="#1、概述" class="headerlink" title="1、概述"></a>1、概述</h1><p>官网：<a href="http://hive.apache.org/" target="_blank" rel="external">http://hive.apache.org/</a><br>Apache Hive™数据仓库软件有助于使用SQL读取，编写和管理驻留在分布式存储中的<br>大型数据集。可以将结构投影到已存储的数据中。提供了命令行工具和JDBC驱动程序以<br>将用户连接到Hive。<br>hive提供了SQL查询功能 hdfs分布式存储。<br>hive本质HQL转化为MapReduce程序。<br>环境前提：</p><ul><li><p>启动hdfs集群</p></li><li><p>启动yarn集群</p><p>如果想用hive的话，需要提前安装部署好hadoop集群。</p></li></ul><h1 id="2、特点"><a href="#2、特点" class="headerlink" title="2、特点"></a>2、特点</h1><p>  <strong>优势：</strong></p><ul><li>操作接口采用类sql语法，select * from stu;<br>简单、上手快！</li><li>hive可以替代mr程序，sqoop。</li><li>hive可以处理海量数据。</li><li>hive支持UDF，自定义函数。</li></ul><p><strong>劣势：</strong></p><ul><li><p>处理数据延迟高，慢。<br>引擎：1.2.2以前版本都是用的mr引擎</p><pre><code>2.x之后用的是spark引擎</code></pre></li><li><p>HQL的表达能力有限<br>一些sql无法解决的场景，依然需要我们写mapreduce.</p><h1 id="3、-架构原理"><a href="#3、-架构原理" class="headerlink" title="3、 架构原理"></a>3、 架构原理</h1></li></ul><p><img src="https://raw.githubusercontent.com/rabbitluluu/pic_resource/master/hive%20%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.png" alt=""></p><blockquote><p>sql-&gt;转换-&gt;mapreduce-&gt;job</p></blockquote><h1 id="4、-安装部署"><a href="#4、-安装部署" class="headerlink" title="4、 安装部署"></a>4、 安装部署</h1><ol><li>下载</li><li>上传</li><li>解压</li><li>重命名conf中配置文件<br>mv hive-env.sh.template hive-env.sh</li><li>修改配置文件<br>vi hive-env.sh<br>HADOOP_HOME=/opt/modules/hadoop-2.8.4<br>export HIVE_CONF_DIR=/opt/modules/hive/conf</li><li>启动</li></ol><h1 id="5、-配置元数据库为Mysql"><a href="#5、-配置元数据库为Mysql" class="headerlink" title="5、 配置元数据库为Mysql"></a>5、 配置元数据库为Mysql</h1><p>因为HIVE自带元数据库derby不支持多客户端同时访问，故此更换Mysql为元数据库</p><p>前提：安装好Mysql</p><p>参考资料：<a href="https://blog.csdn.net/pengjunlee/article/details/81212250" target="_blank" rel="external">https://blog.csdn.net/pengjunlee/article/details/81212250</a></p><ol><li>驱动拷贝<br>拷贝mysql-connector-java-5.1.39-bin.jar到/root/opt/modules/hive/lib/下</li><li>配置Metastore到MySql</li></ol><ul><li><p>在/opt/modules/hive/conf目录下创建一个hive-site.xml</p></li><li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中（hive/conf/下创建文件）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</div><div class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</div><div class="line">&lt;configuration&gt;</div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</div><div class="line">    &lt;value&gt;jdbc:mysql://node01:3306/metastorecreateDatabaseIfNotExist=true&lt;/value&gt;</div><div class="line">  &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</div><div class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</div><div class="line">  &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</div><div class="line">  &lt;value&gt;root&lt;/value&gt;</div><div class="line">  &lt;description&gt;username to use against metastore database&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</div><div class="line">  &lt;value&gt;root密码&lt;/value&gt;</div><div class="line">  &lt;description&gt;password to use against metastore database&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure><p>配置完之后需要重启集群生效。</p></li></ul><h1 id="6、-设置显示字段名与当前状态"><a href="#6、-设置显示字段名与当前状态" class="headerlink" title="6、 设置显示字段名与当前状态"></a>6、 设置显示字段名与当前状态</h1><h1 id="6-1-更改默认的元数据路径"><a href="#6-1-更改默认的元数据路径" class="headerlink" title="6.1 更改默认的元数据路径"></a>6.1 更改默认的元数据路径</h1><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</div><div class="line">  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</div><div class="line">  &lt;description&gt;location of default database for the warehouse&lt;/description&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure><h1 id="6-2-设置显示头信息和当前库"><a href="#6-2-设置显示头信息和当前库" class="headerlink" title="6.2 设置显示头信息和当前库"></a>6.2 设置显示头信息和当前库</h1><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.cli.print.header&lt;/name&gt;</div><div class="line">  &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</div><div class="line">  &lt;value&gt;true&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1、概述&quot;&gt;&lt;a href=&quot;#1、概述&quot; class=&quot;headerlink&quot; title=&quot;1、概述&quot;&gt;&lt;/a&gt;1、概述&lt;/h1&gt;&lt;p&gt;官网：&lt;a href=&quot;http://hive.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;exte
      
    
    </summary>
    
    
      <category term="Hive" scheme="rabbitluluu.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper监听API操作</title>
    <link href="rabbitluluu.github.io/2019/03/08/Zookeeper%E7%9B%91%E5%90%ACAPI%E6%93%8D%E4%BD%9C/"/>
    <id>rabbitluluu.github.io/2019/03/08/Zookeeper监听API操作/</id>
    <published>2019-03-08T10:09:26.000Z</published>
    <updated>2019-03-08T10:13:22.587Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-对目录监听"><a href="#1-对目录监听" class="headerlink" title="1.对目录监听"></a>1.对目录监听</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.watch_;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.KeeperException;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/2/24/14:56</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> * 实现对节点的监听</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">watchDemo_</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</div><div class="line">        <span class="comment">//设置集群地址</span></div><div class="line">        String connected = <span class="string">"node01:2181,node02:2181,node03:2181,node04:2181"</span>;</div><div class="line">        <span class="comment">//设置超时时间</span></div><div class="line">        <span class="keyword">int</span> timeout = <span class="number">2000</span>;</div><div class="line">        <span class="comment">//创建客户端</span></div><div class="line">        ZooKeeper zkCli = <span class="keyword">new</span> ZooKeeper(connected, timeout, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</div><div class="line">                System.out.println(<span class="string">"正在监听：..."</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line"></div><div class="line">        List&lt;String&gt; children = zkCli.getChildren(<span class="string">"/"</span>, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</div><div class="line">                System.out.println(<span class="string">"正在监听路径为："</span> + event.getPath());</div><div class="line">                System.out.println(<span class="string">"正在监听类型为:"</span> + event.getType());</div><div class="line">            &#125;</div><div class="line">        &#125;, <span class="keyword">null</span>);</div><div class="line"></div><div class="line">        <span class="keyword">for</span>(String s:children)&#123;</div><div class="line">            System.out.println(s);</div><div class="line">        &#125;</div><div class="line">        Thread.sleep(Long.MAX_VALUE);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="2-对节点内容监听"><a href="#2-对节点内容监听" class="headerlink" title="2.对节点内容监听"></a>2.对节点内容监听</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.watch_;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.KeeperException;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/2/24/15:08</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> * 实现对节点内容的监听</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">watchDemo1_</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</div><div class="line">        <span class="comment">//设置集群地址</span></div><div class="line">        String connected = <span class="string">"node01:2181,node02:2181,node03:2181,node04:2181"</span>;</div><div class="line">        <span class="comment">//设置超时时间</span></div><div class="line">        <span class="keyword">int</span> timeout = <span class="number">2000</span>;</div><div class="line">        ZooKeeper zkCli = <span class="keyword">new</span> ZooKeeper(connected, timeout, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent watchedEvent)</span> </span>&#123;</div><div class="line">                System.out.println(<span class="string">"正在监听....请注意言行"</span>);</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line"></div><div class="line">        <span class="keyword">byte</span>[] data = zkCli.getData(<span class="string">"/gakki"</span>, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</div><div class="line">                System.out.println(<span class="string">"监听路径为："</span> + event.getPath());</div><div class="line">                System.out.println(<span class="string">"监听类型为："</span> + event.getType());</div><div class="line">            &#125;</div><div class="line">        &#125;, <span class="keyword">null</span>);</div><div class="line"></div><div class="line">        System.out.println(<span class="keyword">new</span> String(data));</div><div class="line">        Thread.sleep(Long.MAX_VALUE);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="3-实现持续监听"><a href="#3-实现持续监听" class="headerlink" title="3.实现持续监听"></a>3.实现持续监听</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">package</span> xyz.llsean.qq_;</div><div class="line"></div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.KeeperException;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</div><div class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</div><div class="line"></div><div class="line"><span class="keyword">import</span> java.io.IOException;</div><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.List;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * <span class="doctag">@author</span>: Sean</div><div class="line"> * <span class="doctag">@date</span>: 2019/2/24/15:13</div><div class="line"> * <span class="doctag">@version</span>: 1.0</div><div class="line"> * 实现持续监听</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">zkClient_</span> </span>&#123;</div><div class="line"></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, KeeperException, InterruptedException </span>&#123;</div><div class="line">        <span class="comment">//1.获取客户端</span></div><div class="line">        zkClient_ zkClient_ = <span class="keyword">new</span> zkClient_();</div><div class="line">        zkClient_.getConnect();</div><div class="line">        <span class="comment">//2.指定监听路径</span></div><div class="line">        zkClient_.getServer();</div><div class="line">        <span class="comment">//3.线程休眠</span></div><div class="line">        zkClient_.getWatchh();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">    * 获取zookeeper客户端连接</div><div class="line">    * */</div><div class="line">    <span class="comment">//设置集群地址</span></div><div class="line">    <span class="keyword">final</span> String connected = <span class="string">"node01:2181,node02:2181,node03:2181,node04:2181"</span>;</div><div class="line">    <span class="comment">//设置超时时间</span></div><div class="line">    <span class="keyword">final</span> <span class="keyword">int</span> timeout = <span class="number">2000</span>;</div><div class="line"></div><div class="line">    ZooKeeper zkCli;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getConnect</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</div><div class="line">        zkCli = <span class="keyword">new</span> ZooKeeper(connected, timeout, <span class="keyword">new</span> Watcher() &#123;</div><div class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(WatchedEvent event)</span> </span>&#123;</div><div class="line">                <span class="comment">//System.out.println("当前正在监听...");</span></div><div class="line">                List&lt;String&gt; children;</div><div class="line"></div><div class="line">                <span class="keyword">try</span> &#123;</div><div class="line">                    children = zkCli.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</div><div class="line">                    <span class="comment">//服务器列表</span></div><div class="line">                    ArrayList&lt;String&gt; serverList_ = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line">                    <span class="comment">//获取每个节点数据</span></div><div class="line">                    <span class="keyword">for</span>(String s:children)&#123;</div><div class="line">                        <span class="keyword">byte</span>[] data = zkCli.getData(<span class="string">"/"</span> + s, <span class="keyword">true</span>, <span class="keyword">null</span>);</div><div class="line">                        serverList_.add(<span class="keyword">new</span> String(data));</div><div class="line">                    &#125;</div><div class="line"></div><div class="line">                    <span class="comment">//查看服务器列表</span></div><div class="line">                    System.out.println(serverList_);</div><div class="line">                &#125; <span class="keyword">catch</span> (KeeperException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 指定监听路径</div><div class="line">     * */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getServer</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException </span>&#123;</div><div class="line"></div><div class="line">        List&lt;String&gt; children = zkCli.getChildren(<span class="string">"/"</span>, <span class="keyword">true</span>);</div><div class="line"></div><div class="line">        <span class="comment">//存储服务器列表</span></div><div class="line">        ArrayList&lt;String&gt; serverList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</div><div class="line"></div><div class="line">        <span class="keyword">for</span>(String s: children)&#123;</div><div class="line">            <span class="keyword">byte</span>[] data = zkCli.getData(<span class="string">"/"</span> + s, <span class="keyword">true</span>, <span class="keyword">null</span>);</div><div class="line">            serverList.add(<span class="keyword">new</span> String(data));</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        <span class="comment">//打印服务器列表</span></div><div class="line">        System.out.println(serverList);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">/**</span></div><div class="line">     * 实现一直监听</div><div class="line">     * */</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getWatchh</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</div><div class="line">        Thread.sleep(Long.MAX_VALUE);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-对目录监听&quot;&gt;&lt;a href=&quot;#1-对目录监听&quot; class=&quot;headerlink&quot; title=&quot;1.对目录监听&quot;&gt;&lt;/a&gt;1.对目录监听&lt;/h2&gt;&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;
      
    
    </summary>
    
    
      <category term="Zookeeper" scheme="rabbitluluu.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper命令行以及API操作</title>
    <link href="rabbitluluu.github.io/2019/03/08/Zookeeper%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BB%A5%E5%8F%8AAPI%E6%93%8D%E4%BD%9C/"/>
    <id>rabbitluluu.github.io/2019/03/08/Zookeeper命令行以及API操作/</id>
    <published>2019-03-08T10:09:15.000Z</published>
    <updated>2019-03-08T10:12:38.556Z</updated>
    
    <content type="html"><![CDATA[<h1 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h1><p>配置环境变量：vi /etc/profile<br>export ZOOKEEPER_HOME=/root/hd/zookeeper-3.4.10<br>export PATH=$PATH:$ZOOKEEPER_HOME/bin<br>声明环境变量：source /etc/profile</p><ul><li><p>启动客户端<br>$ bin/zkCli.sh</p></li><li><p>连接其它机器客户端操作<br>$ connect node02:2181</p></li><li><p>查看历史操作记录<br>$ history</p></li><li><p>查看当前节点的内容<br>$ ls /</p></li><li><p>存储：创建节点<br>$ create /sean 10</p></li><li><p>查看节点的值<br>$ get /sean</p></li><li><p>创建节点的可选项<br>create [-s] [-e] path data acl<br>[-e] <strong>短暂</strong>节点<br>[-s] 带<strong>序号</strong><br>create -e /test 100<br><strong>注意：此时-e创建的是临时的短暂节点，退出客户端后消失。</strong>退出客户端：quit<br>create -s /test 100<br><strong>注意：此时-s创建是带序号的节点，可以创建节点名相同的，序号依次累加。</strong><br>创建短暂带序号节点<br>create -e -s /test 100</p></li><li><p>修改节点值<br>set path data [version]<br>[version] 版本<br>注意：设置版本号 必须从0开始</p></li><li><p>删除节点<br>delete path</p></li><li><p>创建子节点<br>create /sean/xyz</p></li><li><p>递归删除<br>rmr /sean</p></li><li><p>监听<br>获得监听：get path watch<br>获得当前节点下增减变化：ls path watch</p></li><li><p>查看当前节点状态<br>stat /test</p><h1 id="节点状态信息"><a href="#节点状态信息" class="headerlink" title="节点状态信息"></a>节点状态信息</h1><p>cZxid:zookeeper事务id<br>cTime:节点创建时间<br>mZxid：最后更新的czxid<br>mtime:最后修改的时间<br>pZxid：最后更新子节点的czxid<br>cversion:子节点的变化号、子节点修改次数<br>dataVersion：数据变化号<br>aclVersion：访问控制列表的变化号<br>ephemeralOwner：临时节点判断<br>dataLength：节点数据长度<br>numChildren：子节点个数</p></li></ul><h1 id="API操作"><a href="#API操作" class="headerlink" title="API操作"></a>API操作</h1><p><strong>EPHEMERAL</strong>（短暂的znode）、</p><p><strong>EPHEMERAL_SEQUENTIAL</strong>（短暂的顺序znode）、</p><p><strong>PERSISTENT</strong>（持久的znode）</p><p><strong>PERSISTENT_SEQUENTIAL</strong>（持久的顺序znode）</p><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line">package xyz.llsean.zk;</div><div class="line"></div><div class="line">import org.apache.zookeeper.*;</div><div class="line">import org.junit.Before;</div><div class="line">import org.junit.Test;</div><div class="line"></div><div class="line">import java.io.IOException;</div><div class="line">import java.util.List;</div><div class="line"></div><div class="line">/**</div><div class="line"> * @author: Sean</div><div class="line"> * @date: 2019/2/22/22:06</div><div class="line"> * @version: 1.0</div><div class="line"> */</div><div class="line">public class zkClient &#123;</div><div class="line">    //集群地址：端口号</div><div class="line">    private String connected = "node01:2181,node02:2181,node03:2181,node04:2181";</div><div class="line"></div><div class="line">    //超时设置（毫秒）</div><div class="line">    private int timeout = 2000;</div><div class="line"></div><div class="line">    ZooKeeper zkCli = null;</div><div class="line"></div><div class="line">    @Before</div><div class="line">    //连接zookeeper集群</div><div class="line">    public void init() throws IOException &#123;</div><div class="line">        //String:连接集群的ip地址端口 Int：超时设置 Wacher：监听</div><div class="line">        zkCli = new ZooKeeper(connected, timeout, new Watcher() &#123;</div><div class="line">            //回调方法   显示根节点</div><div class="line">            public void process(WatchedEvent event) &#123;</div><div class="line">                System.out.println("已经触发" + event.getType() + "事件！");</div><div class="line">                List&lt;String&gt; chilren;</div><div class="line"></div><div class="line">                //获得节点get</div><div class="line">                try &#123;</div><div class="line">                    chilren = zkCli.getChildren("/", true);</div><div class="line">                &#125; catch (KeeperException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125; catch (InterruptedException e) &#123;</div><div class="line">                    e.printStackTrace();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    //测试 是否联通集群  创建节点</div><div class="line">    @Test</div><div class="line">    public void createNode() throws KeeperException, InterruptedException &#123;</div><div class="line">        String p = zkCli.create("/sean", "2333".getBytes(),</div><div class="line">                ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);</div><div class="line">        //System.out.println("目前节点状态：["+zkCli.exists("/", true)+"]");</div><div class="line">        System.out.println(p);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    /**</div><div class="line">     * 获取节点列表</div><div class="line">     * */</div><div class="line">    @Test</div><div class="line">    public void getChild() throws KeeperException, InterruptedException &#123;</div><div class="line">        List&lt;String&gt; children = zkCli.getChildren("/", true);</div><div class="line">        for(String s:children)&#123;</div><div class="line">            System.out.println(s);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /**</div><div class="line">     * 删除节点</div><div class="line">     * */</div><div class="line">    @Test</div><div class="line">    public void deleteData() throws KeeperException, InterruptedException &#123;</div><div class="line">        zkCli.delete("/sean0000000016", -1);</div><div class="line">        getChild();</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /**</div><div class="line">     * 修改数据</div><div class="line">     * */</div><div class="line">    @Test</div><div class="line">    public void setData() throws KeeperException, InterruptedException &#123;</div><div class="line">        zkCli.setData("/sean", "sb".getBytes(), -1);</div><div class="line"></div><div class="line">        //查看</div><div class="line">        byte[] data = zkCli.getData("/sean", false, new Stat());</div><div class="line">        System.out.println(new String (data));</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /**</div><div class="line">     * 指定的节点是否存在</div><div class="line">     * */</div><div class="line">    @Test</div><div class="line">    public void testExist() throws KeeperException, InterruptedException &#123;</div><div class="line">        Stat exists = zkCli.exists("/llsean", false);</div><div class="line">        System.out.println(exists == null ? "不存在"":"存在");</div><div class="line">    &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;命令行操作&quot;&gt;&lt;a href=&quot;#命令行操作&quot; class=&quot;headerlink&quot; title=&quot;命令行操作&quot;&gt;&lt;/a&gt;命令行操作&lt;/h1&gt;&lt;p&gt;配置环境变量：vi /etc/profile&lt;br&gt;export ZOOKEEPER_HOME=/root/hd/z
      
    
    </summary>
    
    
      <category term="Zookeeper" scheme="rabbitluluu.github.io/tags/Zookeeper/"/>
    
  </entry>
  
</feed>
